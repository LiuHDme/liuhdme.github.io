<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>追本溯源，带你读懂图自编码器 | LiuHDme</title>
  <meta name="author" content="LiuHDme">
  
  <meta name="description" content="这篇文章简要介绍图自编码器的原理和应用">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="追本溯源，带你读懂图自编码器"/>
  <meta property="og:site_name" content="LiuHDme"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/cerulean.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight-default.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/comment.css" media="screen" type="text/css">
  <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.9/es5-shim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/es5-shim/4.5.7/es5-sham.min.js"></script>
  <![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  
  <!-- analytics -->
  



<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">LiuHDme</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="所有文章">
			  <i class="fa fa-archive"></i>归档
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="所有标签">
			  <i class="fa fa-tags"></i>标签
			</a>
		  </li>
		  
		  <li>
			<a href="/photos" title="">
			  <i class="fa fa-camera"></i>相册
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="个人简介">
			  <i class="fa fa-user"></i>关于
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
    <div class="content">
      


	
		<div class="page-header page-header-inverse ">		
			<h1 class="title title-inverse "> 追本溯源，带你读懂图自编码器</h1>
		</div>		
	






<div class="row post">
	<!-- cols -->
	
	<div id="top_meta"></div>
	<div class="col-md-9">
	

	<!-- content -->
	<div class="mypage">		
	  
		 <div class="alert alert-success description">
			<i class="fa fa-info-circle"></i> <p>这篇文章简要介绍图自编码器的原理和应用</p>
			
		 </div> <!-- alert -->
	  		

	  <h2 id="介绍">介绍</h2>
<p>Kipf 与 Welling 16 年发表了「Variational Graph Auto-Encoders」，提出了基于图的（变分）自编码器 <strong>Variational Graph Auto-Encoder（VGAE）</strong>。</p>
<p><strong>论文地址</strong>：http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf</p>
<p><strong>代码地址</strong>：https://github.com/tkipf/gae</p>
<p><strong>先介绍一下 intention 和用途</strong>。了解 embedding 概念的应该知道，获取一个合适的 embedding 来表示一张图中的节点不是一件容易的事，另外，如果我们能找到合适的 embedding，就能将它们用在上层任务中。VGAE 的用途之一就是通过 encoder-decoder 的结构来获取图中节点的 embedding。</p>
<p><strong>VGAE 的思想和变分自编码器（VAE）很像</strong>：利用隐变量（latent variables），让模型学习出一些分布（distribution），再从这些分布中采样得到 latent representations（或者说 embedding），<strong>这个过程是 encode 阶段</strong>，然后再利用得到的 latent representations 重构（reconstruct）出原始的图，<strong>这个过程是 decode 阶段</strong>。只不过，VGAE 的 encoder 使用了 GCN，decoder 是简单的内积（inner product）形式。</p>
<p><strong>下面具体讲解变分图自编码器（VGAE）。先讲 GAE，即图自编码器（没有变分）。</strong></p>
<a id="more"></a>
<h2 id="图自编码器gae">图自编码器（GAE）</h2>
<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-094210.png" /></p>
<p><strong>统一规范，规定几个 notation 如下：</strong></p>
<ul>
<li>图用 <span class="math inline">\(\mathcal{G} = (\mathcal{V}, \mathcal{E})\)</span> 表示，其中 <span class="math inline">\(\mathcal{V}\)</span> 表示节点集合，<span class="math inline">\(\mathcal{E}\)</span> 表示边集合</li>
<li><span class="math inline">\(\mathbf{A}\)</span>: 邻接矩阵</li>
<li><span class="math inline">\(\mathbf{D}\)</span>: 度矩阵</li>
<li><span class="math inline">\(N\)</span>: 节点数</li>
<li><span class="math inline">\(d\)</span>: 节点的特征（features）维度</li>
<li><span class="math inline">\(\mathbf{X} \in \Bbb{R}^{N \times d}\)</span>表示节点的特征矩阵</li>
<li><span class="math inline">\(f\)</span>: embedding 维度</li>
<li><span class="math inline">\(\mathbf{Z} \in \Bbb{R}^{N \times f}\)</span>: 节点的 embedding</li>
</ul>
<h3 id="encoder">Encoder</h3>
<p><strong>GAE 使用 GCN 作为 encoder</strong>，来得到节点的 latent representations（或者说 embedding），这个过程可用一行简短的公式表达：</p>
<p><span class="math display">\[\mathbf{Z} = \mathrm{GCN}(\mathbf{X}, \mathbf{A})\]</span></p>
<p>我们将 <span class="math inline">\(\mathrm{GCN}\)</span> 视为一个函数，然后将 <span class="math inline">\(\mathbf{X}\)</span> 和 <span class="math inline">\(\mathbf{A}\)</span> 作为输入，输入到 <span class="math inline">\(\mathrm{GCN}\)</span> 这个函数中，输出 <span class="math inline">\(\mathbf{Z} \in \Bbb{R}^{N×f}\)</span>，<span class="math inline">\(\mathbf{Z}\)</span> 代表的就是所有节点的 latent representations，或者说 embedding。</p>
<p>如何定义 <span class="math inline">\(\mathrm{GCN}\)</span> 这个函数？kipf 在论文中定义如下：</p>
<p><span class="math display">\[\mathrm{GCN}(\mathbf{X}, \mathbf{A}) = \tilde{\mathbf{A}} \mathrm{ReLU} (\tilde{\mathbf{A}}\mathbf{XW_0})\mathbf{W_1}\]</span></p>
<p>其中，<span class="math inline">\(\tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}\)</span>，<span class="math inline">\(\mathbf{W_0}\)</span> 和 <span class="math inline">\(\mathbf{W_1}\)</span> 是待学习的参数。</p>
<p>简言之，<strong>这里 <span class="math inline">\(\mathrm{GCN}\)</span> 就相当于一个以节点特征和邻接矩阵为输入、以节点 embedding 为输出的函数，目的只是为了得到 embedding</strong>。</p>
<h3 id="decoder">Decoder</h3>
<p><strong>GAE 采用 inner-product 作为 decoder 来重构（reconstruct）原始的图</strong>：</p>
<p><span class="math display">\[\hat{\mathbf{A}} = \sigma(\mathbf{Z} \mathbf{Z}^\mathrm{T})\]</span></p>
<p>上式中，<span class="math inline">\(\hat{\mathbf{A}}\)</span> 就是重构（reconstruct）出来的邻接矩阵。</p>
<h3 id="how-to-learn">How to learn</h3>
<p><strong>一个好的 <span class="math inline">\(\mathbf{Z}\)</span>，就应该使重构出的邻接矩阵与原始的邻接矩阵尽可能的相似</strong>，因为邻接矩阵决定了图的结构。因此，GAE 在训练过程中，采用交叉熵作为损失函数：</p>
<p><span class="math display">\[\mathcal{L} = - \frac{1}{N} \sum y \log \hat{y} + (1-y) \log (1-\hat{y})\]</span></p>
<p>上式中，<span class="math inline">\(y\)</span> 代表邻接矩阵 <span class="math inline">\(\mathbf{A}\)</span> 中某个元素的值（0 或 1），<span class="math inline">\(\hat{y}\)</span> 代表重构的邻接矩阵 <span class="math inline">\(\mathbf{\hat{A}}\)</span> 中相应元素的值（0 到 1 之间）。</p>
<p>从损失函数可以看出来，我们希望重构的邻接矩阵（或者说重构的图），与原始的邻接矩阵（或者说原始的图）越接近、越相似越好。</p>
<h3 id="小结">小结</h3>
<p><strong>图自编码器（GAE）的原理简明、清晰，训练起来不麻烦</strong>，因为可训练的参数只有 <span class="math inline">\(\mathbf{W_0}\)</span> 和 <span class="math inline">\(\mathbf{W_1}\)</span>，kipf 的代码实现中，这两个参数矩阵的维度分别是 <span class="math inline">\(d \times 32\)</span> 和 <span class="math inline">\(32 \times 16\)</span>，如果 <span class="math inline">\(d = 1000\)</span>，也只有 32512 个参数，相当少了。</p>
<p>下面讲 VGAE（<strong>变分</strong>图自编码器）。</p>
<h2 id="变分图自编码器vgae">变分图自编码器（VGAE）</h2>
<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-094333.png" /></p>
<p><strong>在 GAE 中，一旦 <span class="math inline">\(\mathrm{GCN}\)</span> 中的 <span class="math inline">\(\mathbf{W_0}\)</span> 和 <span class="math inline">\(\mathbf{W_1}\)</span> 确定了，那么 <span class="math inline">\(\mathrm{GCN}\)</span> 就是一个确定的函数，给定 <span class="math inline">\(\mathbf{X}\)</span> 和 <span class="math inline">\(\mathbf{A}\)</span>，输出的 <span class="math inline">\(\mathbf{Z}\)</span> 就是确定的</strong>。</p>
<p>而<strong>在 VGAE 中，<span class="math inline">\(\mathbf{Z}\)</span> 不再由一个确定的函数得到</strong>，而是从一个（多维）高斯分布中采样得到，说得更明确些，就是我们先通过 <span class="math inline">\(\mathrm{GCN}\)</span> 确定一个（多维）高斯分布，再从这个分布中采样得到 <span class="math inline">\(\mathbf{Z}\)</span>。下面简单描述一下这个过程。</p>
<h3 id="确定均值和方差">确定均值和方差</h3>
<p><strong>高斯分布可以唯一地由二阶矩确定</strong>。因此，想要确定一个高斯分布，我们只需要知道均值和方差。<strong>VGAE 利用 <span class="math inline">\(\mathrm{GCN}\)</span> 来分别计算均值和方差</strong>：</p>
<p><span class="math display">\[\mathbf{\mu} = \mathrm{GCN}_{\mu} (\mathbf{X}, \mathbf{A})\]</span></p>
<p><span class="math display">\[\log \mathbf{\sigma} = \mathrm{GCN}_{\sigma} (\mathbf{X}, \mathbf{A})\]</span></p>
<p>这里有两个要注意的地方，第一个是 <span class="math inline">\(\mathrm{GCN}\)</span> 的下标，<span class="math inline">\(\mathrm{GCN}_\mu\)</span> 和 <span class="math inline">\(\mathrm{GCN}_\sigma\)</span> 中的 <span class="math inline">\(\mathbf{W_0}\)</span> 是相同的、共享的，但 <span class="math inline">\(\mathbf{W_1}\)</span> 是不同的，因此用下标来作区分；第二个是通过 <span class="math inline">\(\mathrm{GCN}\_{\sigma}\)</span> 得到的是 <span class="math inline">\(\log \sigma\)</span>，这样可以方便后续的计算。</p>
<h3 id="采样">采样</h3>
<p>既然已经得到了均值和方差（准确来说应该是均值向量和协方差矩阵），就可以通过采样得到 <span class="math inline">\(\mathbf{Z}\)</span> 了，但是，<strong>采样操作无法提供梯度信息</strong>，也就是说，反向传播在采样操作无法计算梯度，也就无法更新 <span class="math inline">\(\mathbf{W_0}\)</span> 和 <span class="math inline">\(\mathbf{W_1}\)</span>。解决办法是<strong>重参数化（reparameterization）</strong>。<strong>了解（连续分布）重参数化的可以跳过下面一段。</strong></p>
<blockquote>
<p>简单说一下重参数化。大家应该知道，如果 <span class="math inline">\(\epsilon\)</span> 服从 <span class="math inline">\(\cal{N}(0, 1)\)</span>，那么 <span class="math inline">\(z = \mu + \epsilon \sigma\)</span> 就服从 <span class="math inline">\(\cal{N} (\mu, \sigma^2)\)</span>。因此，我们可以先从标准高斯中采样一个 <span class="math inline">\(\epsilon\)</span>，再通过 <span class="math inline">\(\mu + \epsilon \sigma\)</span> 计算出 <span class="math inline">\(z\)</span>，这样一来，<span class="math inline">\(z\)</span> 的表达式清晰可见，梯度信息也就有了。</p>
</blockquote>
<p><strong>VGAE 的 decoder 也是一个简单的 inner-product，与 GAE 的 decoder 没有区别。</strong></p>
<h3 id="how-to-learn-1">How to learn</h3>
<p>VGAE 依然希望重构出的图和原始的图尽可能相似，除此之外，还希望 <span class="math inline">\(\mathrm{GCN}\)</span> 计算出的分布与标准高斯尽可能相似。因此<strong>损失函数由交叉熵和 KL 散度两部分构成</strong>：</p>
<p><span class="math display">\[\cal{L} = 
\cal{L}_{\mathrm{CE}} \mathrm{+} \mathrm{KL[\mathit{q}(\mathbf{Z} | \mathbf{X}, \mathbf{A}) || \mathit{p}(\mathbf{Z})]}\]</span></p>
<p>其中，<span class="math inline">\(q(\mathbf{Z} | \mathbf{X}, \mathbf{A})\)</span> 是 <span class="math inline">\(\mathrm{GCN}\)</span> 计算出的分布，<span class="math inline">\(p(\mathbf{Z})\)</span> 是标准高斯。</p>
<p>不过，论文中的优化目标是这样的：</p>
<p><span class="math display">\[
\mathcal{L}=\mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{A})}[\log p(\mathbf{A} | \mathbf{Z})]-\operatorname{KL}[q(\mathbf{Z} | \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})]
\]</span></p>
<p>这是用变分下界写出的优化目标，第一项是期望（<strong>我看到有的博客说第一项是交叉熵，这不对</strong>），第二项是 KL 散度。</p>
<h2 id="实验效果">实验效果</h2>
<p>Kipf 和 Welling 在三个数据集上进行了效果分析，任务是链接预测，详情见下表：</p>
<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-084139.png" /></p>
<p>可见，（变分）图自编码器在三个数据集上的效果都很好，不过，<strong>注意带星号的 GAE* 和 VGAE*，这两个模型是不带 input features 的，就是说节点的 features 就是 one-hot 向量，这种情况下，（变分）图自编码器的效果不仅不比 SC 和 DW 好，甚至还有点差</strong>。不过，Kipf 和 Welling 在论文中指出，SC 和 DW 不支持 input features，就是说这是这两个方法本身的缺陷，就不能怪 Kipf 他们使用 input features 了。</p>
<hr />
<p>到这里，这篇介绍（变分）图自编码器的文章应该告一段落了。不过，<strong>关于 kipf 开源的代码，还有很多有意思的事情</strong>。</p>
<h2 id="analyze-the-code">Analyze the code</h2>
<h3 id="带权交叉熵">带权交叉熵</h3>
<p>不管是 GAE 还是 VGAE，损失函数中交叉熵的实现都使用的是这样一个 API：<code>tf.nn.weighted_cross_entropy_with_logits</code>，当我尝试把它换成 <code>tf.nn.sigmoid_cross_entropy_with_logits</code>（即最常用的交叉熵的定义）后，<strong>各项指标从 90 多下降到了 50 左右</strong>，所以有必要分析一下这个带权重的交叉熵在这里的意义。</p>
<p>带权交叉熵损失的数学定义如下：</p>
<p><span class="math display">\[- \mathrm{w} · y \log \hat{y} - (1 - y) \log (1 - \hat{y})\]</span></p>
<p>可以看到，与常用的交叉熵损失相比，只是在正样本的那一项前面乘了一个系数，这个系数是一个参数，是需要传进去的。可以来分析一下这个系数 <span class="math inline">\(\mathrm{w}\)</span> 大于 1 和大于 0 小于 1 对交叉熵损失的影响：</p>
<ul>
<li><span class="math inline">\(\mathrm{w} \gt 1\)</span>: 当模型去预测一个 true label 是正样本的样本时，如果模型误预测成了负样本，那么由于 <span class="math inline">\(\mathrm{w}\)</span> 大于 1，此时交叉熵损失会更大。换句话说，<span class="math inline">\(\mathrm{w} \gt 1\)</span> 使得模型分错一个正样本时受到的惩罚变大了，因此这会使模型更“专注”于对正样本的分类。说的再专业些，就是 <span class="math inline">\(\mathrm{w} \gt 1\)</span> 会减少 false negative 的数量，增加 true positive 的数量，这会使 recall 上升。</li>
<li><span class="math inline">\(0 \lt \mathrm{w} \lt 1\)</span>: 与 <span class="math inline">\(\mathrm{w} \gt 1\)</span> 相反，会减少 false positive 的数量，增加 true negative 的数量，使 precision 上升。</li>
</ul>
<p>什么情况下要设置一个 <span class="math inline">\(\mathrm{w}\)</span> ？没错，就是当训练集正负样本数量不平衡的时候。在一张图（graph）里，往往边的数量远少于节点数量，这会使邻接矩阵中 1 的数量远少于 0 的数量，即<strong>正样本的数量远少于负样本的数量</strong>，这种情况下，我们可以设定一个大于 1 的 <span class="math inline">\(\mathrm{w}\)</span>，让模型更“专注”于正样本的分类。可是如何设置一个比较好的 <span class="math inline">\(\mathrm{w}\)</span> 呢？可以这样，用负样本数与正样本数的比例作为 <span class="math inline">\(\mathrm{w}\)</span> 的值，因为这个比例如果大于 1，就说明正样本比负样本少，刚好可以用这个比例作为 <span class="math inline">\(\mathrm{w}\)</span> 的值，来让模型“专注”与正样本；如果这个比例小于 1，就说明正样本比负样本多，也可以用这个比例作为 <span class="math inline">\(\mathrm{w}\)</span> 的值。</p>
<p>下面是 kipf 的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pos_weight = <span class="built_in">float</span>(adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) / adj.<span class="built_in">sum</span>()</span><br><span class="line">norm = adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] / <span class="built_in">float</span>((adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) * <span class="number">2</span>)</span><br><span class="line">self.cost = norm * tf.reduce_mean(</span><br><span class="line">            tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))</span><br></pre></td></tr></table></figure>
<p><code>adj</code> 是邻接矩阵，<code>pos_weight</code> 就是上面说的系数 <span class="math inline">\(\mathrm{w}\)</span>，<code>adj.sum()</code> 就是正样本数，<code>adj.shape[0] * adj.shape[0] - adj.sum()</code> 就是负样本数，因此 <code>pos_weight</code> 就是负样本数/正样本数。</p>
<p><strong>另外，这里有一个 <code>norm</code></strong>，因为我们用负样本数/正样本数作为系数 <span class="math inline">\(\mathrm{w}\)</span>，相当于把正样本的数目增大到和负样本相同的数目，因此计算损失的时候，应该在总损失的基础上除以 2 倍的负样本数目，而不是原始的所有样本数 <span class="math inline">\(N\)</span>（正样本数 <span class="math inline">\(N_1\)</span>+负样本数 <span class="math inline">\(N_2\)</span>），而 <code>tf.reduce_mean</code> 相当于总损失乘上 <span class="math inline">\(\frac{1}{N}\)</span>，因此 <code>norm</code> 应该被设置成 <span class="math inline">\(\frac{N}{2 N_2}\)</span>，最后的效果就是总损失乘上 <span class="math inline">\(\frac{1}{2N_2}\)</span>。</p>
<p><strong>我觉得这样有点麻烦，直接把 <code>norm</code> 设置成 <span class="math inline">\(\frac{1}{2N_2}\)</span>，然后把 <code>tf.reduce_mean</code> 换成 <code>tf.reduce_sum</code> 就好了</strong>，即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">norm = <span class="number">1</span> / <span class="built_in">float</span>((adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) * <span class="number">2</span>)</span><br><span class="line">self.cost = norm * tf.reduce_sum(</span><br><span class="line">            tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))</span><br></pre></td></tr></table></figure>
<p><strong>我重新跑了一下代码，效果一样。</strong></p>
<h2 id="github-issues">Github Issues</h2>
<p><strong>说了这么多，图自编码器也有让人质疑的地方</strong>。下面是 Github 上的一些 issues：</p>
<ol type="1">
<li>重构的邻接矩阵中，TP 非常大，导致 recall（TP/(TP+FN)）很高，但 precision（TP/(TP+FP)）非常低，<span class="citation" data-cites="tkipf">[@tkipf]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-455966135) 建议找到一个合适的threshold，来平衡 recall 和 precision，但 <span class="citation" data-cites="zzheyu">[@zzheyu]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-456907910) 发现即使将 threshold 调整到 0.9 也不能解决这个问题。</li>
<li>另外，<span class="citation" data-cites="zzheyu">[@zzheyu]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-456907910) 认为论文中 VGAE 在 validation 和 test set 上表现出色的原因之一是 validation 和 test set 的数据非常 balance，即 half edges and half non-edges，为了印证这一点，他将 validation 和 test set 中 non-edges 的个数依次增加为之前的 5 倍、30 倍和 100 倍，发现 average precision score 随着 non-edges 的数目增多而大幅下降，同时，他发现 val/test sets 上的 F1 score 也非常低。对此，<span class="citation" data-cites="tkipf">[@tkipf]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-456912625) 表示可以尝试用负采样来解决这一问题。</li>
<li>此外，<span class="citation" data-cites="tkipf">@tkipf</span> 还表示，他们用 GAE 的变体在 knowledge base completion (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.06103">https://arxiv.org/abs/1703.06103</a>) 和 recommender system (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02263">https://arxiv.org/abs/1706.02263</a>) 进行了测试，实验效果很好。</li>
<li><span class="citation" data-cites="jlevy44">[@jlevy44]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-478657131) 评论说，他的 dataset 的 pos edges 的数量远少于 neg edges 的数量，他通过调整threshold 缓解了这一问题，但是，他发现模型 reconstruct 的邻接矩阵与原始邻接矩阵差异很大，他表示疑问。<span class="citation" data-cites="tkipf">[@tkipf]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-478707136) 回复说，可以把 score function 从 <span class="math inline">\(\text{sigmoid}(x^T, x)\)</span> 替换成 <span class="math inline">\(e^{-d(x, y)}\)</span>，其中 <span class="math inline">\(d(x, y)\)</span> 表示欧氏距离，他认为这样 reconstruct 的图多多少少会不那么密集。
<ol type="1">
<li>关于这一点，他建议 <span class="citation" data-cites="jlevy44">@jlevy44</span> 可以看"A Tutorial on Energy-Based Learning" by LeCun et al.，使用 <span class="math inline">\(e^{-d(x,y)}\)</span> 作为 score function，相当于在使用一个 energy-based model</li>
<li>关于在链接预测任务中使用 energy-based loss，可以参考 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.03815">https://arxiv.org/abs/1707.03815</a></li>
<li>使用内积作为 score function，会带来 graphons 问题（<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Graphon">https://en.wikipedia.org/wiki/Graphon</a>），这会使生成模型倾向于生成一些紧致的区块（dense blobs）</li>
</ol></li>
<li><span class="citation" data-cites="XuanHeIIIS">[@XuanHeIIIS]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-478915428) 询问 <span class="citation" data-cites="tkipf">@tkipf</span> 是否尝试过以减小 feature reconstruction loss 作为训练目标来训练 GCN，<span class="citation" data-cites="tkipf">[@tkipf]</span>(https://github.com/tkipf/gae/issues/20#issuecomment-478925040) 给出的方法是，将 decoder 的 output 输入到 node-level MLP 中，以重构node features；如果 features 是二元的值，可以用交叉熵作为损失，如果是连续的值，用 MSE也可以。</li>
</ol>
<h2 id="总结">总结</h2>
<p>用一句话总结图自编码器：<strong>利用 GCN 计算出节点的 embedding，再重构原始的图，要求重构的图与原始的图尽可能相似。</strong> 只不过，在这个过程中海油很多要考虑的细节，比如 <span class="math inline">\(\mathbf{Z}\)</span> 的先验，论文里使用的是高斯，但 kipf 明确指出高斯先验会带来新的问题，比如损失函数中，KL 散度前需要乘上一个系数，减小 KL 散度的影响，否则模型效果会下降，等等，还有很多值得商榷与改进的地方。众所周知，今年图神经网络是大势所趋，我们能否继续发现图自编码器的身影呢？</p>
<p><strong>References：</strong></p>
<p>[1] http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf</p>
<p>[2] https://github.com/tkipf/gae</p>
<p>[3] https://zhuanlan.zhihu.com/p/107854139</p>
	  
	</div>

	<!-- recommended posts -->
	

	<div>
  	<center>
	<div class="pagination">
<ul class="pagination">
	 
				
    	<li class="prev"><a href="/posts/undefined.html" class="alignleft prev"><i class="fa fa-arrow-circle-o-left"></i>Prev</a></li>
  		

        <li><a href="/archives"><i class="fa fa-archive"></i>Archive</a></li>

		
		   <li class="next"><a href="/posts/5d50ba61.html" class="alignright next">Next<i class="fa fa-arrow-circle-o-right"></i></a></li>         
        
	
</ul>
</div>

    </center>
	</div>

    <!-- share -->
    
        

        

    
	
	<!-- comment -->
	
<section id="comment">
  <h2 class="title">Comments</h2>
  
</section>


	</div> <!-- col-md-9/col-md-12 -->
		
	
	<div id="side_meta">
		<div class="col-md-3" id="post_meta"> 

	<!-- date -->
	
	<div class="meta-widget">
	<i class="fa fa-clock-o"></i>
	2020-02-17 
	</div>
	

	<!-- categories -->
    
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#categorys"><i class="fa fa-folder"></i></a>	
    <ul id="categorys" class="tag_box list-unstyled collapse in">
          
  <li>
    <li><a href="/categories/技术/">技术<span>35</span></a></li> <li><a href="/categories/技术/Machine-Learning/">Machine Learning<span>27</span></a></li>
  </li>

    </ul>
	</div>
	

	<!-- tags -->
	
	<div class="meta-widget">
	<a data-toggle="collapse" data-target="#tags"><i class="fa fa-tags"></i></a>		  
    <ul id="tags" class="tag_box list-unstyled collapse in">	  
	    
  <li><a href="/tags/Graph-Neural-Network/">Graph Neural Network<span>1</span></a></li>
    </ul>
	</div>
	

	<!-- toc -->
	<div class="meta-widget">
	
	   <a data-toggle="collapse" data-target="#toc"><i class="fa fa-bars"></i></a>
	   <div id="toc" class="toc collapse in">
			<ol class="toc-article"><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-article-text">介绍</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%9B%BE%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8gae"><span class="toc-article-text">图自编码器（GAE）</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#encoder"><span class="toc-article-text">Encoder</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#decoder"><span class="toc-article-text">Decoder</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#how-to-learn"><span class="toc-article-text">How to learn</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%B0%8F%E7%BB%93"><span class="toc-article-text">小结</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%8F%98%E5%88%86%E5%9B%BE%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8vgae"><span class="toc-article-text">变分图自编码器（VGAE）</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E7%A1%AE%E5%AE%9A%E5%9D%87%E5%80%BC%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-article-text">确定均值和方差</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E9%87%87%E6%A0%B7"><span class="toc-article-text">采样</span></a></li><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#how-to-learn-1"><span class="toc-article-text">How to learn</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C"><span class="toc-article-text">实验效果</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#analyze-the-code"><span class="toc-article-text">Analyze the code</span></a><ol class="toc-article-child"><li class="toc-article-item toc-article-level-3"><a class="toc-article-link" href="#%E5%B8%A6%E6%9D%83%E4%BA%A4%E5%8F%89%E7%86%B5"><span class="toc-article-text">带权交叉熵</span></a></li></ol></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#github-issues"><span class="toc-article-text">Github Issues</span></a></li><li class="toc-article-item toc-article-level-2"><a class="toc-article-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-article-text">总结</span></a></li></ol>
		</div>
	
	</div>
	
    <hr>
	
</div><!-- col-md-3 -->

	</div>
		

</div><!-- row -->



      
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    </div>
  </div>
  <div class="container-narrow">
    <footer> <p>
  &copy; 2021 LiuHDme
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
  </div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>


<!-- syntax highlighting -->


</body>
</html>