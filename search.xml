<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>贷款违约预测赛题理解</title>
      <link href="2020/09/15/2020/%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
      <url>2020/09/15/2020/%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>本次的比赛的任务是二分类，即预测用户贷款是否会违约。数据为结构化数据，一共有 47 个特征，因此对特征如何筛选、优化、组合应该会对结果产生很大影响，后续的特征工程需要仔细开展。数据量上，train 有 80 万，testA 和 testB 各 20 万，数据量不算大，可以先用传统机器学习模型，后续再上深度学习模型，然后考虑模型融合。</p><p>比赛地址：<a href="https://tianchi.aliyun.com/competition/entrance/531830/introduction">https://tianchi.aliyun.com/competition/entrance/531830/introduction</a></p><a id="more"></a><h2 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h2><p>这次赛题的指标是 AUC，AUC 越大，模型的预测性能越好，但 AUC 具体该如何计算，下面详细讲讲。</p><h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p>AUC 就是 ROC 的曲线下面积，而 ROC 是根据 fpr 和 tpr 得到的，因此需要先了解 fpr 和 tpr 的概念。</p><p>fpr 和 tpr 的定义如下面两个式子所示：(tp: true positive, tn: true negative, fp: false positive, fn: false negative)</p><script type="math/tex; mode=display">tpr = \frac{tp}{tp+fn}</script><script type="math/tex; mode=display">fpr = \frac{fp}{fp+tn}</script><p>其中，tpr 也叫 recall，意义是把正样本预测为正样本占所有正样本的比例，fpr 的意义是把负样本预测成正样本占所有负样本的比例。在计算 AUC 前，需要先得到 ROC，而得到 ROC 需要得到 fpr 和 tpr，上面的代码通过调用 <code>sklearn.metrics.roc_curve</code> 来计算 fpr 和 tpr。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">y = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>下面来模拟一下计算过程。</p><p>首先得到阈值为：<code>[1.8, 0.8, 0.4, 0.35, 0.1]</code>，即把最大预测值加 1 然后从大到小排序。</p><ol><li>阈值为 1.8 时，tp = 0, tn = 2, fp = 0, fn = 2，所以 tpr = 0, fpr = 0</li><li>阈值为 0.8 时，tp = 1, tn = 2, fp = 0, fn = 1，所以 tpr = 0.5, fpr = 0</li><li><p>阈值为 0.4 时，tp = 1, tn = 1, fp = 1, fn = 1，所以 tpr = 0.5, fpr = 0.5</p></li><li><p>阈值为 0.35 时，tp = 2, tn = 1, fp = 1, fn = 0，所以 tpr = 1, fpr = 0.5</p></li><li>阈值为 0.1 时，tp = 2, tn = 0, fp = 2, fn = 0，所以 tpr = 1, fpr = 1</li></ol><p>即最后有，<code>tpr = [0, 0.5, 0.5, 1, 1], fpr = [0, 0, 0.5, 0.5, 1]</code>，这和程序的输出是一致的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>fpr</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1.</span> ])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tpr</span><br><span class="line">array([<span class="number">0.</span> , <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">1.</span> , <span class="number">1.</span> ])</span><br></pre></td></tr></table></figure><p>根据上面的 tpr 和 fpr，可以画出 ROC 如下：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-09-15-073418.png" style="zoom:50%;" /></p><p>可以看出 AUC 应该是 0.75，验证一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>metrics.roc_auc_score(y, scores)</span><br><span class="line"><span class="number">0.75</span></span><br></pre></td></tr></table></figure><p>确实如此。</p><hr><h3 id="PR-CUrve"><a href="#PR-CUrve" class="headerlink" title="PR-CUrve"></a>PR-CUrve</h3><p>和 ROC 类似的还有 PR 曲线，就是 precision-recall 曲线，precision 和 recall 的定义如下：</p><script type="math/tex; mode=display">Precision = \frac{tp}{tp+fp}</script><script type="math/tex; mode=display">Recall = \frac{tp}{tp+fn}</script><p>Recall 就是计算 ROC 时提到的 tpr，precision 的意义是，你预测为正样本的所有样本中，有多少真的是正样本。显然，如果把所有样本都预测为正样本，那 recall 取到最大值 1，而 precision 会降低，至于多低取决于负样本的数量，负样本越多 precision 越低。</p><p>下面的代码调用 <code>sklearn.metrics.precision_recall_curve</code> 得到不同阈值下的 precision 和 recall</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line">precision, recall, thresholds = precision_recall_curve(y_true, y_scores)</span><br></pre></td></tr></table></figure><p>下面模拟计算过程。</p><p>首先和 ROC 不同，这里阈值只取 <code>[0.35, 0.4, 0.8]</code>，即去掉最小值然后从小到大排。</p><ol><li><p>阈值为 0.35 时，tp = 2, tn = 1, fp = 1, fn = 0，所以 recall = 1, precision = 2/3</p></li><li><p>阈值为 0.4 时，tp = 1, tn = 1, fp = 1, fn = 1，所以 recall = 0.5, precision = 0.5</p></li><li><p>阈值为 0.8 时，tp = 1, tn = 2, fp = 0, fn = 1，所以 recall = 0.5, precision = 1</p></li><li><p>阈值结束，最后的 recall = 0, precision = 1</p><blockquote><p>根据 sklearn 文档：The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.</p></blockquote></li></ol><p>最后有，<code>recall = [1, 0.5, 0.5, 0], precision = [2/3, 0.5, 1, 1]</code></p><p>画出 PR-curve 如下：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-09-15-075807.png" style="zoom:50%;" /></p><p>ROC 有曲线下面积 AUC，PR-curve 也有类似的概念：AP，即 average precision，类似于 PR-curve 的曲线下面积。</p><hr><h3 id="Average-Precision"><a href="#Average-Precision" class="headerlink" title="Average Precision"></a>Average Precision</h3><p>PR-Curve 对应的 AP 的是一个值，计算方式如下：</p><script type="math/tex; mode=display">AP = \sum_{n}(R_n - R_{n-1})P_n</script><p>其中 $R_n$ 表示第 $n$ 个阈值时的 recall，$P_n$ 表示第 $n$ 个阈值时的 precision</p><p>用刚刚计算的 PR-Curve 为例，计算其 AP 如下：</p><script type="math/tex; mode=display">AP = (1-0.5) \times \frac{2}{3} + (0.5-0.5) \times 0.5 + (0.5-0) \times 1 ≈ 0.83</script><p>用 <code>sklearn.metrics.average_precision_score</code> 验证一下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_true = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y_scores = np.array([<span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.35</span>, <span class="number">0.8</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>average_precision_score(y_true, y_scores)</span><br><span class="line"><span class="number">0.8333333333333333</span></span><br></pre></td></tr></table></figure><p>确实如此。</p><p>仔细观察 AP 的计算式，可以发现这是对 PR-Curve 下面积的近似，比如上面的式子其实是在计算下图被框出来的部分的面积：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-09-15-081631.jpg" style="zoom:25%;" /></p><hr><p>以上就是 ROC, AUC, PR-Curve 和 AP 的一些概念和计算细节，其中 AUC是 ROC 的曲线下面积，AP 是 PR-Curve 的下面积的近似，ROC 的纵轴是 tpr 也就是 recall，PR-Curve 的横轴是 recall，即 ROC 的纵轴和 PR-Curve 的横轴表示的量是相同的。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve</a></p><p>[2] <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html</a></p><p>[3] <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score">https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Data Competition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>追本溯源，带你读懂图自编码器</title>
      <link href="2020/02/17/2020/%E8%BF%BD%E6%9C%AC%E6%BA%AF%E6%BA%90%EF%BC%8C%E5%B8%A6%E4%BD%A0%E8%AF%BB%E6%87%82%E5%9B%BE%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/"/>
      <url>2020/02/17/2020/%E8%BF%BD%E6%9C%AC%E6%BA%AF%E6%BA%90%EF%BC%8C%E5%B8%A6%E4%BD%A0%E8%AF%BB%E6%87%82%E5%9B%BE%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Kipf 与 Welling 16 年发表了「Variational Graph Auto-Encoders」，提出了基于图的（变分）自编码器 <strong>Variational Graph Auto-Encoder（VGAE）</strong>。</p><p><strong>论文地址</strong>：<a href="http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf">http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf</a></p><p><strong>代码地址</strong>：<a href="https://github.com/tkipf/gae">https://github.com/tkipf/gae</a></p><p><strong>先介绍一下 intention 和用途</strong>。了解 embedding 概念的应该知道，获取一个合适的 embedding 来表示一张图中的节点不是一件容易的事，另外，如果我们能找到合适的 embedding，就能将它们用在上层任务中。VGAE 的用途之一就是通过 encoder-decoder 的结构来获取图中节点的 embedding。</p><p><strong>VGAE 的思想和变分自编码器（VAE）很像</strong>：利用隐变量（latent variables），让模型学习出一些分布（distribution），再从这些分布中采样得到 latent representations（或者说 embedding），<strong>这个过程是 encode 阶段</strong>，然后再利用得到的 latent representations 重构（reconstruct）出原始的图，<strong>这个过程是 decode 阶段</strong>。只不过，VGAE 的 encoder 使用了 GCN，decoder 是简单的内积（inner product）形式。</p><p><strong>下面具体讲解变分图自编码器（VGAE）。先讲 GAE，即图自编码器（没有变分）。</strong></p><a id="more"></a><h2 id="图自编码器（GAE）"><a href="#图自编码器（GAE）" class="headerlink" title="图自编码器（GAE）"></a>图自编码器（GAE）</h2><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-094210.png" alt=""></p><p><strong>统一规范，规定几个 notation 如下：</strong></p><ul><li>图用 $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ 表示，其中 $\mathcal{V}$ 表示节点集合，$\mathcal{E}$ 表示边集合</li><li>$\mathbf{A}$: 邻接矩阵</li><li>$\mathbf{D}$: 度矩阵</li><li>$N$: 节点数</li><li>$d$: 节点的特征（features）维度</li><li>$\mathbf{X} \in \Bbb{R}^{N \times d}$表示节点的特征矩阵</li><li>$f$: embedding 维度</li><li>$\mathbf{Z} \in \Bbb{R}^{N \times f}$: 节点的 embedding</li></ul><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>GAE 使用 GCN 作为 encoder</strong>，来得到节点的 latent representations（或者说 embedding），这个过程可用一行简短的公式表达：</p><script type="math/tex; mode=display">\mathbf{Z} = \mathrm{GCN}(\mathbf{X}, \mathbf{A})</script><p>我们将 $\mathrm{GCN}$ 视为一个函数，然后将 $\mathbf{X}$ 和 $\mathbf{A}$ 作为输入，输入到 $\mathrm{GCN}$ 这个函数中，输出 $\mathbf{Z} \in \Bbb{R}^{N×f}$，$\mathbf{Z}$ 代表的就是所有节点的 latent representations，或者说 embedding。</p><p>如何定义 $\mathrm{GCN}$ 这个函数？kipf 在论文中定义如下：</p><script type="math/tex; mode=display">\mathrm{GCN}(\mathbf{X}, \mathbf{A}) = \tilde{\mathbf{A}} \mathrm{ReLU} (\tilde{\mathbf{A}}\mathbf{XW_0})\mathbf{W_1}</script><p>其中，$\tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$，$\mathbf{W_0}$ 和 $\mathbf{W_1}$ 是待学习的参数。</p><p>简言之，<strong>这里 $\mathrm{GCN}$ 就相当于一个以节点特征和邻接矩阵为输入、以节点 embedding 为输出的函数，目的只是为了得到 embedding</strong>。</p><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p><strong>GAE 采用 inner-product 作为 decoder 来重构（reconstruct）原始的图</strong>：</p><script type="math/tex; mode=display">\hat{\mathbf{A}} = \sigma(\mathbf{Z} \mathbf{Z}^\mathrm{T})</script><p>上式中，$\hat{\mathbf{A}}$ 就是重构（reconstruct）出来的邻接矩阵。</p><h3 id="How-to-learn"><a href="#How-to-learn" class="headerlink" title="How to learn"></a>How to learn</h3><p><strong>一个好的 $\mathbf{Z}$，就应该使重构出的邻接矩阵与原始的邻接矩阵尽可能的相似</strong>，因为邻接矩阵决定了图的结构。因此，GAE 在训练过程中，采用交叉熵作为损失函数：</p><script type="math/tex; mode=display">\mathcal{L} = - \frac{1}{N} \sum y \log \hat{y} + (1-y) \log (1-\hat{y})</script><p>上式中，$y$ 代表邻接矩阵 $\mathbf{A}$ 中某个元素的值（0 或 1），$\hat{y}$ 代表重构的邻接矩阵 $\mathbf{\hat{A}}$ 中相应元素的值（0 到 1 之间）。</p><p>从损失函数可以看出来，我们希望重构的邻接矩阵（或者说重构的图），与原始的邻接矩阵（或者说原始的图）越接近、越相似越好。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>图自编码器（GAE）的原理简明、清晰，训练起来不麻烦</strong>，因为可训练的参数只有 $\mathbf{W_0}$ 和 $\mathbf{W_1}$，kipf 的代码实现中，这两个参数矩阵的维度分别是 $d \times 32$ 和 $32 \times 16$，如果 $d = 1000$，也只有 32512 个参数，相当少了。</p><p>下面讲 VGAE（<strong>变分</strong>图自编码器）。</p><h2 id="变分图自编码器（VGAE）"><a href="#变分图自编码器（VGAE）" class="headerlink" title="变分图自编码器（VGAE）"></a>变分图自编码器（VGAE）</h2><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-094333.png" alt=""></p><p><strong>在 GAE 中，一旦 $\mathrm{GCN}$ 中的 $\mathbf{W_0}$ 和 $\mathbf{W_1}$ 确定了，那么 $\mathrm{GCN}$ 就是一个确定的函数，给定 $\mathbf{X}$ 和 $\mathbf{A}$，输出的 $\mathbf{Z}$ 就是确定的</strong>。</p><p>而<strong>在 VGAE 中，$\mathbf{Z}$ 不再由一个确定的函数得到</strong>，而是从一个（多维）高斯分布中采样得到，说得更明确些，就是我们先通过 $\mathrm{GCN}$ 确定一个（多维）高斯分布，再从这个分布中采样得到 $\mathbf{Z}$。下面简单描述一下这个过程。</p><h3 id="确定均值和方差"><a href="#确定均值和方差" class="headerlink" title="确定均值和方差"></a>确定均值和方差</h3><p><strong>高斯分布可以唯一地由二阶矩确定</strong>。因此，想要确定一个高斯分布，我们只需要知道均值和方差。<strong>VGAE 利用 $\mathrm{GCN}$ 来分别计算均值和方差</strong>：</p><script type="math/tex; mode=display">\mathbf{\mu} = \mathrm{GCN}_{\mu} (\mathbf{X}, \mathbf{A})</script><script type="math/tex; mode=display">\log \mathbf{\sigma} = \mathrm{GCN}_{\sigma} (\mathbf{X}, \mathbf{A})</script><p>这里有两个要注意的地方，第一个是 $\mathrm{GCN}$ 的下标，$\mathrm{GCN}<em>{\mu}$ 和 $\mathrm{GCN}</em>{\sigma}$ 中的 $\mathbf{W<em>0}$ 是相同的、共享的，但 $\mathbf{W_1}$ 是不同的，因此用下标来作区分；第二个是通过 $\mathrm{GCN}</em>{\sigma}$ 得到的是 $\log \sigma$，这样可以方便后续的计算。</p><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>既然已经得到了均值和方差（准确来说应该是均值向量和协方差矩阵），就可以通过采样得到 $\mathbf{Z}$ 了，但是，<strong>采样操作无法提供梯度信息</strong>，也就是说，反向传播在采样操作无法计算梯度，也就无法更新 $\mathbf{W_0}$ 和 $\mathbf{W_1}$。解决办法是<strong>重参数化（reparameterization）</strong>。<strong>了解（连续分布）重参数化的可以跳过下面一段。</strong></p><blockquote><p>简单说一下重参数化。大家应该知道，如果 $\epsilon$ 服从 $\cal{N}(0, 1)$，那么 $z = \mu + \epsilon \sigma$ 就服从 $\cal{N} (\mu, \sigma^2)$。因此，我们可以先从标准高斯中采样一个 $\epsilon$，再通过 $\mu + \epsilon \sigma$ 计算出 $z$，这样一来，$z$ 的表达式清晰可见，梯度信息也就有了。</p></blockquote><p><strong>VGAE 的 decoder 也是一个简单的 inner-product，与 GAE 的 decoder 没有区别。</strong></p><h3 id="How-to-learn-1"><a href="#How-to-learn-1" class="headerlink" title="How to learn"></a>How to learn</h3><p>VGAE 依然希望重构出的图和原始的图尽可能相似，除此之外，还希望 $\mathrm{GCN}$ 计算出的分布与标准高斯尽可能相似。因此<strong>损失函数由交叉熵和 KL 散度两部分构成</strong>：</p><script type="math/tex; mode=display">\cal{L} = \cal{L}_{\mathrm{CE}} \mathrm{+} \mathrm{KL[\mathit{q}(\mathbf{Z} | \mathbf{X}, \mathbf{A}) || \mathit{p}(\mathbf{Z})]}</script><p>其中，$q(\mathbf{Z} | \mathbf{X}, \mathbf{A})$ 是 $\mathrm{GCN}$ 计算出的分布，$p(\mathbf{Z})$ 是标准高斯。</p><p>不过，论文中的优化目标是这样的：</p><script type="math/tex; mode=display">\mathcal{L}=\mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{A})}[\log p(\mathbf{A} | \mathbf{Z})]-\operatorname{KL}[q(\mathbf{Z} | \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})]</script><p>这是用变分下界写出的优化目标，第一项是期望（<strong>我看到有的博客说第一项是交叉熵，这不对</strong>），第二项是 KL 散度。</p><h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p>Kipf 和 Welling 在三个数据集上进行了效果分析，任务是链接预测，详情见下表：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-084139.png" alt=""></p><p>可见，（变分）图自编码器在三个数据集上的效果都很好，不过，<strong>注意带星号的 GAE* 和 VGAE*，这两个模型是不带 input features 的，就是说节点的 features 就是 one-hot 向量，这种情况下，（变分）图自编码器的效果不仅不比 SC 和 DW 好，甚至还有点差</strong>。不过，Kipf 和 Welling 在论文中指出，SC 和 DW 不支持 input features，就是说这是这两个方法本身的缺陷，就不能怪 Kipf 他们使用 input features 了。</p><hr><p>到这里，这篇介绍（变分）图自编码器的文章应该告一段落了。不过，<strong>关于 kipf 开源的代码，还有很多有意思的事情</strong>。</p><h2 id="Analyze-the-code"><a href="#Analyze-the-code" class="headerlink" title="Analyze the code"></a>Analyze the code</h2><h3 id="带权交叉熵"><a href="#带权交叉熵" class="headerlink" title="带权交叉熵"></a>带权交叉熵</h3><p>不管是 GAE 还是 VGAE，损失函数中交叉熵的实现都使用的是这样一个 API：<code>tf.nn.weighted_cross_entropy_with_logits</code>，当我尝试把它换成 <code>tf.nn.sigmoid_cross_entropy_with_logits</code>（即最常用的交叉熵的定义）后，<strong>各项指标从 90 多下降到了 50 左右</strong>，所以有必要分析一下这个带权重的交叉熵在这里的意义。</p><p>带权交叉熵损失的数学定义如下：</p><script type="math/tex; mode=display">- \mathrm{w} · y \log \hat{y} - (1 - y) \log (1 - \hat{y})</script><p>可以看到，与常用的交叉熵损失相比，只是在正样本的那一项前面乘了一个系数，这个系数是一个参数，是需要传进去的。可以来分析一下这个系数 $\mathrm{w}$ 大于 1 和大于 0 小于 1 对交叉熵损失的影响：</p><ul><li>$\mathrm{w} \gt 1$: 当模型去预测一个 true label 是正样本的样本时，如果模型误预测成了负样本，那么由于 $\mathrm{w}$ 大于 1，此时交叉熵损失会更大。换句话说，$\mathrm{w} \gt 1$ 使得模型分错一个正样本时受到的惩罚变大了，因此这会使模型更“专注”于对正样本的分类。说的再专业些，就是 $\mathrm{w} \gt 1$ 会减少 false negative 的数量，增加 true positive 的数量，这会使 recall 上升。</li><li>$0 \lt \mathrm{w} \lt 1$: 与 $\mathrm{w} \gt 1$ 相反，会减少 false positive 的数量，增加 true negative 的数量，使 precision 上升。</li></ul><p>什么情况下要设置一个 $\mathrm{w}$ ？没错，就是当训练集正负样本数量不平衡的时候。在一张图（graph）里，往往边的数量远少于节点数量，这会使邻接矩阵中 1 的数量远少于 0 的数量，即<strong>正样本的数量远少于负样本的数量</strong>，这种情况下，我们可以设定一个大于 1 的 $\mathrm{w}$，让模型更“专注”于正样本的分类。可是如何设置一个比较好的 $\mathrm{w}$ 呢？可以这样，用负样本数与正样本数的比例作为 $\mathrm{w}$ 的值，因为这个比例如果大于 1，就说明正样本比负样本少，刚好可以用这个比例作为 $\mathrm{w}$ 的值，来让模型“专注”与正样本；如果这个比例小于 1，就说明正样本比负样本多，也可以用这个比例作为 $\mathrm{w}$ 的值。</p><p>下面是 kipf 的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pos_weight = <span class="built_in">float</span>(adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) / adj.<span class="built_in">sum</span>()</span><br><span class="line">norm = adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] / <span class="built_in">float</span>((adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) * <span class="number">2</span>)</span><br><span class="line">self.cost = norm * tf.reduce_mean(</span><br><span class="line">            tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))</span><br></pre></td></tr></table></figure><p><code>adj</code> 是邻接矩阵，<code>pos_weight</code> 就是上面说的系数 $\mathrm{w}$，<code>adj.sum()</code> 就是正样本数，<code>adj.shape[0] * adj.shape[0] - adj.sum()</code> 就是负样本数，因此 <code>pos_weight</code> 就是负样本数/正样本数。</p><p><strong>另外，这里有一个 <code>norm</code></strong>，因为我们用负样本数/正样本数作为系数 $\mathrm{w}$，相当于把正样本的数目增大到和负样本相同的数目，因此计算损失的时候，应该在总损失的基础上除以 2 倍的负样本数目，而不是原始的所有样本数 $N$（正样本数 $N_1$+负样本数 $N_2$），而 <code>tf.reduce_mean</code> 相当于总损失乘上 $\frac{1}{N}$，因此 <code>norm</code> 应该被设置成 $\frac{N}{2 N_2}$，最后的效果就是总损失乘上 $\frac{1}{2N_2}$。</p><p><strong>我觉得这样有点麻烦，直接把 <code>norm</code> 设置成 $\frac{1}{2N_2}$，然后把 <code>tf.reduce_mean</code> 换成 <code>tf.reduce_sum</code> 就好了</strong>，即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">norm = <span class="number">1</span> / <span class="built_in">float</span>((adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) * <span class="number">2</span>)</span><br><span class="line">self.cost = norm * tf.reduce_sum(</span><br><span class="line">            tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))</span><br></pre></td></tr></table></figure><p><strong>我重新跑了一下代码，效果一样。</strong></p><h2 id="Github-Issues"><a href="#Github-Issues" class="headerlink" title="Github Issues"></a>Github Issues</h2><p><strong>说了这么多，图自编码器也有让人质疑的地方</strong>。下面是 Github 上的一些 issues：</p><ol><li>重构的邻接矩阵中，TP 非常大，导致 recall（TP/(TP+FN)）很高，但 precision（TP/(TP+FP)）非常低，<a href="https://github.com/tkipf/gae/issues/20#issuecomment-455966135">@tkipf</a> 建议找到一个合适的threshold，来平衡 recall 和 precision，但 <a href="https://github.com/tkipf/gae/issues/20#issuecomment-456907910">@zzheyu</a> 发现即使将 threshold 调整到 0.9 也不能解决这个问题。</li><li>另外，<a href="https://github.com/tkipf/gae/issues/20#issuecomment-456907910">@zzheyu</a> 认为论文中 VGAE 在 validation 和 test set 上表现出色的原因之一是 validation 和 test set 的数据非常 balance，即 half edges and half non-edges，为了印证这一点，他将 validation 和 test set 中 non-edges 的个数依次增加为之前的 5 倍、30 倍和 100 倍，发现 average precision score 随着 non-edges 的数目增多而大幅下降，同时，他发现 val/test sets 上的 F1 score 也非常低。对此，<a href="https://github.com/tkipf/gae/issues/20#issuecomment-456912625">@tkipf</a> 表示可以尝试用负采样来解决这一问题。</li><li>此外，@tkipf 还表示，他们用 GAE 的变体在 knowledge base completion (<a href="https://arxiv.org/abs/1703.06103">https://arxiv.org/abs/1703.06103</a>) 和 recommender system (<a href="https://arxiv.org/abs/1706.02263">https://arxiv.org/abs/1706.02263</a>) 进行了测试，实验效果很好。</li><li><a href="https://github.com/tkipf/gae/issues/20#issuecomment-478657131">@jlevy44</a> 评论说，他的 dataset 的 pos edges 的数量远少于 neg edges 的数量，他通过调整threshold 缓解了这一问题，但是，他发现模型 reconstruct 的邻接矩阵与原始邻接矩阵差异很大，他表示疑问。<a href="https://github.com/tkipf/gae/issues/20#issuecomment-478707136">@tkipf</a> 回复说，可以把 score function 从 $\text{sigmoid}(x^T, x)$ 替换成 $e^{-d(x, y)}$，其中 $d(x, y)$ 表示欧氏距离，他认为这样 reconstruct 的图多多少少会不那么密集。<ol><li>关于这一点，他建议 @jlevy44 可以看”A Tutorial on Energy-Based Learning” by LeCun et al.，使用 $e^{-d(x,y)}$ 作为 score function，相当于在使用一个 energy-based model</li><li>关于在链接预测任务中使用 energy-based loss，可以参考 <a href="https://arxiv.org/abs/1707.03815">https://arxiv.org/abs/1707.03815</a></li><li>使用内积作为 score function，会带来 graphons 问题（<a href="https://en.wikipedia.org/wiki/Graphon">https://en.wikipedia.org/wiki/Graphon</a>），这会使生成模型倾向于生成一些紧致的区块（dense blobs）</li></ol></li><li><a href="https://github.com/tkipf/gae/issues/20#issuecomment-478915428">@XuanHeIIIS</a> 询问 @tkipf 是否尝试过以减小 feature reconstruction loss 作为训练目标来训练 GCN，<a href="https://github.com/tkipf/gae/issues/20#issuecomment-478925040">@tkipf</a> 给出的方法是，将 decoder 的 output 输入到 node-level MLP 中，以重构node features；如果 features 是二元的值，可以用交叉熵作为损失，如果是连续的值，用 MSE也可以。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用一句话总结图自编码器：<strong>利用 GCN 计算出节点的 embedding，再重构原始的图，要求重构的图与原始的图尽可能相似。</strong> 只不过，在这个过程中海油很多要考虑的细节，比如 $\mathbf{Z}$ 的先验，论文里使用的是高斯，但 kipf 明确指出高斯先验会带来新的问题，比如损失函数中，KL 散度前需要乘上一个系数，减小 KL 散度的影响，否则模型效果会下降，等等，还有很多值得商榷与改进的地方。众所周知，今年图神经网络是大势所趋，我们能否继续发现图自编码器的身影呢？</p><p><strong>References：</strong></p><p>[1] <a href="http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf">http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf</a></p><p>[2] <a href="https://github.com/tkipf/gae">https://github.com/tkipf/gae</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/107854139">https://zhuanlan.zhihu.com/p/107854139</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graph Neural Network </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于相对熵的推导证明和推论</title>
      <link href="2019/12/14/2019/%E5%85%B3%E4%BA%8E%E7%9B%B8%E5%AF%B9%E7%86%B5%E7%9A%84%E6%8E%A8%E5%AF%BC%E8%AF%81%E6%98%8E%E5%92%8C%E6%8E%A8%E8%AE%BA/"/>
      <url>2019/12/14/2019/%E5%85%B3%E4%BA%8E%E7%9B%B8%E5%AF%B9%E7%86%B5%E7%9A%84%E6%8E%A8%E5%AF%BC%E8%AF%81%E6%98%8E%E5%92%8C%E6%8E%A8%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<p>相对熵可以衡量两个分布之间的不相似性，即 P 和 Q 两个分布越相似，相对熵越小，否则越大。</p><p>相对熵的公式是</p><script type="math/tex; mode=display">D_{KL}(\mathrm{P} || \mathrm{Q}) = \sum_{i} p_i \log{\frac{p_i}{q_i}}</script><p>显然，当 P 和 Q 的分布完全相同时，log 里就是 1，所以每项都是 0，相对熵也为 0，印证了开头的那句话。</p><p>关于相对熵有个不等式：</p><script type="math/tex; mode=display">D_{KL}(\mathrm{P} || \mathrm{Q}) \geq 0</script><p>下面给出两种证明方法：</p><ol><li><p>由于在 $x \in (0, 1] $ 时，有 $\ln(x) \leq x-1$ 当且仅当 $x=1$ 时等号成立，因此有  </p><script type="math/tex; mode=display">-D_{KL}(\mathrm{P}||\mathrm{Q}) = \sum_i p_i \log{\frac{q_i}{p_i}} \leq \sum_i p_i (\frac{q_i}{p_i} - 1) = \sum_i (q_i - p_i) = \sum_i q_i - \sum_i p_i = 0</script><p>所以有 $D_{KL}(P || Q) \geq 0$</p></li><li><p>根据 Jensen 不等式，由于 $\log(x)$ 是一个严格的 concave 函数，所以有  </p><script type="math/tex; mode=display">-D_{KL}(\mathrm{P}||\mathrm{Q}) = \sum_i p_i \log{\frac{q_i}{p_i}} \leq \log \sum_i p_i \frac{q_i}{p_i} = \log \sum_i q_i = 0</script><p>所以有 $D_{KL}(P || Q) \geq 0$</p></li></ol><p>如果我们把相对熵公式展开，会得到</p><script type="math/tex; mode=display">D_{KL}(\mathrm{P} || \mathrm{Q}) = \sum_i p_i \log{p_i} - \sum_i p_i \log{q_i} = \mathbf{H} (\mathrm{P}, \mathrm{Q}) - \mathbf{H} (\mathrm{P}) \geq 0</script><p>我们就得到了 Gibbis 不等式：</p><script type="math/tex; mode=display">-\sum_i p_i \log{q_i} \geq -\sum_i p_i \log{p_i}</script><p>即因为相对熵 = 交叉熵 - 熵且其大于等于 0，故有交叉熵 ≥ 熵</p><p>相对熵又叫 KL 散度，或信息增益，也称信息散度。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Information Theory </tag>
            
            <tag> Probability theory </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>scipy 稀疏矩阵小结</title>
      <link href="2019/11/09/2019/scipy%20%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5%E5%B0%8F%E7%BB%93/"/>
      <url>2019/11/09/2019/scipy%20%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5%E5%B0%8F%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="coo-matrix"><a href="#coo-matrix" class="headerlink" title="coo_matrix"></a>coo_matrix</h1><p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html</a></p><p>坐标（COOrdinate）形式的稀疏矩阵  </p><p><strong>几种创建方法：</strong><br><code>coo_matrix(D)</code><br><code>coo_matrix((M, N), [dtype])</code> # (M, N) 的空阵<br><code>coo_matrix((data, (i, j)), [shape=(M, N)])</code></p><p><strong>优点：</strong></p><ul><li>快速和 CSR/CSC 形式互相转换  </li><li>允许重复元素（见下面例子）</li></ul><p><strong>缺点：</strong></p><ul><li>不直接支持科学算术和切片操作</li></ul><p><strong>用途：</strong></p><ul><li>快速创建稀疏矩阵  </li><li>转换到 CSR 或 CSC 形式以进行快速科学算术以及矩阵、向量操作  </li><li>转换到 CSR 或 CSC 形式时，重复的 (i, j) 元素会加在一起（见下面例子）</li></ul><p><strong>例子：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 构建空阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> coo_matrix</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>coo_matrix((<span class="number">3</span>, <span class="number">4</span>), dtype=np.int8).toarray()</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], dtype=int8)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 用 ijv 形式构建矩阵</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>row  = np.array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>col  = np.array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>coo_matrix((data, (row, col)), shape=(<span class="number">4</span>, <span class="number">4</span>)).toarray()</span><br><span class="line">array([[<span class="number">4</span>, <span class="number">0</span>, <span class="number">9</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">7</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 坐标有重复</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>row  = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>col  = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>coo = coo_matrix((data, (row, col)), shape=(<span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 重复元素会保留</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.<span class="built_in">max</span>(coo.data)</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># 重复元素被加起来</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>coo.toarray()</span><br><span class="line">array([[<span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="csr-matrix"><a href="#csr-matrix" class="headerlink" title="csr_matrix"></a>csr_matrix</h1><p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html</a></p><p>Compressed Sparse Row matrix</p><p><strong>几种创建方法：</strong></p><p><code>csr_matrix(D)</code><br><code>csr_matrix((M, N), [dtype])</code> # (M, N) 的空阵<br><code>csr_matrix((data, (row_ind, col_ind)), [shape=(M, N)])</code>  # a[row_ind[k], col_ind[k]] = data[k]<br><code>csr_matrix((data, indices, indptr), [shape=(M, N)])</code> # 标准的 CSR 表示形式，第 i 行的非零元素的所在列是 indices[indptr[i]:indptr[i+1]]，相应的元素是 data[indptr[i]:indptr[i+1]]</p><p><strong>优点：</strong></p><ul><li>科学计算效率高</li><li>行切片效率高</li><li>矩阵乘法效率高</li></ul><p><strong>缺点：</strong></p><ul><li>列切片效率低（用 CSC）</li><li>元素赋值效率低（用 LIL 或 DOK）</li></ul><p><strong>例子：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csr_matrix</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csr_matrix((<span class="number">3</span>, <span class="number">4</span>), dtype=np.int8).toarray()</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], dtype=int8)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>row = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>col = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csr_matrix((data, (row, col)), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>indptr = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csr_matrix((data, indices, indptr), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure><p>CSR 的重复元素也会累加：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>docs = [[<span class="string">&quot;hello&quot;</span>, <span class="string">&quot;world&quot;</span>, <span class="string">&quot;hello&quot;</span>], [<span class="string">&quot;goodbye&quot;</span>, <span class="string">&quot;cruel&quot;</span>, <span class="string">&quot;world&quot;</span>]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indptr = [<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocabulary = &#123;&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> d <span class="keyword">in</span> docs:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> term <span class="keyword">in</span> d:</span><br><span class="line"><span class="meta">... </span>        index = vocabulary.setdefault(term, <span class="built_in">len</span>(vocabulary))</span><br><span class="line"><span class="meta">... </span>        indices.append(index)</span><br><span class="line"><span class="meta">... </span>        data.append(<span class="number">1</span>)</span><br><span class="line"><span class="meta">... </span>    indptr.append(<span class="built_in">len</span>(indices))</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csr_matrix((data, indices, indptr), dtype=<span class="built_in">int</span>).toarray()</span><br><span class="line">array([[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure><h1 id="csc-matrix"><a href="#csc-matrix" class="headerlink" title="csc_matrix"></a>csc_matrix</h1><p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html</a></p><p>Compressed Sparse Column matrix</p><p><strong>几种创建方法：</strong></p><p><code>csc_matrix(D)</code><br><code>csc_matrix(S)</code><br><code>csc_matrix((M, N), [dtype])</code><br><code>csc_matrix((data, (row_ind, col_ind)), [shape=(M, N)])</code><br><code>csc_matrix((data, indices, indptr), [shape=(M, N)])</code> # CSC 的标准表示形式，第 i 列的非零元素所在行是 indices[indptr[i]:indptr[i+1]]，相应元素是 data[indptr[i]:indptr[i+1]]</p><p><strong>优点：</strong></p><ul><li>科学计算效率高</li><li>列切片效率高</li><li>矩阵乘法速度快（CSR、BSR 可能更快）</li></ul><p><strong>缺点：</strong></p><ul><li>行切片效率低（用 CSR）</li><li>元素赋值效率低（用 LIL 或 DOK）</li></ul><p><strong>例子：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> csc_matrix</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csc_matrix((<span class="number">3</span>, <span class="number">4</span>), dtype=np.int8).toarray()</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], dtype=int8)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>row = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>col = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csc_matrix((data, (row, col)), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>indptr = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = np.array([<span class="number">0</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>csc_matrix((data, indices, indptr), shape=(<span class="number">3</span>, <span class="number">3</span>)).toarray()</span><br><span class="line">array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">0</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure><h1 id="dok-matrix"><a href="#dok-matrix" class="headerlink" title="dok_matrix"></a>dok_matrix</h1><p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.dok_matrix.html</a></p><p>Dictionary Of Keys based sparse matrix.</p><p><strong>几种构建方式：</strong></p><p><code>dok_matrix(D)</code><br><code>dok_matrix((M,N), [dtype])</code></p><p><strong>特点：</strong></p><ul><li>访问某个元素的时间复杂度为 O(1)</li><li>转换到 coo_matrix 效率高</li></ul><p><strong>例子：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> scipy.sparse <span class="keyword">import</span> dok_matrix</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>S = dok_matrix((<span class="number">5</span>, <span class="number">5</span>), dtype=np.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line"><span class="meta">... </span>        S[i, j] = i + j    <span class="comment"># Update element</span></span><br></pre></td></tr></table></figure><h1 id="lil-matrix"><a href="#lil-matrix" class="headerlink" title="lil_matrix"></a>lil_matrix</h1><p><a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.lil_matrix.html</a></p><p>Row-based linked list sparse matrix</p><p><strong>几种构建方法：</strong></p><p><code>lil_matrix(D)</code><br><code>lil_matrix((M, N), [dtype])</code></p><p><strong>优点：</strong></p><ul><li>可以灵活切片</li><li>元素赋值效率高</li></ul><p><strong>缺点：</strong></p><ul><li>LIL + LIL 效率低（用 CSR 或 CSC）</li><li>列切片效率低（用 CSC）</li><li>矩阵乘法效率低（用 CSR 或 CSC）</li></ul><p><strong>一般用法：</strong></p><ul><li>快速构建稀疏矩阵</li><li>转化到 CSR 或 CSC 形式</li><li>矩阵很大时最好用 COO 形式</li></ul><p><strong>例子：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> scipy.sparse <span class="keyword">as</span> sp</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lil = sp.lil_matrix(np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">5</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lil.rows</span><br><span class="line">array([<span class="built_in">list</span>([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]), <span class="built_in">list</span>([<span class="number">1</span>, <span class="number">3</span>])], dtype=<span class="built_in">object</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>lil.data</span><br><span class="line">array([<span class="built_in">list</span>([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), <span class="built_in">list</span>([<span class="number">3</span>, <span class="number">5</span>])], dtype=<span class="built_in">object</span>)</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>快速构建稀疏矩阵用 COO 、DOK 或 LIL 形式</li><li>给元素赋值用 DOK 或 LIL 形式</li><li>进行科学计算用 CSC 或 CSR 形式</li><li>行切片用 CSR，列切片用 CSC</li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Scipy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reasoning Over Knowledge Graph Paths for Recommendation</title>
      <link href="2019/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Reasoning%20Over%20Knowledge%20Graph%20Paths%20for%20Recommendation/"/>
      <url>2019/09/28/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Reasoning%20Over%20Knowledge%20Graph%20Paths%20for%20Recommendation/</url>
      
        <content type="html"><![CDATA[<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Propose <strong>Knowledge-aware Path Recurrent Network (KPRN)</strong> which can generate paths representations by composing the semantics of both entities and relations. Two main main keys:</p><ul><li>adopting LSTM to capture sequential dependencies </li><li>adopting a weighted pooling operation to discriminate different contributions of different paths, increasing explainability. </li></ul><h2 id="Research-Objective"><a href="#Research-Objective" class="headerlink" title="Research Objective"></a>Research Objective</h2><p>Contribute a KG based model to exploit knowledge-graph for recommendation and analyze the influence of different setting of the proposed model.</p><h2 id="Background-and-Problems"><a href="#Background-and-Problems" class="headerlink" title="Background and Problems"></a>Background and Problems</h2><p>Recommender system incorporated with KG has attracted increasing attention these years, however existing efforts have not fully explored this connectivity to infer user preferences, especially in terms of modeling the sequential dependencies within and holistic semantics of a path. Following are two existing efforts: </p><ul><li><p><strong>Path-based models</strong> (Yu et al. 2014; 2013; Gao et al. 2018): introduce <strong>meta-paths</strong> to refine the similarities between users and items.</p><ul><li>hardly specify the holistic semantics of paths, especially when similar entities but different relations are involved in a meta-path.</li><li>fail to automatically uncover and reason on unseen connectivity patterns, since meta-paths requires <strong>domain knowledge</strong> to be predefined.</li></ul></li><li><p><strong>Knowledge graph embedding</strong> (KGE) (TransE (Bordes et al. 2013), TransR (Lin et al. 2015)): regularize the representations of items.</p><ul><li>only considers direct relations between entities, rather than multiple-hop relation paths.</li><li>the characterization of user-item connectivity is achieved in a rather implicit way, that is, to guide the representation learning, but not to infer a user’s preference.</li></ul></li></ul><h2 id="Method-s"><a href="#Method-s" class="headerlink" title="Method(s)"></a>Method(s)</h2><ul><li>In terms of reasoning, the method should model sequential dependencies.</li><li>In terms of explainability, the method should discriminate different contributions of different paths, when inferring user interests.</li></ul><p>Firstly, extract qualified paths between a user-item pair from KG, each of which consists of the related entities and relations. Then adopt a <strong>LSTM</strong> network to model the sequential dependencies. Thereafter, a pooling operation if performed to aggregate the representations of paths to obtain prediction signal of the user-item pair.</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-09-28-130645.png" alt=""></p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><ul><li>Dataset: MI (MovieLens-1M + IMDB) (Movie), KKBox (Music)</li><li>Compare their proposed model’s performance with others’ on the above two datasets.</li><li>Evaluation metrics: hit@K, ndcg@K, K={1, 2, …, 15}</li><li>Use grid search to find learning rate and the coefficient of L2 regularization. (lr = {0.001, 0.002, 0.01, 0.02}, L2={10^−5, 10^−4, 10^−3, 10^−2})</li><li>Explore the influence of relation in paths by discarding relation embedding to generate the input vector.</li><li>Demonstrate reasoning capacity and explainability by visualizing a subgraph as an example.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><ul><li>By adopting LSTM, the model can capture the sequential dependencies of elements and reason on paths to infer user preferences.</li><li>By adopting a pooling operation, the model can discriminate different contributions of different paths when inferring user interests, which also benefits the explainability of the model.</li><li>We can mimic the propagation process of user preferences within KG via <strong>Graph Neural Network</strong>, since extracting qualified paths is labor-intensive.</li><li>We can adopt zero-shot learning to solve the cold start issues in the large domain, since KG can links multiple domains together with overlapped entities.</li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Bordes, A.; Usunier, N.; García-Durían, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In <em>NIPS</em>, 2787–2795.</p><p>Gao, L.; Yang, H.; Wu, J.; Zhou, C.; Lu, W.; and Hu, Y. 2018. Recommendation with multi-source heterogeneous information. In <em>IJCAI</em>, 3378–3384.</p><p>Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015. Learn- ing entity and relation embeddings for knowledge graph completion. In <em>AAAI</em>, 2181–2187.</p><p>Yu, X.; Ren, X.; Gu, Q.; Sun, Y.; and Han, J. 2013. Collab- orative filtering with entity similarity regularization in het- erogeneous information networks. <em>IJCAI</em> 27.</p><p>Yu, X.; Ren, X.; Sun, Y.; Gu, Q.; Sturt, B.; Khandelwal, U.; Norick, B.; and Han, J. 2014. Personalized entity recom- mendation: a heterogeneous information network approach. In <em>WSDM</em>, 283–292.</p><p><em><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4470">link for this paper</a></em></p>]]></content>
      
      
      <categories>
          
          <category> Paper Notes </category>
          
          <category> Recommender System </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Recommender System </tag>
            
            <tag> Knowledge Graph </tag>
            
            <tag> Paper Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>风格迁移-TensorFlow</title>
      <link href="2019/02/02/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%204/neural_style_transfer_tensorflow/"/>
      <url>2019/02/02/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%204/neural_style_transfer_tensorflow/</url>
      
        <content type="html"><![CDATA[<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-08-063511.png" alt=""></p><p>这篇文章使用 TensorFlow 实现风格迁移。</p><a id="more"></a><h2 id="导包"><a href="#导包" class="headerlink" title="导包"></a>导包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> nst_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><h2 id="定义-Cost"><a href="#定义-Cost" class="headerlink" title="定义 Cost"></a>定义 Cost</h2><h3 id="Content-Cost"><a href="#Content-Cost" class="headerlink" title="Content Cost"></a>Content Cost</h3><script type="math/tex; mode=display">J_{content}(C,G) =  \frac{1}{4 \times n_H \times n_W \times n_C}\sum _{ \text{all entries}} (a^{(C)} - a^{(G)})^2\tag{1}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_content_cost</span>(<span class="params">a_C, a_G</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    a_G -- 图片G的隐藏层，(1, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    a_C -- 图片C的隐藏层，(1, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取维度</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 展开</span></span><br><span class="line">    a_C_unrolled = tf.transpose(tf.reshape(a_C, [n_H*n_W, n_C]))</span><br><span class="line">    a_G_unrolled = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算</span></span><br><span class="line">    J_content = tf.reduce_sum((a_C - a_G) ** <span class="number">2</span>) / (<span class="number">4</span> * n_H * n_W * n_C)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_content</span><br></pre></td></tr></table></figure><h3 id="Style-Cost"><a href="#Style-Cost" class="headerlink" title="Style Cost"></a>Style Cost</h3><h4 id="格拉姆矩阵（style-martix）"><a href="#格拉姆矩阵（style-martix）" class="headerlink" title="格拉姆矩阵（style martix）"></a>格拉姆矩阵（style martix）</h4><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-07-114408.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gram_matrix</span>(<span class="params">A</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算格拉姆矩阵</span></span><br><span class="line"><span class="string">    A -- 矩阵 (n_C, n_H*n_W)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    GA = tf.matmul(A, tf.transpose(A))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> GA</span><br></pre></td></tr></table></figure><h4 id="某一层的-style-cost"><a href="#某一层的-style-cost" class="headerlink" title="某一层的 style cost"></a>某一层的 style cost</h4><script type="math/tex; mode=display">J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2\tag{2}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_layer_style_cost</span>(<span class="params">a_S, a_G</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    a_S -- 图片S的隐藏层，(1, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    a_G -- 图片G的隐藏层，(1, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取维度</span></span><br><span class="line">    m, n_H, n_W, n_C = a_G.get_shape().as_list()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 展开</span></span><br><span class="line">    a_S = tf.transpose(tf.reshape(a_S, [n_H*n_W, n_C]))</span><br><span class="line">    a_G = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算格拉姆矩阵</span></span><br><span class="line">    GS = gram_matrix(a_S)</span><br><span class="line">    GG = gram_matrix(a_G)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算 style cost</span></span><br><span class="line">    J_style_layer = tf.reduce_sum(tf.square(GS - GG)) / (<span class="number">4</span> * (n_C * n_H * n_W)**<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J_style_layer</span><br></pre></td></tr></table></figure><h4 id="将多个隐藏层的-style-cost-组合"><a href="#将多个隐藏层的-style-cost-组合" class="headerlink" title="将多个隐藏层的 style cost 组合"></a>将多个隐藏层的 style cost 组合</h4><script type="math/tex; mode=display">J_{style}(S,G) = \sum_{l} \lambda^{[l]} J^{[l]}_{style}(S,G)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_style_cost</span>(<span class="params">model, STYLE_LAYERS</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    计算最终的 style cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    STYLE_LAYERS -- 列表：</span></span><br><span class="line"><span class="string">                        - 隐藏层的名字</span></span><br><span class="line"><span class="string">                        - 系数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    J_style = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> layer_name, coeff <span class="keyword">in</span> STYLE_LAYERS:</span><br><span class="line">        out = model[layer_name]</span><br><span class="line">        a_S = sess.run(out)</span><br><span class="line">        a_G = out</span><br><span class="line">        </span><br><span class="line">        J_style_layer = compute_layer_style_cost(a_S, a_G)</span><br><span class="line">        J_style += coeff * J_style_layer</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> J_style</span><br></pre></td></tr></table></figure><h3 id="最终的-Cost"><a href="#最终的-Cost" class="headerlink" title="最终的 Cost"></a>最终的 Cost</h3><script type="math/tex; mode=display">J(G) = \alpha J_{content}(C,G) + \beta J_{style}(S,G)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">total_cost</span>(<span class="params">J_content, J_style, alpha = <span class="number">10</span>, beta = <span class="number">40</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    J = J_content * alpha + J_style * beta</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></table></figure><h2 id="定义-Session-并准备数据和模型"><a href="#定义-Session-并准备数据和模型" class="headerlink" title="定义 Session 并准备数据和模型"></a>定义 Session 并准备数据和模型</h2><h3 id="定义-Session"><a href="#定义-Session" class="headerlink" title="定义 Session"></a>定义 Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><h3 id="准备数据和模型"><a href="#准备数据和模型" class="headerlink" title="准备数据和模型"></a>准备数据和模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">content_image = scipy.misc.imread(<span class="string">&quot;images/content.jpg&quot;</span>)</span><br><span class="line">content_image = reshape_and_normalize_image(content_image)</span><br><span class="line"></span><br><span class="line">style_image = scipy.misc.imread(<span class="string">&quot;images/picasso&quot;</span>)</span><br><span class="line">style_image = reshape_and_normalize_image(style_image)</span><br><span class="line"></span><br><span class="line">generated_image = generate_noise_image(content_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入模型</span></span><br><span class="line">model = load_vgg_model(<span class="string">&quot;../models/imagenet-vgg-verydeep-19.mat&quot;</span>)</span><br><span class="line"></span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure><blockquote><p>{‘input’: <tf.Variable 'Variable:0' shape=(1, 300, 400, 3) dtype=float32_ref>, ‘conv1_1’: <tf.Tensor 'Relu:0' shape=(1, 300, 400, 64) dtype=float32>, ‘conv1_2’: <tf.Tensor 'Relu_1:0' shape=(1, 300, 400, 64) dtype=float32>, ‘avgpool1’: <tf.Tensor 'AvgPool:0' shape=(1, 150, 200, 64) dtype=float32>, ‘conv2_1’: <tf.Tensor 'Relu_2:0' shape=(1, 150, 200, 128) dtype=float32>, ‘conv2_2’: <tf.Tensor 'Relu_3:0' shape=(1, 150, 200, 128) dtype=float32>, ‘avgpool2’: <tf.Tensor 'AvgPool_1:0' shape=(1, 75, 100, 128) dtype=float32>, ‘conv3_1’: <tf.Tensor 'Relu_4:0' shape=(1, 75, 100, 256) dtype=float32>, ‘conv3_2’: <tf.Tensor 'Relu_5:0' shape=(1, 75, 100, 256) dtype=float32>, ‘conv3_3’: <tf.Tensor 'Relu_6:0' shape=(1, 75, 100, 256) dtype=float32>, ‘conv3_4’: <tf.Tensor 'Relu_7:0' shape=(1, 75, 100, 256) dtype=float32>, ‘avgpool3’: <tf.Tensor 'AvgPool_2:0' shape=(1, 38, 50, 256) dtype=float32>, ‘conv4_1’: <tf.Tensor 'Relu_8:0' shape=(1, 38, 50, 512) dtype=float32>, ‘conv4_2’: <tf.Tensor 'Relu_9:0' shape=(1, 38, 50, 512) dtype=float32>, ‘conv4_3’: <tf.Tensor 'Relu_10:0' shape=(1, 38, 50, 512) dtype=float32>, ‘conv4_4’: <tf.Tensor 'Relu_11:0' shape=(1, 38, 50, 512) dtype=float32>, ‘avgpool4’: <tf.Tensor 'AvgPool_3:0' shape=(1, 19, 25, 512) dtype=float32>, ‘conv5_1’: <tf.Tensor 'Relu_12:0' shape=(1, 19, 25, 512) dtype=float32>, ‘conv5_2’: <tf.Tensor 'Relu_13:0' shape=(1, 19, 25, 512) dtype=float32>, ‘conv5_3’: <tf.Tensor 'Relu_14:0' shape=(1, 19, 25, 512) dtype=float32>, ‘conv5_4’: <tf.Tensor 'Relu_15:0' shape=(1, 19, 25, 512) dtype=float32>, ‘avgpool5’: <tf.Tensor 'AvgPool_4:0' shape=(1, 10, 13, 512) dtype=float32>}</p></blockquote><p>这里，我传入的 content_image 和 style_image 分别是</p><div class="table-container"><table><thead><tr><th>content_image</th><th>style_image</th></tr></thead><tbody><tr><td><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-08-content.jpeg" alt=""></td><td><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-08-picasso.jpg" alt=""></td></tr></tbody></table></div><h2 id="计算-Cost"><a href="#计算-Cost" class="headerlink" title="计算 Cost"></a>计算 Cost</h2><h3 id="计算-Content-Cost"><a href="#计算-Content-Cost" class="headerlink" title="计算 Content Cost"></a>计算 Content Cost</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将 content_image 输入网络</span></span><br><span class="line">sess.run(model[<span class="string">&#x27;input&#x27;</span>].assign(content_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择隐藏层</span></span><br><span class="line">out = model[<span class="string">&#x27;conv4_2&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># a_C 为计算 Content Cost 的隐藏层</span></span><br><span class="line">a_C = sess.run(out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一会我们再把 generated_image 输入网络，即执行 sess.run(model[&#x27;input&#x27;]).assign(generated_image)</span></span><br><span class="line">a_G = out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 Content Cost</span></span><br><span class="line">J_content = compute_content_cost(a_C, a_G)</span><br></pre></td></tr></table></figure><h3 id="计算-Style-Cost"><a href="#计算-Style-Cost" class="headerlink" title="计算 Style Cost"></a>计算 Style Cost</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">STYLE_LAYERS = [</span><br><span class="line">    (<span class="string">&#x27;conv1_1&#x27;</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">&#x27;conv2_1&#x27;</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">&#x27;conv3_1&#x27;</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">&#x27;conv4_1&#x27;</span>, <span class="number">0.2</span>),</span><br><span class="line">    (<span class="string">&#x27;conv5_1&#x27;</span>, <span class="number">0.2</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 style_image 输入网络</span></span><br><span class="line">sess.run(model[<span class="string">&#x27;input&#x27;</span>].assign(style_image))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 Style Cost</span></span><br><span class="line">J_style = compute_style_cost(model, STYLE_LAYERS)</span><br></pre></td></tr></table></figure><h3 id="计算最终的-Cost"><a href="#计算最终的-Cost" class="headerlink" title="计算最终的 Cost"></a>计算最终的 Cost</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">J = total_cost(J_content, J_style, alpha=<span class="number">10</span>, beta=<span class="number">40</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="生成图片"><a href="#生成图片" class="headerlink" title="生成图片"></a>生成图片</h2><h3 id="定义-optimizer-和-train-step"><a href="#定义-optimizer-和-train-step" class="headerlink" title="定义 optimizer 和 train_step"></a>定义 optimizer 和 train_step</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.AdamOptimizer(<span class="number">2.0</span>)</span><br><span class="line"></span><br><span class="line">train_step = optimizer.minimize(J)</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_nn</span>(<span class="params">sess, input_image, num_iterations = <span class="number">200</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将 input_image 输入网络</span></span><br><span class="line">    sess.run(model[<span class="string">&#x27;input&#x27;</span>].assign(input_image))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 迭代</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">    </span><br><span class="line">        sess.run(train_step)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 得到网络的输出</span></span><br><span class="line">        generated_image = sess.run(model[<span class="string">&#x27;input&#x27;</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i%<span class="number">20</span> == <span class="number">0</span>:</span><br><span class="line">            Jt, Jc, Js = sess.run([J, J_content, J_style])</span><br><span class="line">            print(<span class="string">&quot;Iteration &quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot; :&quot;</span>)</span><br><span class="line">            print(<span class="string">&quot;total cost = &quot;</span> + <span class="built_in">str</span>(Jt))</span><br><span class="line">            print(<span class="string">&quot;content cost = &quot;</span> + <span class="built_in">str</span>(Jc))</span><br><span class="line">            print(<span class="string">&quot;style cost = &quot;</span> + <span class="built_in">str</span>(Js))</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 保存当前图片</span></span><br><span class="line">            save_image(<span class="string">&quot;output/&quot;</span> + <span class="built_in">str</span>(i) + <span class="string">&quot;.png&quot;</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 保存最终图片</span></span><br><span class="line">    save_image(<span class="string">&#x27;output/generated_image.jpg&#x27;</span>, generated_image)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_image</span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_nn(sess, generated_image)</span><br></pre></td></tr></table></figure><blockquote><p> Iteration 0 :<br>    total cost = 9490227000.0<br>    content cost = 7180.4365<br>    style cost = 237253890.0<br>    Iteration 20 :<br>    total cost = 2201664800.0<br>    content cost = 17532.338<br>    style cost = 55037236.0<br>    Iteration 40 :<br>    total cost = 1026127170.0<br>    content cost = 19366.562<br>    style cost = 25648338.0<br>    Iteration 60 :<br>    total cost = 647172200.0<br>    content cost = 19991.74<br>    style cost = 16174307.0</p></blockquote><p>最终生成的图片为</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-08-generated_image.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>(译) Pytorch 教程：迁移学习</title>
      <link href="2019/02/01/2019/%EF%BC%88%E8%AF%91%EF%BC%89Pytorch%E6%95%99%E7%A8%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
      <url>2019/02/01/2019/%EF%BC%88%E8%AF%91%EF%BC%89Pytorch%E6%95%99%E7%A8%8B%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇文章翻译自 <a href="https://pytorch.org/">Pytorch</a> 官方教程 <a href="https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html">Transfer Learning Tutorial</a><br><strong>原作者：</strong><a href="https://chsasank.github.io/">Sasank Chilamkurthy</a></p><p><strong>Note:</strong> 点击<a href="#code">这里</a>下载完整示例代码</p></blockquote><p>在这篇教程中，你将会学到如何利用迁移学习来训练你的网络。你可以通过 <a href="https://cs231n.github.io/transfer-learning/">cs231n notes</a> 了解更多关于迁移学习的信息。</p><p>引用 <a href="https://cs231n.github.io/transfer-learning/">cs231n notes</a> 中的一段话</p><blockquote><p>在实践中，很少有人会从头开始训练一个卷积神经网络（随机初始化），因为你很难拥有一个足够大的数据集。事实上，更常见的做法是先在一个非常大的数据集（比如 ImageNet，该数据集含有涵盖了 1000 个类别的 120 万张图片）上预训练一个卷积神经网络，然后利用该网络中参数作为初始参数，或者把该网络当作另一项任务的固定特征提取器。</p></blockquote><a id="more"></a><p>迁移学习主要在以下两个场景下使用：</p><ul><li><strong>网络调优：</strong>使用预训练网络（比如在 ImageNet 上训练的网络）中的参数作为初始参数，而不是随机初始化。其余部分的训练流程和往常一样。</li><li><strong>固定特征提取器：</strong>除了最后的全连接层，我们会冻结网络中其余部分的参数，最后的全连接层中的参数会重新随机初始化，只有该层中的参数会在训练中更新。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># License: BSD</span></span><br><span class="line"><span class="comment"># Author: Sasank Chilamkurthy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function, division</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, models, transforms</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line">plt.ion()   <span class="comment"># 交互模式</span></span><br></pre></td></tr></table></figure><h2 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h2><p>我们将使用 <code>torchvision</code> 和 <code>torch.utils.data</code> 两个 packages 来读取数据。</p><p>我们今天的目标是建立一个可以分辨<strong>蚂蚁</strong>和<strong>蜜蜂</strong>的分类器，但是我们只有蚂蚁和蜜蜂的图片各约 120 张用于训练，75 张用于验证集。通常来说，如果要从头训练一个模型，这个数据集是非常小的。因此我们要利用迁移学习。</p><p>这个数据集是 ImageNet 的一个很小的子集。</p><p><strong>Note:</strong> 从 <a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip">这里</a> 下载数据并将其解压到当前文件夹。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对训练集使用 data augmentation 和 normalization</span></span><br><span class="line"><span class="comment"># 对验证集只使用 normalization</span></span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    <span class="string">&#x27;train&#x27;</span>: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(<span class="number">224</span>  ),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>  , <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">    <span class="string">&#x27;val&#x27;</span>: transforms.Compose([</span><br><span class="line">        transforms.Resize(<span class="number">256</span>),</span><br><span class="line">        transforms.CenterCrop(<span class="number">224</span>  ),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>], [<span class="number">0.229</span>, <span class="number">0.224</span>  , <span class="number">0.225</span>])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = <span class="string">&#x27;data/hymenoptera_data&#x27;</span></span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">                                          data_transforms[x])</span><br><span class="line">                  <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=<span class="number">4</span>,</span><br><span class="line">                                             shuffle=<span class="literal">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line">              <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line">dataset_sizes = &#123;x: <span class="built_in">len</span>(image_datasets[x]) <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]&#125;</span><br><span class="line">class_names = image_datasets[<span class="string">&#x27;train&#x27;</span>].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="可视化一些图像"><a href="#可视化一些图像" class="headerlink" title="可视化一些图像"></a>可视化一些图像</h3><p>为了理解 data augmentation，我们来看看一些图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imshow</span>(<span class="params">inp, title=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot;</span></span><br><span class="line">    inp = inp.numpy().transpose((<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>))</span><br><span class="line">    mean = np.array([<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>])</span><br><span class="line">    std = np.array([<span class="number">0.229</span>, <span class="number">0.224</span>  , <span class="number">0.225</span>])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    <span class="keyword">if</span> title <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(<span class="number">0.001</span>)  <span class="comment"># pause a bit so that plots are updated</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取训练数据中的一个 batch</span></span><br><span class="line">inputs, classes = <span class="built_in">next</span>(<span class="built_in">iter</span>(dataloaders[<span class="string">&#x27;train&#x27;</span>]))</span><br><span class="line"></span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] <span class="keyword">for</span> x <span class="keyword">in</span> classes])</span><br></pre></td></tr></table></figure><p><img src="https://pytorch.org/tutorials/_images/sphx_glr_transfer_learning_tutorial_001.png" alt=""></p><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>现在，为了训练模型，我们应该写一些通用函数。在这里我们将阐述以下两点</p><ul><li>Scheduling 学习率</li><li>保存最好的模型</li></ul><p>下面参数中的 <code>scheduler</code> 是<code>torch.optim.lr_scheduler</code> 包中的 LR scheduler 对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model, criterion, optimizer, scheduler, num_&gt; Epochs=<span class="number">25</span></span>):</span></span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> &gt; Epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_&gt; Epochs):</span><br><span class="line">        print(<span class="string">&#x27;&gt; Epoch &#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(&gt; Epoch, num_&gt; Epochs - <span class="number">1</span>))</span><br><span class="line">        print(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 每次遍历都要经过训练集和验证集</span></span><br><span class="line">        <span class="keyword">for</span> phase <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;val&#x27;</span>]:</span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line">                model.train()  <span class="comment"># 设置模型为训练模式</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                model.<span class="built_in">eval</span>()   <span class="comment"># 设置模型为验证模式</span></span><br><span class="line"></span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">            running_corrects = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 迭代</span></span><br><span class="line">            <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 清零梯度</span></span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 前向传播</span></span><br><span class="line">                <span class="comment"># 只在训练时计算梯度</span></span><br><span class="line">                <span class="keyword">with</span> torch.set_grad_enabled(phase == <span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 只有在训练时才进行反向传播和参数更新</span></span><br><span class="line">                    <span class="keyword">if</span> phase == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 统计</span></span><br><span class="line">                running_loss += loss.item() * inputs.size(<span class="number">0</span>)</span><br><span class="line">                running_corrects += torch.<span class="built_in">sum</span>(preds == labels.data)</span><br><span class="line"></span><br><span class="line">            &gt; Epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            &gt; Epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(<span class="string">&#x27;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                phase, &gt; Epoch_loss, &gt; Epoch_acc))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 找到最好的模型</span></span><br><span class="line">            <span class="keyword">if</span> phase == <span class="string">&#x27;val&#x27;</span> <span class="keyword">and</span> &gt; Epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = &gt; Epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(<span class="string">&#x27;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        time_elapsed // <span class="number">60</span>, time_elapsed % <span class="number">60</span>))</span><br><span class="line">    print(<span class="string">&#x27;Best val Acc: &#123;:4f&#125;&#x27;</span>.<span class="built_in">format</span>(best_acc))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 读取最好模型</span></span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><h3 id="观察模型给出的预测"><a href="#观察模型给出的预测" class="headerlink" title="观察模型给出的预测"></a>观察模型给出的预测</h3><p>这是一个用于展示预测结果的通用函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize_model</span>(<span class="params">model, num_images=<span class="number">6</span></span>):</span></span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    images_so_far = <span class="number">0</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloaders[<span class="string">&#x27;val&#x27;</span>]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(inputs.size()[<span class="number">0</span>]):</span><br><span class="line">                images_so_far += <span class="number">1</span></span><br><span class="line">                ax = plt.subplot(num_images//<span class="number">2</span>, <span class="number">2</span>, images_so_far)</span><br><span class="line">                ax.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">                ax.set_title(<span class="string">&#x27;predicted: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    <span class="keyword">return</span></span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure><h2 id="网络调优"><a href="#网络调优" class="headerlink" title="网络调优"></a>网络调优</h2><p>读取一个预训练网络并重置最后的全连接层</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里所有参数都会更新</span></span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率每 7 次迭代以 0.1 为因子衰减</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h3 id="训练与验证"><a href="#训练与验证" class="headerlink" title="训练与验证"></a>训练与验证</h3><p>在 CPU 上训练会花费大约 15-25 分钟，而在 GPU 上则要不了一分钟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_&gt; Epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>Epoch 0/24<br>——————<br>train Loss: 0.5900 Acc: 0.7131<br>val Loss: 0.2508 Acc: 0.9020</p><p>Epoch 1/24<br>——————<br>train Loss: 0.6034 Acc: 0.7828<br>val Loss: 0.3181 Acc: 0.8627  </p><p>Epoch 2/24<br>——————<br>train Loss: 0.6150 Acc: 0.7582<br>val Loss: 0.4903 Acc: 0.8366  </p><p>Epoch 3/24<br>——————<br>train Loss: 0.6650 Acc: 0.7377<br>val Loss: 0.6294 Acc: 0.7582  </p><p>Epoch 4/24<br>——————<br>train Loss: 0.4935 Acc: 0.7828<br>val Loss: 0.2644 Acc: 0.8889  </p><p>Epoch 5/24<br>——————<br>train Loss: 0.3841 Acc: 0.8238<br>val Loss: 0.24  08 Acc: 0.9216  </p><p>Epoch 6/24<br>——————<br>train Loss: 0.5352 Acc: 0.8156<br>val Loss: 0.2250 Acc: 0.9150  </p><p>Epoch 7/24<br>——————<br>train Loss: 0.2252 Acc: 0.9385<br>val Loss: 0.1917 Acc: 0.9477  </p><p>Epoch 8/24<br>——————<br>train Loss: 0.3395 Acc: 0.8197<br>val Loss: 0.1738 Acc: 0.9477  </p><p>Epoch 9/24<br>——————<br>train Loss: 0.3363 Acc: 0.8607<br>val Loss: 0.2522 Acc: 0.9216  </p><p>Epoch 10/24<br>——————<br>train Loss: 0.2878 Acc: 0.8607<br>val Loss: 0.1787 Acc: 0.9412  </p><p>Epoch 11/24<br>——————<br>train Loss: 0.2831 Acc: 0.8770<br>val Loss: 0.1805 Acc: 0.9346  </p><p>Epoch 12/24<br>——————<br>train Loss: 0.2290 Acc: 0.9016<br>val Loss: 0.1898 Acc: 0.9412  </p><p>Epoch 13/24<br>——————<br>train Loss: 0.24  94 Acc: 0.9016<br>val Loss: 0.1729 Acc: 0.9412  </p><p>Epoch 14/24<br>——————<br>train Loss: 0.3435 Acc: 0.8689<br>val Loss: 0.1736 Acc: 0.9412  </p><p>Epoch 15/24<br>——————<br>train Loss: 0.2274 Acc: 0.9057<br>val Loss: 0.1692 Acc: 0.9542  </p><p>Epoch 16/24<br>——————<br>train Loss: 0.3154 Acc: 0.8689<br>val Loss: 0.1742 Acc: 0.9412  </p><p>Epoch 17/24<br>——————<br>train Loss: 0.2749 Acc: 0.8893<br>val Loss: 0.1826 Acc: 0.9412  </p><p>Epoch 18/24<br>——————<br>train Loss: 0.2673 Acc: 0.8770<br>val Loss: 0.1731 Acc: 0.9281  </p><p>Epoch 19/24<br>——————<br>train Loss: 0.2865 Acc: 0.8730<br>val Loss: 0.1867 Acc: 0.9346  </p><p>Epoch 20/24<br>——————<br>train Loss: 0.3061 Acc: 0.8648<br>val Loss: 0.1966 Acc: 0.9477  </p><p>Epoch 21/24<br>——————<br>train Loss: 0.2638 Acc: 0.9016<br>val Loss: 0.1973 Acc: 0.9477  </p><p>Epoch 22/24<br>——————<br>train Loss: 0.2602 Acc: 0.8893<br>val Loss: 0.1769 Acc: 0.9281  </p><p>Epoch 23/24<br>——————<br>train Loss: 0.2817 Acc: 0.9016<br>val Loss: 0.1756 Acc: 0.9412  </p><p>Epoch 24  /24<br>——————<br>train Loss: 0.2959 Acc: 0.8730<br>val Loss: 0.1790 Acc: 0.9281</p><p>Training complete in 1m 8s<br>Best val Acc: 0.95424  8</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure><p><img src="https://pytorch.org/tutorials/_images/sphx_glr_transfer_learning_tutorial_002.png" alt=""></p><h2 id="固定特征提取器"><a href="#固定特征提取器" class="headerlink" title="固定特征提取器"></a>固定特征提取器</h2><p>现在，除了最后的全连接层，我们要冻结网络中其余部分的所有参数。我们使用 <code>requires_grad = False</code> 来冻结参数，<code>bachward()</code> 便不会计算这些参数的梯度。</p><p>你可以在 <a href="https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward">这里</a> 读到更多信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model_conv.parameters():</span><br><span class="line">    param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新构建模块中的参数的 requires_grad 默认为 True</span></span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在只有最后的全连接层的参数会更新</span></span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率每 7 次迭代以 0.1 为因子衰减</span></span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=<span class="number">7</span>, gamma=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h3 id="训练与验证-1"><a href="#训练与验证-1" class="headerlink" title="训练与验证"></a>训练与验证</h3><p>在 CPU 上，这会花费约之前一半的时间，因为大多数参数的梯度不用计算了，不过这些参数仍然会参与前向传播。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_&gt; Epochs=<span class="number">25</span>)</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>Epoch 0/24<br>——————<br>train Loss: 0.6463 Acc: 0.6803<br>val Loss: 0.1949 Acc: 0.9477  </p><p>Epoch 1/24<br>——————<br>train Loss: 0.4923 Acc: 0.8033<br>val Loss: 0.1696 Acc: 0.9477  </p><p>Epoch 2/24<br>——————<br>train Loss: 0.4234 Acc: 0.8115<br>val Loss: 0.4379 Acc: 0.7712  </p><p>Epoch 3/24<br>——————<br>train Loss: 0.5606 Acc: 0.7582<br>val Loss: 0.6383 Acc: 0.7451  </p><p>Epoch 4/24<br>——————<br>train Loss: 0.7560 Acc: 0.7295<br>val Loss: 0.1888 Acc: 0.9412  </p><p>Epoch 5/24<br>——————<br>train Loss: 0.4316 Acc: 0.8197<br>val Loss: 0.1999 Acc: 0.9477  </p><p>Epoch 6/24<br>——————<br>train Loss: 0.7722 Acc: 0.7131<br>val Loss: 0.1975 Acc: 0.9477  </p><p>Epoch 7/24<br>——————<br>train Loss: 0.3685 Acc: 0.8607<br>val Loss: 0.2000 Acc: 0.9477  </p><p>Epoch 8/24<br>——————<br>train Loss: 0.2968 Acc: 0.8811<br>val Loss: 0.1916 Acc: 0.9477  </p><p>Epoch 9/24<br>——————<br>train Loss: 0.3396 Acc: 0.8525<br>val Loss: 0.2165 Acc: 0.9542  </p><p>Epoch 10/24<br>——————<br>train Loss: 0.3885 Acc: 0.8320<br>val Loss: 0.2109 Acc: 0.9542  </p><p>Epoch 11/24<br>——————<br>train Loss: 0.4107 Acc: 0.8156<br>val Loss: 0.1881 Acc: 0.9477  </p><p>Epoch 12/24<br>——————<br>train Loss: 0.3249 Acc: 0.8730<br>val Loss: 0.1747 Acc: 0.9542  </p><p>Epoch 13/24<br>——————<br>train Loss: 0.3439 Acc: 0.8525<br>val Loss: 0.1950 Acc: 0.9477  </p><p>Epoch 14/24<br>——————<br>train Loss: 0.3641 Acc: 0.8443<br>val Loss: 0.1992 Acc: 0.9412  </p><p>Epoch 15/24<br>——————<br>train Loss: 0.3272 Acc: 0.8443<br>val Loss: 0.2320 Acc: 0.9412  </p><p>Epoch 16/24<br>——————<br>train Loss: 0.3102 Acc: 0.8730<br>val Loss: 0.1867 Acc: 0.9477  </p><p>Epoch 17/24<br>——————<br>train Loss: 0.4226 Acc: 0.8238<br>val Loss: 0.1872 Acc: 0.9542  </p><p>Epoch 18/24<br>——————<br>train Loss: 0.3452 Acc: 0.8443<br>val Loss: 0.1812 Acc: 0.9542  </p><p>Epoch 19/24<br>——————<br>train Loss: 0.3697 Acc: 0.8525<br>val Loss: 0.1890 Acc: 0.9477  </p><p>Epoch 20/24<br>——————<br>train Loss: 0.3078 Acc: 0.8607<br>val Loss: 0.1976 Acc: 0.9608  </p><p>Epoch 21/24<br>——————<br>train Loss: 0.3161 Acc: 0.8770<br>val Loss: 0.1982 Acc: 0.9412  </p><p>Epoch 22/24<br>——————<br>train Loss: 0.3749 Acc: 0.8320<br>val Loss: 0.2035 Acc: 0.9477  </p><p>Epoch 23/24<br>——————<br>train Loss: 0.3298 Acc: 0.8525<br>val Loss: 0.1855 Acc: 0.9477  </p><p>Epoch 24/24<br>——————<br>train Loss: 0.3597 Acc: 0.8402<br>val Loss: 0.1878 Acc: 0.9542  </p><p>Training complete in 0m 34s<br>Best val Acc: 0.960784</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://pytorch.org/tutorials/_images/sphx_glr_transfer_learning_tutorial_003.png" alt=""></p><hr><p><span id="code"></p><p><a href="https://pytorch.org/tutorials/_downloads/07d5af1ef41e43c07f848afaf5a1c3cc/transfer_learning_tutorial.py">下载 Python 源代码：transfer_learning_tutorial.py</a></p><p><a href="https://pytorch.org/tutorials/_downloads/62840b1eece760d5e42593187847261f/transfer_learning_tutorial.ipynb">下载 Jupyter Notebook: transfer_learning_tutorial.ipynb</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 译文 </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Learning 文章索引</title>
      <link href="2019/01/28/2019/Deep%20Learning%20%E6%96%87%E7%AB%A0%E7%B4%A2%E5%BC%95/"/>
      <url>2019/01/28/2019/Deep%20Learning%20%E6%96%87%E7%AB%A0%E7%B4%A2%E5%BC%95/</url>
      
        <content type="html"><![CDATA[<p>今天是我的生日。最近不知不觉已经完成了 todo-list 中的一大半了，由于近期文章数量较多，在此做一个索引，方便日后查阅。</p><div class="table-container"><table><thead><tr><th>优化算法</th><th>卷积神经网络</th><th>序列模型</th></tr></thead><tbody><tr><td><a href="http://www.liuhdme.com/2019/01/03/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1%20Mini-Batch%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">Mini-Batch 梯度下降</a></td><td><a href="http://www.liuhdme.com/2019/01/13/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/1%20CNN%20%E9%83%A8%E5%88%86%E7%90%86%E8%AE%BA/">CNN 部分理论</a></td><td><a href="http://www.liuhdme.com/2019/01/22/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/1%20%E4%BD%BF%E7%94%A8%20Numpy%20%E5%BB%BA%E7%AB%8B%20RNN/">使用 Numpy 建立 RNN</a></td></tr><tr><td><a href="http://www.liuhdme.com/2019/01/04/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2%20%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/">指数加权平均</a></td><td><a href="http://www.liuhdme.com/2019/01/14/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/2%20%E4%BD%BF%E7%94%A8%20Numpy%20%E5%BB%BA%E7%AB%8B%20CNN/">使用 Numpy 建立 CNN</a></td><td><a href="http://www.liuhdme.com/2019/01/23/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/2%20%E7%94%A8%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%91%BD%E5%90%8D%E6%81%90%E9%BE%99/">用语言模型命名恐龙</a></td></tr><tr><td><a href="http://www.liuhdme.com/2019/01/05/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3%20%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">动量梯度下降算法</a></td><td><a href="http://www.liuhdme.com/2019/01/16/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/Transfer%20Learning%20%E8%AF%86%E5%88%AB%E5%8A%A8%E7%89%A9%E5%93%81%E7%A7%8D/">Transform Learning 识别狗猫品种</a></td><td><a href="http://www.liuhdme.com/2019/01/24/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/3%20%E7%94%A8%20LSTM%20%E7%94%9F%E6%88%90%E5%B0%8F%E6%AE%B5%E9%9F%B3%E4%B9%90/">用 LSTM 生成小段音乐</a></td></tr><tr><td><a href="http://www.liuhdme.com/2019/01/06/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4%20RMSprop/">RMSprop</a></td><td><a href="http://www.liuhdme.com/2019/01/17/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/1%20%E4%B8%80%E4%BA%9B%E7%BB%8F%E5%85%B8%20CNN/">一些经典 CNN</a></td><td><a href="http://www.liuhdme.com/2019/01/27/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%203/1%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/">注意力模型</a></td></tr><tr><td><a href="http://www.liuhdme.com/2019/01/07/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5%20Adam%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/">Adam 优化算法</a></td><td><a href="http://www.liuhdme.com/2019/01/19/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/2%20Keras-Happy%20House/">Keras 实现 Happy House</a></td><td></td></tr><tr><td><a href="http://www.liuhdme.com/2019/01/08/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6%20%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/">学习率衰减</a></td><td><a href="http://www.liuhdme.com/2019/01/20/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/3%20Keras%20%E5%AE%9E%E7%8E%B0%20ResNet/">Keras 实现 ResNet</a></td><td></td></tr><tr><td></td><td><a href="http://www.liuhdme.com/2019/01/16/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/3%20TensorFlow%20%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/">TensorFlow 实现手势识别</a></td></tr></tbody></table></div><blockquote><p>学如逆水行舟，不进则退。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>注意力模型</title>
      <link href="2019/01/27/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%203/1%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/"/>
      <url>2019/01/27/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%203/1%20%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>注意力模型是深度学习领域的一个非常重要的理论，这篇文章简要谈及我对注意力模型的理解。</p></blockquote><a id="more"></a><h2 id="注意力模型解决了什么问题"><a href="#注意力模型解决了什么问题" class="headerlink" title="注意力模型解决了什么问题"></a>注意力模型解决了什么问题</h2><p>以机器翻译为例，在之前的 Seq2Seq 模型中，我们的做法是先通过一个模型将整个待翻译的句子编码，再通过另一个模型对该编码进行解码，同时输出每一个翻译好的单词，但这不符合人的习惯。面对一个很长的句子时，人们并不是先记住整个句子，然后再逐词翻译的，相反，人们的注意力是会不断转移的，举例来说，当人们翻译句子的开头时，注意力集中在待翻译句子的开头部分，翻译句子结尾时，注意力集中在待翻译句子的结尾部分（不同的语言结构可能会有差异，但能保证的一点是注意力在发生转移），因此，若按照之前的方法，待翻译句子一旦很长，得到的结果便会很差，可以用一张图来描述：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhrd6ztmmj30m0088gls.jpg" alt=""></p><p>图中蓝线表示普通编码解码模型，绿线表示注意力模型，横轴为句子长度，竖轴为衡量翻译质量的一种分数，可以看到，随着句子变长，普通 Encoder-Decoder 模型的表现逐渐下降，而注意力模型的表现一直很稳定。</p><h2 id="大致阐述"><a href="#大致阐述" class="headerlink" title="大致阐述"></a>大致阐述</h2><p>假设我们要把下面这句法语：Jane visite l’Afrique en septembre 翻译成英语：Jane visite Africa in September. 说句题外话，虽然我不懂法语，但是可以看出来这句话在句子结构上，英语和法语差不多，人们在翻译的时候，先看到开头的人名 Jane 然后翻译成英语 Jane，换句话说，当翻译第一个单词的时候，后面几个单词对于第一个单词的翻译的影响并不大，人们的注意力主要集中在第一个单词上，用一张图来表示就是这样的：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhrqfb00ej31300jmjtr.jpg" alt=""></p><p>图中左上角的 \(y^{<1>}\) 就是翻译后的单词，这个翻译的结果取决于对法语句子中不同单词的注意力，这里用几个箭头来表示，每个箭头旁边有一个 \(\alpha\) 用来表示注意力，当然每个 \(\alpha\) 是不一样的，开头的 Jane 的注意力肯定是最大的。</p><p>以此类推，生成英语的每个单词的时候，针对法语的每个单词都有一个注意力参数，这些参数往往都是不同的，代表着对句子不同部分的注意力不同。那么这些注意力参数怎么得到呢，这里的方法是再用一个神经网络用来学习这些参数，如下图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhs8gz648j30fa0c2js3.jpg" alt=""></p><p>图中下方的 \(s^{<t-1>}\) 由 Decoder 中当前结点的前一个结点传来，\(a^{&lt;t^{‘}&gt;}\) 由 Encoder 中的某一个结点传来，将这两个值作为输入传入一个小型神经网络以得到一个输出，这个输出还不能当做注意力权重，因为所有的注意力权重加起来规定为 1，为了保证这一点，需要利用 <code>softmax()</code>  函数，即图中上方的式子，最终的结果才是法语句子中每个单词的注意力权重。</p><h2 id="具体代码实现"><a href="#具体代码实现" class="headerlink" title="具体代码实现"></a>具体代码实现</h2><p>上面只是对注意力模型进行了大致阐述，很多地方的描述其实不够准确，而且还有很多没有讲到的地方，这些地方可以通过具体的代码实现来学习，参考另一篇文章。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM</title>
      <link href="2019/01/26/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%203/2%20LSTM/"/>
      <url>2019/01/26/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%203/2%20LSTM/</url>
      
        <content type="html"><![CDATA[<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-13-LSTM_rnn.png" alt=""></p><a id="more"></a><h2 id="Gated-Recurrent-Unit"><a href="#Gated-Recurrent-Unit" class="headerlink" title="Gated Recurrent Unit"></a>Gated Recurrent Unit</h2><p>GRU 和 LSTM 都可以很好的解决 RNN 中的梯度消失问题，而 GRU 与 LSTM 在某些方面很相似，为了阐述 LSTM，先阐述 GRU。</p><p>下图所示是普通 RNN 单元</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-13-rnn_step_forward.png" alt=""></p><p>GRU 的 RNN 单元与其类似，但有所不同，其中对于 a 的计算分为三部：</p><ol><li>计算 $\tilde{a}^{\langle t \rangle} = tanh(w_a[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_a)$</li><li>计算 $\Gamma_u = \sigma(w_u[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u)$</li><li>最终 $a^{\langle t \rangle} = \Gamma_u \cdot \tilde{a}^{\langle t \rangle} + (1-\Gamma_u) \cdot a^{\langle {t-1} \rangle}$</li></ol><p>其中 $\Gamma<em>u$ 为 _update gate</em>，即更新门，其值域为 $[0, 1]$. 从上式可以看出，最终的 $a^{\langle t \rangle}$ 是当前激活值与一个时间步骤前的激活值的线性组合，通过这种方式，可以使得先前激活值有一定概率传播到当前激活值，即记住了句子之前的信息。然后用最终的 $a^{\langle t \rangle}$ 计算 $y^{\langle t \rangle}$.</p><p>另外为了与普通 RNN 单元进行区分，GRU 中的激活值一般以 c 表示，将上式中的 a 替换为 c 即可，下面将使用 c 阐述其他内容。</p><p>目前为止介绍的 GRU 其实做了简化，完整的 GRU 还有一个相关门，即 relevant gate，用来确定 $c^{\langle {t-1} \rangle}$ 与 $c^{\langle t \rangle}$ 的相关程度，加入了更新门后对于 c 的计算过程如下。</p><ol><li>计算 $\Gamma_r = \sigma(w_r[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_r)$</li><li>计算 $\tilde{c}^{\langle{t}\rangle} = tanh(w_c[\Gamma_r \cdot c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)$ </li><li>计算 $\Gamma_u = \sigma(w_u[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u)$</li><li>最终 $c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + (1-\Gamma_u) \cdot c^{\langle {t-1} \rangle}$</li></ol><p>其中第一步计算 <em>relevant gate</em>，第二步利用 <em>relevatn gate</em> 计算 $\tilde{c}^{\langle t \rangle}$，其余步骤与之前相同。</p><p>关于为什么要使用这样的 RNN 单元，Andrew NG 对此有下面这一番话：</p><blockquote><p>So why we use these architectures, why don’t we change them, how we know they will work, why not add another gate, why not use the simpler GRU instead of the full GRU; well researchers has experimented over years all the various types of these architectures with many many different versions and also addressing the vanishing gradient problem. They have found that full GRUs are one of the best RNN architectures to be used for many different problems. You can make your design but put in mind that GRUs and LSTMs are standards.</p></blockquote><h2 id="Long-Short-Term-Memory"><a href="#Long-Short-Term-Memory" class="headerlink" title="Long Short Term Memory"></a>Long Short Term Memory</h2><p>GRU 在解决梯度消失问题上的表现很不错，但在 GRU 提出之前，LSTM 存在已久，而 LSTM 比起 GRU 使用得更加普遍。</p><p>LSTM 与 GRU 十分相似。在 GRU 中，我们有 <em>update gate</em> 和 <em>relevant gate</em>，以及激活单元 c，而在 LSTM 中，没有<em>relevant gate</em>，但新增了 <em>forget gate</em> 和 <em>output gate</em>，以及激活单元 c 和 a，下面我们来详细阐述。</p><p>在 GRU 中，我们使用 <em>update gate</em> 的来控制激活单元是否更新以及更新的程度，其目的是减少激活单元更新的次数或程度，好让之前的激活单元的值得到保留，换言之，记住句子前面部分的信息，这一点在 LSTM 中并没改变，只不过相比于 GRU 使用 $1 - \Gamma<em>u$ 来表示不更新的概率，LSTM 直接使用一个 _forget gate</em> 即 $\Gamma_f$ 来代替 $1 - \Gamma_u$，下表可以清楚看出 GRU 与 LSTM 在计算 $c^{\langle t \rangle}$ 时的区别。</p><div class="table-container"><table><thead><tr><th>GRU</th><th>LSTM</th></tr></thead><tbody><tr><td>$c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + (1-\Gamma_u) \cdot c^{\langle{t-1}\rangle}$</td><td>$c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + \Gamma_f \cdot c^{\langle{t-1}\rangle}$</td></tr></tbody></table></div><p>另一个门，即 <em>output gate</em> 的作用是进一步控制激活单元更新的程度，在 GRU 中，上表算出的 $c^{\langle t \rangle}$ 就是激活单元，而在 LSTM 中还需进一步计算，再用一张表表示。</p><div class="table-container"><table><thead><tr><th>GRU 的激活单元</th><th>LSTM 的激活单元</th></tr></thead><tbody><tr><td>$c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + (1-\Gamma_u) \cdot c^{\langle{t-1}\rangle}$</td><td>$c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + \Gamma_f \cdot c^{\langle{t-1}\rangle}$<br>$a^{\langle t \rangle} = \Gamma_o \cdot tanh(c^{\langle t \rangle})$</td></tr></tbody></table></div><p>从表中可以看出，LSTM 的最终激活单元是 a，即 $y^{\langle t \rangle}$ 是通过 $a^{\langle t \rangle}$ 的计算得出的，$c^{\langle t \rangle}$ 只是中间变量，不过 $c^{\langle t \rangle}$ 和 $a^{\langle t \rangle}$ 都会传向下一个单元，一会会用一张图表示这个过程。</p><p>介绍了 <em>forget gate</em> 和 <em>output gate</em> 的作用后，让我们把 LSTM 的激活单元计算过程中涉及的计算式完整写一遍：</p><ol><li>$\tilde{c}^{\langle t \rangle} = tanh(w_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)$</li><li>$\Gamma_u = \sigma(w_u[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u)$</li><li>$\Gamma_f = \sigma(w_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)$</li><li>$\Gamma_o = \sigma(w_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)$</li><li>$c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + \Gamma_f \cdot c^{\langle{t-1}\rangle}$</li><li>$a^{\langle t \rangle} = \Gamma_o \cdot tanh(c^{\langle t \rangle})$</li></ol><p>计算顺序不一定按照上面的序号来。可以用一张图来表示 LSTM 的 RNN 单元的计算过程：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-13-LSTM.png" alt=""></p><p>从图中可以看出，$c^{\langle t \rangle}$ 和 $a^{\langle t \rangle}$ 都传向了下一个单元（这里说下一个单元有些不太准确，准确形容应该是 the next time step），但只有 $a^{\langle t \rangle}$ 参与了 $y^{\langle t \rangle}$ 的计算。</p><p>下面这幅图展示了 LSTM 的前向传播过程。</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-13-LSTM_rnn.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用 LSTM 生成小段音乐</title>
      <link href="2019/01/24/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/3%20%E7%94%A8%20LSTM%20%E7%94%9F%E6%88%90%E5%B0%8F%E6%AE%B5%E9%9F%B3%E4%B9%90/"/>
      <url>2019/01/24/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/3%20%E7%94%A8%20LSTM%20%E7%94%9F%E6%88%90%E5%B0%8F%E6%AE%B5%E9%9F%B3%E4%B9%90/</url>
      
        <content type="html"><![CDATA[<blockquote><p>学习完 LSTM 原理，用 LSTM 生成一小段音乐</p></blockquote><audio controls="controls" ><source src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-02-30s_trained_model.mp3" type="audio/mpeg" />                    Your browser does not support the audio element.</audio><a id="more"></a><h3 id="导入所需的包"><a href="#导入所需的包" class="headerlink" title="导入所需的包"></a>导入所需的包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> music21 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> grammar <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> preprocess <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> music_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> data_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model, Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br></pre></td></tr></table></figure><h3 id="读入数据"><a href="#读入数据" class="headerlink" title="读入数据"></a>读入数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, Y, n_values, indices_values = load_music_utils() <span class="comment"># 读入数据</span></span><br><span class="line">n_a = <span class="number">64</span>  <span class="comment"># 隐藏层节点数</span></span><br></pre></td></tr></table></figure><ul><li>X.shape: (60, 30, 78)</li><li>训练样本数: 60</li><li>序列长度: 30</li><li>音域: 78</li><li>Y.shape: (30, 60, 78)</li></ul><p>Y 和 X 的 shape 不一样是为了更方便地将其喂入 LSTM。</p><h3 id="创建层对象"><a href="#创建层对象" class="headerlink" title="创建层对象"></a>创建层对象</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 层对象</span></span><br><span class="line">reshapor = Reshape((<span class="number">1</span>, <span class="number">78</span>))</span><br><span class="line">LSTM_cell = LSTM(n_a, return_state = <span class="literal">True</span>)</span><br><span class="line">densor = Dense(n_values, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>模型结构如图所示</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-134954.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">djmodel</span>(<span class="params">Tx, n_a, n_values</span>):</span></span><br><span class="line">    X = Input(shape=(Tx, n_values))</span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">&#x27;a0&#x27;</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">&#x27;c0&#x27;</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line"></span><br><span class="line">    outputs = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(Tx):</span><br><span class="line">        x = Lambda(<span class="keyword">lambda</span> x: X[:, t, :])(X)</span><br><span class="line">        x = reshapor(x)</span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        out = densor(a)</span><br><span class="line">        outputs.append(out)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=[X, a0, c0], outputs=outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line">    </span><br><span class="line">model = Model(Tx = <span class="number">30</span>, n_a = <span class="number">64</span>, n_values = <span class="number">78</span>)</span><br></pre></td></tr></table></figure><h3 id="配置、训练模型"><a href="#配置、训练模型" class="headerlink" title="配置、训练模型"></a>配置、训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 Adam 优化算法</span></span><br><span class="line">opt = Adam(lr=<span class="number">0.01</span>, beta_1=<span class="number">0.9</span>, beta_2=<span class="number">0.999</span>, decay=<span class="number">0.01</span>)</span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=opt, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">m = <span class="number">60</span></span><br><span class="line"><span class="comment"># 初始化为 0</span></span><br><span class="line">a0 = np.zeros((m, n_a))</span><br><span class="line">c0 = np.zeros((m, n_a))</span><br><span class="line"><span class="comment"># 训练 100 epochs</span></span><br><span class="line">model.fit([X, a0, c0], <span class="built_in">list</span>(Y), epochs=<span class="number">100</span>)</span><br></pre></td></tr></table></figure><h3 id="定义音乐生成模型"><a href="#定义音乐生成模型" class="headerlink" title="定义音乐生成模型"></a>定义音乐生成模型</h3><p>模型结构如图所示</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-135100.jpg" alt=""></p><p>\(x^{＜t＞} = y^{＜t-1＞}\)</p><p>\(x^{<1>}, a_0, c_0\) 均初始化为 0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">music_inference_model</span>(<span class="params">LSTM_cell, densor, n_values = <span class="number">78</span>, n_a = <span class="number">64</span>, Ty = <span class="number">100</span></span>):</span></span><br><span class="line">    x0 = Input(shape=(<span class="number">1</span>, n_values))</span><br><span class="line">    a0 = Input(shape=(n_a,), name=<span class="string">&#x27;a0&#x27;</span>)</span><br><span class="line">    c0 = Input(shape=(n_a,), name=<span class="string">&#x27;c0&#x27;</span>)</span><br><span class="line">    a = a0</span><br><span class="line">    c = c0</span><br><span class="line">    x = x0</span><br><span class="line"></span><br><span class="line">    outputs = <span class="built_in">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(Ty):</span><br><span class="line">        a, _, c = LSTM_cell(x, initial_state=[a, c])</span><br><span class="line">        out = densor(a)</span><br><span class="line">        outputs.append(out)</span><br><span class="line">        x = Lambda(one_hot)(out)</span><br><span class="line"></span><br><span class="line">    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inference_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练好的 LSTM_cell 和 densor 层对象</span></span><br><span class="line">inference_model = music_inference_model(LSTM_cell, densor, n_values = <span class="number">78</span>, n_a = <span class="number">64</span>, Ty = <span class="number">50</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化为 0</span></span><br><span class="line">x_initializer = np.zeros((<span class="number">1</span>, <span class="number">1</span>, <span class="number">78</span>))</span><br><span class="line">a_initializer = np.zeros((<span class="number">1</span>, n_a))</span><br><span class="line">c_initializer = np.zeros((<span class="number">1</span>, n_a))</span><br></pre></td></tr></table></figure><h3 id="生成音乐"><a href="#生成音乐" class="headerlink" title="生成音乐"></a>生成音乐</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_and_sample</span>(<span class="params">inference_model, x_initializer, a_initializer, c_initializer</span>):</span></span><br><span class="line">    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])</span><br><span class="line">    indices = np.argmax(np.array(pred), axis=-<span class="number">1</span>)</span><br><span class="line">    results = to_categorical(indices)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> results, indices</span><br><span class="line"></span><br><span class="line">results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成音乐</span></span><br><span class="line">out_stream = generate_music(inference_model)</span><br></pre></td></tr></table></figure><audio controls="controls" ><source src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-02-30s_trained_model.mp3" type="audio/mpeg" />                    Your browser does not support the audio element.</audio>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用语言模型命名恐龙</title>
      <link href="2019/01/23/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/2%20%E7%94%A8%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%91%BD%E5%90%8D%E6%81%90%E9%BE%99/"/>
      <url>2019/01/23/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/2%20%E7%94%A8%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%91%BD%E5%90%8D%E6%81%90%E9%BE%99/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是序列模型课程第一周的编程作业 Part 2</p></blockquote><a id="more"></a><h1 id="Character-level-language-model-Dinosaurus-land"><a href="#Character-level-language-model-Dinosaurus-land" class="headerlink" title="Character level language model - Dinosaurus land"></a>Character level language model - Dinosaurus land</h1><p>Welcome to Dinosaurus Island! 65 million years ago, dinosaurs existed, and in this assignment they are back. You are in charge of a special task. Leading biology researchers are creating new breeds of dinosaurs and bringing them to life on earth, and your job is to give names to these dinosaurs. If a dinosaur does not like its name, it might go beserk, so choose wisely! </p><table><td><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-141056.jpg"></td></table><p>Luckily you have learned some deep learning and you will use it to save the day. Your assistant has collected a list of all the dinosaur names they could find, and compiled them into this <a href="dinos.txt">dataset</a>. (Feel free to take a look by clicking the previous link.) To create new dinosaur names, you will build a character level language model to generate new names. Your algorithm will learn the different name patterns, and randomly generate new names. Hopefully this algorithm will keep you and your team safe from the dinosaurs’ wrath! </p><p>By completing this assignment you will learn:</p><ul><li>How to store text data for processing using an RNN </li><li>How to synthesize data, by sampling predictions at each time step and passing it to the next RNN-cell unit</li><li>How to build a character-level text generation recurrent neural network</li><li>Why clipping the gradients is important</li></ul><p>We will begin by loading in some functions that we have provided for you in <code>rnn_utils</code>. Specifically, you have access to functions such as <code>rnn_forward</code> and <code>rnn_backward</code> which are equivalent to those you’ve implemented in the previous assignment. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><h2 id="1-Problem-Statement"><a href="#1-Problem-Statement" class="headerlink" title="1 - Problem Statement"></a>1 - Problem Statement</h2><h3 id="1-1-Dataset-and-Preprocessing"><a href="#1-1-Dataset-and-Preprocessing" class="headerlink" title="1.1 - Dataset and Preprocessing"></a>1.1 - Dataset and Preprocessing</h3><p>Run the following cell to read the dataset of dinosaur names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="built_in">open</span>(<span class="string">&#x27;dinos.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>).read()</span><br><span class="line">data= data.lower()</span><br><span class="line">chars = <span class="built_in">list</span>(<span class="built_in">set</span>(data))</span><br><span class="line">data_size, vocab_size = <span class="built_in">len</span>(data), <span class="built_in">len</span>(chars)</span><br><span class="line">print(<span class="string">&#x27;There are %d total characters and %d unique characters in your data.&#x27;</span> % (data_size, vocab_size))</span><br></pre></td></tr></table></figure><pre><code>There are 19909 total characters and 27 unique characters in your data.</code></pre><p>The characters are a-z (26 characters) plus the “\n” (or newline character), which in this assignment plays a role similar to the <code>&lt;EOS&gt;</code> (or “End of sentence”) token we had discussed in lecture, only here it indicates the end of the dinosaur name rather than the end of a sentence. In the cell below, we create a python dictionary (i.e., a hash table) to map each character to an index from 0-26. We also create a second python dictionary that maps each index back to the corresponding character character. This will help you figure out what index corresponds to what character in the probability distribution output of the softmax layer. Below, <code>char_to_ix</code> and <code>ix_to_char</code> are the python dictionaries. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">char_to_ix = &#123; ch:i <span class="keyword">for</span> i,ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">sorted</span>(chars)) &#125;</span><br><span class="line">ix_to_char = &#123; i:ch <span class="keyword">for</span> i,ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">sorted</span>(chars)) &#125;</span><br><span class="line">print(ix_to_char)</span><br></pre></td></tr></table></figure><pre><code>&#123;0: &#39;\n&#39;, 1: &#39;a&#39;, 2: &#39;b&#39;, 3: &#39;c&#39;, 4: &#39;d&#39;, 5: &#39;e&#39;, 6: &#39;f&#39;, 7: &#39;g&#39;, 8: &#39;h&#39;, 9: &#39;i&#39;, 10: &#39;j&#39;, 11: &#39;k&#39;, 12: &#39;l&#39;, 13: &#39;m&#39;, 14: &#39;n&#39;, 15: &#39;o&#39;, 16: &#39;p&#39;, 17: &#39;q&#39;, 18: &#39;r&#39;, 19: &#39;s&#39;, 20: &#39;t&#39;, 21: &#39;u&#39;, 22: &#39;v&#39;, 23: &#39;w&#39;, 24: &#39;x&#39;, 25: &#39;y&#39;, 26: &#39;z&#39;&#125;</code></pre><h3 id="1-2-Overview-of-the-model"><a href="#1-2-Overview-of-the-model" class="headerlink" title="1.2 - Overview of the model"></a>1.2 - Overview of the model</h3><p>Your model will have the following structure: </p><ul><li>Initialize parameters </li><li>Run the optimization loop<ul><li>Forward propagation to compute the loss function</li><li>Backward propagation to compute the gradients with respect to the loss function</li><li>Clip the gradients to avoid exploding gradients</li><li>Using the gradients, update your parameter with the gradient descent update rule.</li></ul></li><li>Return the learned parameters </li></ul><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-141147.jpg" alt=""></p><caption><center> **Figure 1**: Recurrent Neural Network, similar to what you had built in the previous notebook "Building a RNN - Step by Step".  </center></caption><p>At each time-step, the RNN tries to predict what is the next character given the previous characters. The dataset $X = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, …, x^{\langle T_x \rangle})$ is a list of characters in the training set, while $Y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, …, y^{\langle T_x \rangle})$ is such that at every time-step $t$, we have $y^{\langle t \rangle} = x^{\langle t+1 \rangle}$. </p><h2 id="2-Building-blocks-of-the-model"><a href="#2-Building-blocks-of-the-model" class="headerlink" title="2 - Building blocks of the model"></a>2 - Building blocks of the model</h2><p>In this part, you will build two important blocks of the overall model:</p><ul><li>Gradient clipping: to avoid exploding gradients</li><li>Sampling: a technique used to generate characters</li></ul><p>You will then apply these two functions to build the model.</p><h3 id="2-1-Clipping-the-gradients-in-the-optimization-loop"><a href="#2-1-Clipping-the-gradients-in-the-optimization-loop" class="headerlink" title="2.1 - Clipping the gradients in the optimization loop"></a>2.1 - Clipping the gradients in the optimization loop</h3><p>In this section you will implement the <code>clip</code> function that you will call inside of your optimization loop. Recall that your overall loop structure usually consists of a forward pass, a cost computation, a backward pass, and a parameter update. Before updating the parameters, you will perform gradient clipping when needed to make sure that your gradients are not “exploding,” meaning taking on overly large values. </p><p>In the exercise below, you will implement a function <code>clip</code> that takes in a dictionary of gradients and returns a clipped version of gradients if needed. There are different ways to clip gradients; we will use a simple element-wise clipping procedure, in which every element of the gradient vector is clipped to lie between some range [-N, N]. More generally, you will provide a <code>maxValue</code> (say 10). In this example, if any component of the gradient vector is greater than 10, it would be set to 10; and if any component of the gradient vector is less than -10, it would be set to -10. If it is between -10 and 10, it is left alone. </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-141208.jpg" alt=""></p><caption><center> **Figure 2**: Visualization of gradient descent with and without gradient clipping, in a case where the network is running into slight "exploding gradient" problems. </center></caption><p><strong>Exercise</strong>: Implement the function below to return the clipped gradients of your dictionary <code>gradients</code>. Your function takes in a maximum threshold and returns the clipped versions of your gradients. You can check out this <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html">hint</a> for examples of how to clip in numpy. You will need to use the argument <code>out = ...</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### GRADED FUNCTION: clip</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clip</span>(<span class="params">gradients, maxValue</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Clips the gradients&#x27; values between minimum and maximum.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    gradients -- a dictionary containing the gradients &quot;dWaa&quot;, &quot;dWax&quot;, &quot;dWya&quot;, &quot;db&quot;, &quot;dby&quot;</span></span><br><span class="line"><span class="string">    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    gradients -- a dictionary with the clipped gradients.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    </span><br><span class="line">    dWaa, dWax, dWya, db, dby = gradients[<span class="string">&#x27;dWaa&#x27;</span>], gradients[<span class="string">&#x27;dWax&#x27;</span>], gradients[<span class="string">&#x27;dWya&#x27;</span>], gradients[<span class="string">&#x27;db&#x27;</span>], gradients[<span class="string">&#x27;dby&#x27;</span>]</span><br><span class="line">   </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)</span></span><br><span class="line">    <span class="keyword">for</span> gradient <span class="keyword">in</span> [dWax, dWaa, dWya, db, dby]:</span><br><span class="line">        np.clip(gradient, -maxValue, maxValue, out=gradient)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">&quot;dWaa&quot;</span>: dWaa, <span class="string">&quot;dWax&quot;</span>: dWax, <span class="string">&quot;dWya&quot;</span>: dWya, <span class="string">&quot;db&quot;</span>: db, <span class="string">&quot;dby&quot;</span>: dby&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">3</span>)</span><br><span class="line">dWax = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)*<span class="number">10</span></span><br><span class="line">dWaa = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)*<span class="number">10</span></span><br><span class="line">dWya = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)*<span class="number">10</span></span><br><span class="line">db = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)*<span class="number">10</span></span><br><span class="line">dby = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)*<span class="number">10</span></span><br><span class="line">gradients = &#123;<span class="string">&quot;dWax&quot;</span>: dWax, <span class="string">&quot;dWaa&quot;</span>: dWaa, <span class="string">&quot;dWya&quot;</span>: dWya, <span class="string">&quot;db&quot;</span>: db, <span class="string">&quot;dby&quot;</span>: dby&#125;</span><br><span class="line">gradients = clip(gradients, <span class="number">10</span>)</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dWaa\&quot;][1][2] =&quot;</span>, gradients[<span class="string">&quot;dWaa&quot;</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dWax\&quot;][3][1] =&quot;</span>, gradients[<span class="string">&quot;dWax&quot;</span>][<span class="number">3</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dWya\&quot;][1][2] =&quot;</span>, gradients[<span class="string">&quot;dWya&quot;</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;db\&quot;][4] =&quot;</span>, gradients[<span class="string">&quot;db&quot;</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dby\&quot;][1] =&quot;</span>, gradients[<span class="string">&quot;dby&quot;</span>][<span class="number">1</span>])</span><br></pre></td></tr></table></figure><pre><code>gradients[&quot;dWaa&quot;][1][2] = 10.0gradients[&quot;dWax&quot;][3][1] = -10.0gradients[&quot;dWya&quot;][1][2] = 0.29713815361gradients[&quot;db&quot;][4] = [ 10.]gradients[&quot;dby&quot;][1] = [ 8.45833407]</code></pre><p><strong> Expected output:</strong></p><table><tr>    <td>     **gradients["dWaa"][1][2] **    </td>    <td>     10.0    </td></tr><tr>    <td>     **gradients["dWax"][3][1]**    </td>    <td>     -10.0    </td>    </td></tr><tr>    <td>     **gradients["dWya"][1][2]**    </td>    <td> 0.29713815361    </td></tr><tr>    <td>     **gradients["db"][4]**    </td>    <td> [ 10.]    </td></tr><tr>    <td>     **gradients["dby"][1]**    </td>    <td> [ 8.45833407]    </td></tr></table><h3 id="2-2-Sampling"><a href="#2-2-Sampling" class="headerlink" title="2.2 - Sampling"></a>2.2 - Sampling</h3><p>Now assume that your model is trained. You would like to generate new text (characters). The process of generation is explained in the picture below:</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-141227.jpg" alt=""></p><caption><center> **Figure 3**: In this picture, we assume the model is already trained. We pass in $x^{\langle 1\rangle} = \vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption><p><strong>Exercise</strong>: Implement the <code>sample</code> function below to sample characters. You need to carry out 4 steps:</p><ul><li><p><strong>Step 1</strong>: Pass the network the first “dummy” input $x^{\langle 1 \rangle} = \vec{0}$ (the vector of zeros). This is the default input before we’ve generated any characters. We also set $a^{\langle 0 \rangle} = \vec{0}$</p></li><li><p><strong>Step 2</strong>: Run one step of forward propagation to get $a^{\langle 1 \rangle}$ and $\hat{y}^{\langle 1 \rangle}$. Here are the equations:</p></li></ul><script type="math/tex; mode=display">a^{\langle t+1 \rangle} = \tanh(W_{ax}  x^{\langle t \rangle } + W_{aa} a^{\langle t \rangle } + b)\tag{1}</script><script type="math/tex; mode=display">z^{\langle t + 1 \rangle } = W_{ya}  a^{\langle t + 1 \rangle } + b_y \tag{2}</script><script type="math/tex; mode=display">\hat{y}^{\langle t+1 \rangle } = softmax(z^{\langle t + 1 \rangle })\tag{3}</script><p>Note that $\hat{y}^{\langle t+1 \rangle }$ is a (softmax) probability vector (its entries are between 0 and 1 and sum to 1). $\hat{y}^{\langle t+1 \rangle}_i$ represents the probability that the character indexed by “i” is the next character.  We have provided a <code>softmax()</code> function that you can use.</p><ul><li><strong>Step 3</strong>: Carry out sampling: Pick the next character’s index according to the probability distribution specified by $\hat{y}^{\langle t+1 \rangle }$. This means that if $\hat{y}^{\langle t+1 \rangle }_i = 0.16$, you will pick the index “i” with 16% probability. To implement it, you can use <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html"><code>np.random.choice</code></a>.</li></ul><p>Here is an example of how to use <code>np.random.choice()</code>:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">p = np.array([<span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.7</span>, <span class="number">0.2</span>])</span><br><span class="line">index = np.random.choice([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], p = p.ravel())</span><br></pre></td></tr></table></figure><br>This means that you will pick the <code>index</code> according to the distribution:<br>$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$.</p><ul><li><strong>Step 4</strong>: The last step to implement in <code>sample()</code> is to overwrite the variable <code>x</code>, which currently stores $x^{\langle t \rangle }$, with the value of $x^{\langle t + 1 \rangle }$. You will represent $x^{\langle t + 1 \rangle }$ by creating a one-hot vector corresponding to the character you’ve chosen as your prediction. You will then forward propagate $x^{\langle t + 1 \rangle }$ in Step 1 and keep repeating the process until you get a “\n” character, indicating you’ve reached the end of the dinosaur name. </li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sample</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">parameters, char_to_ix, seed</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Sample a sequence of characters according to a sequence of probability distributions output of the RNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. </span></span><br><span class="line"><span class="string">    char_to_ix -- python dictionary mapping each character to an index.</span></span><br><span class="line"><span class="string">    seed -- used for grading purposes. Do not worry about it.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    indices -- a list of length n containing the indices of the sampled characters.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters and relevant shapes from &quot;parameters&quot; dictionary</span></span><br><span class="line">    Waa, Wax, Wya, by, b = parameters[<span class="string">&#x27;Waa&#x27;</span>], parameters[<span class="string">&#x27;Wax&#x27;</span>], parameters[<span class="string">&#x27;Wya&#x27;</span>], parameters[<span class="string">&#x27;by&#x27;</span>], parameters[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line">    vocab_size = by.shape[<span class="number">0</span>]</span><br><span class="line">    n_a = Waa.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)</span></span><br><span class="line">    x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">    <span class="comment"># Step 1&#x27;: Initialize a_prev as zeros (≈1 line)</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)</span></span><br><span class="line">    indices = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Idx is a flag to detect a newline character, we initialize it to -1</span></span><br><span class="line">    idx = -<span class="number">1</span> </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop over time-steps t. At each time-step, sample a character from a probability distribution and append </span></span><br><span class="line">    <span class="comment"># its index to &quot;indices&quot;. We&#x27;ll stop if we reach 50 characters (which should be very unlikely with a well </span></span><br><span class="line">    <span class="comment"># trained model), which helps debugging and prevents entering an infinite loop. </span></span><br><span class="line">    counter = <span class="number">0</span></span><br><span class="line">    newline_character = char_to_ix[<span class="string">&#x27;\n&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> (idx != newline_character <span class="keyword">and</span> counter != <span class="number">50</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Forward propagate x using the equations (1), (2) and (3)</span></span><br><span class="line">        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)</span><br><span class="line">        z = np.dot(Wya, a) + by</span><br><span class="line">        y = softmax(z)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        np.random.seed(counter + seed) </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: Sample the index of a character within the vocabulary from the probability distribution y</span></span><br><span class="line">        idx = np.random.choice(<span class="built_in">list</span>(<span class="built_in">range</span>(vocab_size)), p=y.ravel())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append the index to &quot;indices&quot;</span></span><br><span class="line">        indices.append(idx)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Overwrite the input character as the one corresponding to the sampled index.</span></span><br><span class="line">        x = np.zeros((vocab_size, <span class="number">1</span>))</span><br><span class="line">        x[idx] = <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update &quot;a_prev&quot; to be &quot;a&quot;</span></span><br><span class="line">        a_prev = a</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for grading purposes</span></span><br><span class="line">        seed += <span class="number">1</span></span><br><span class="line">        counter +=<span class="number">1</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (counter == <span class="number">50</span>):</span><br><span class="line">        indices.append(char_to_ix[<span class="string">&#x27;\n&#x27;</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> indices</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">2</span>)</span><br><span class="line">_, n_a = <span class="number">20</span>, <span class="number">100</span></span><br><span class="line">Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)</span><br><span class="line">b, by = np.random.randn(n_a, <span class="number">1</span>), np.random.randn(vocab_size, <span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">&quot;Wax&quot;</span>: Wax, <span class="string">&quot;Waa&quot;</span>: Waa, <span class="string">&quot;Wya&quot;</span>: Wya, <span class="string">&quot;b&quot;</span>: b, <span class="string">&quot;by&quot;</span>: by&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">indices = sample(parameters, char_to_ix, <span class="number">0</span>)</span><br><span class="line">print(<span class="string">&quot;Sampling:&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;list of sampled indices:&quot;</span>, indices)</span><br><span class="line">print(<span class="string">&quot;list of sampled characters:&quot;</span>, [ix_to_char[i] <span class="keyword">for</span> i <span class="keyword">in</span> indices])</span><br></pre></td></tr></table></figure><pre><code>Sampling:list of sampled indices: [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, 7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 5, 6, 12, 25, 0, 0]list of sampled characters: [&#39;l&#39;, &#39;q&#39;, &#39;x&#39;, &#39;n&#39;, &#39;m&#39;, &#39;i&#39;, &#39;j&#39;, &#39;v&#39;, &#39;x&#39;, &#39;f&#39;, &#39;m&#39;, &#39;k&#39;, &#39;l&#39;, &#39;f&#39;, &#39;u&#39;, &#39;o&#39;, &#39;u&#39;, &#39;n&#39;, &#39;c&#39;, &#39;b&#39;, &#39;a&#39;, &#39;u&#39;, &#39;r&#39;, &#39;x&#39;, &#39;g&#39;, &#39;y&#39;, &#39;f&#39;, &#39;y&#39;, &#39;r&#39;, &#39;j&#39;, &#39;p&#39;, &#39;b&#39;, &#39;c&#39;, &#39;h&#39;, &#39;o&#39;, &#39;l&#39;, &#39;k&#39;, &#39;g&#39;, &#39;a&#39;, &#39;l&#39;, &#39;j&#39;, &#39;b&#39;, &#39;g&#39;, &#39;g&#39;, &#39;k&#39;, &#39;e&#39;, &#39;f&#39;, &#39;l&#39;, &#39;y&#39;, &#39;\n&#39;, &#39;\n&#39;]</code></pre><p><strong> Expected output:</strong></p><table><tr>    <td>     **list of sampled indices:**    </td>    <td>     [12, 17, 24, 14, 13, 9, 10, 22, 24, 6, 13, 11, 12, 6, 21, 15, 21, 14, 3, 2, 1, 21, 18, 24, <br>    7, 25, 6, 25, 18, 10, 16, 2, 3, 8, 15, 12, 11, 7, 1, 12, 10, 2, 7, 7, 11, 5, 6, 12, 25, 0, 0]    </td>    </tr><tr>    <td>     **list of sampled characters:**    </td>    <td>     ['l', 'q', 'x', 'n', 'm', 'i', 'j', 'v', 'x', 'f', 'm', 'k', 'l', 'f', 'u', 'o', <br>    'u', 'n', 'c', 'b', 'a', 'u', 'r', 'x', 'g', 'y', 'f', 'y', 'r', 'j', 'p', 'b', 'c', 'h', 'o', <br>    'l', 'k', 'g', 'a', 'l', 'j', 'b', 'g', 'g', 'k', 'e', 'f', 'l', 'y', '\n', '\n']    </td></tr></table><h2 id="3-Building-the-language-model"><a href="#3-Building-the-language-model" class="headerlink" title="3 - Building the language model"></a>3 - Building the language model</h2><p>It is time to build the character-level language model for text generation. </p><h3 id="3-1-Gradient-descent"><a href="#3-1-Gradient-descent" class="headerlink" title="3.1 - Gradient descent"></a>3.1 - Gradient descent</h3><p>In this section you will implement a function performing one step of stochastic gradient descent (with clipped gradients). You will go through the training examples one at a time, so the optimization algorithm will be stochastic gradient descent. As a reminder, here are the steps of a common optimization loop for an RNN:</p><ul><li>Forward propagate through the RNN to compute the loss</li><li>Backward propagate through time to compute the gradients of the loss with respect to the parameters</li><li>Clip the gradients if necessary </li><li>Update your parameters using gradient descent </li></ul><p><strong>Exercise</strong>: Implement this optimization process (one step of stochastic gradient descent). </p><p>We provide you with the following functions: </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span>(<span class="params">X, Y, a_prev, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Performs the forward propagation through the RNN and computes the cross-entropy loss.</span></span><br><span class="line"><span class="string">    It returns the loss&#x27; value as well as a &quot;cache&quot; storing values to be used in the backpropagation.&quot;&quot;&quot;</span></span><br><span class="line">    ....</span><br><span class="line">    <span class="keyword">return</span> loss, cache</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span>(<span class="params">X, Y, parameters, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Performs the backward propagation through time to compute the gradients of the loss with respect</span></span><br><span class="line"><span class="string">    to the parameters. It returns also all the hidden states.&quot;&quot;&quot;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> gradients, a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span>(<span class="params">parameters, gradients, learning_rate</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Updates parameters using the Gradient Descent Update Rule.&quot;&quot;&quot;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: optimize</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span>(<span class="params">X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Execute one step of the optimization to train the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.</span></span><br><span class="line"><span class="string">    Y -- list of integers, exactly the same as X but shifted one index to the left.</span></span><br><span class="line"><span class="string">    a_prev -- previous hidden state.</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        b --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for the model.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- value of the loss function (cross-entropy)</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        db -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        dby -- Gradients of output bias vector, of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward propagate through time (≈1 line)</span></span><br><span class="line">    loss, cache = rnn_forward(X, Y, a_prev, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagate through time (≈1 line)</span></span><br><span class="line">    gradients, a = rnn_backward(X, Y, parameters, cache)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Clip your gradients between -5 (min) and 5 (max) (≈1 line)</span></span><br><span class="line">    gradients = clip(gradients, <span class="number">5</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update parameters (≈1 line)</span></span><br><span class="line">    parameters = update_parameters(parameters, gradients, learning_rate)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss, gradients, a[<span class="built_in">len</span>(X)-<span class="number">1</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">vocab_size, n_a = <span class="number">27</span>, <span class="number">100</span></span><br><span class="line">a_prev = np.random.randn(n_a, <span class="number">1</span>)</span><br><span class="line">Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)</span><br><span class="line">b, by = np.random.randn(n_a, <span class="number">1</span>), np.random.randn(vocab_size, <span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">&quot;Wax&quot;</span>: Wax, <span class="string">&quot;Waa&quot;</span>: Waa, <span class="string">&quot;Wya&quot;</span>: Wya, <span class="string">&quot;b&quot;</span>: b, <span class="string">&quot;by&quot;</span>: by&#125;</span><br><span class="line">X = [<span class="number">12</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">11</span>,<span class="number">22</span>,<span class="number">3</span>]</span><br><span class="line">Y = [<span class="number">4</span>,<span class="number">14</span>,<span class="number">11</span>,<span class="number">22</span>,<span class="number">25</span>, <span class="number">26</span>]</span><br><span class="line"></span><br><span class="line">loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = <span class="number">0.01</span>)</span><br><span class="line">print(<span class="string">&quot;Loss =&quot;</span>, loss)</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dWaa\&quot;][1][2] =&quot;</span>, gradients[<span class="string">&quot;dWaa&quot;</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">&quot;np.argmax(gradients[\&quot;dWax\&quot;]) =&quot;</span>, np.argmax(gradients[<span class="string">&quot;dWax&quot;</span>]))</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dWya\&quot;][1][2] =&quot;</span>, gradients[<span class="string">&quot;dWya&quot;</span>][<span class="number">1</span>][<span class="number">2</span>])</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;db\&quot;][4] =&quot;</span>, gradients[<span class="string">&quot;db&quot;</span>][<span class="number">4</span>])</span><br><span class="line">print(<span class="string">&quot;gradients[\&quot;dby\&quot;][1] =&quot;</span>, gradients[<span class="string">&quot;dby&quot;</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;a_last[4] =&quot;</span>, a_last[<span class="number">4</span>])</span><br></pre></td></tr></table></figure><pre><code>Loss = 126.503975722gradients[&quot;dWaa&quot;][1][2] = 0.194709315347np.argmax(gradients[&quot;dWax&quot;]) = 93gradients[&quot;dWya&quot;][1][2] = -0.007773876032gradients[&quot;db&quot;][4] = [-0.06809825]gradients[&quot;dby&quot;][1] = [ 0.01538192]a_last[4] = [-1.]</code></pre><p><strong> Expected output:</strong></p><table><tr>    <td>     **Loss **    </td>    <td>     126.503975722    </td></tr><tr>    <td>     **gradients["dWaa"][1][2]**    </td>    <td>     0.194709315347    </td><tr>    <td>     **np.argmax(gradients["dWax"])**    </td>    <td> 93    </td></tr><tr>    <td>     **gradients["dWya"][1][2]**    </td>    <td> -0.007773876032    </td></tr><tr>    <td>     **gradients["db"][4]**    </td>    <td> [-0.06809825]    </td></tr><tr>    <td>     **gradients["dby"][1]**    </td>    <td>[ 0.01538192]    </td></tr><tr>    <td>     **a_last[4]**    </td>    <td> [-1.]    </td></tr></table><h3 id="3-2-Training-the-model"><a href="#3-2-Training-the-model" class="headerlink" title="3.2 - Training the model"></a>3.2 - Training the model</h3><p>Given the dataset of dinosaur names, we use each line of the dataset (one name) as one training example. Every 100 steps of stochastic gradient descent, you will sample 10 randomly chosen names to see how the algorithm is doing. Remember to shuffle the dataset, so that stochastic gradient descent visits the examples in random order. </p><p><strong>Exercise</strong>: Follow the instructions and implement <code>model()</code>. When <code>examples[index]</code> contains one dinosaur name (string), to create an example (X, Y), you can use this:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index = j % <span class="built_in">len</span>(examples)</span><br><span class="line">X = [<span class="literal">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]] </span><br><span class="line">Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">&quot;\n&quot;</span>]]</span><br></pre></td></tr></table></figure><br>Note that we use: <code>index= j % len(examples)</code>, where <code>j = 1....num_iterations</code>, to make sure that <code>examples[index]</code> is always a valid statement (<code>index</code> is smaller than <code>len(examples)</code>).<br>The first entry of <code>X</code> being <code>None</code> will be interpreted by <code>rnn_forward()</code> as setting $x^{\langle 0 \rangle} = \vec{0}$. Further, this ensures that <code>Y</code> is equal to <code>X</code> but shifted one step to the left, and with an additional “\n” appended to signify the end of the dinosaur name. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">data, ix_to_char, char_to_ix, num_iterations = <span class="number">35000</span>, n_a = <span class="number">50</span>, dino_names = <span class="number">7</span>, vocab_size = <span class="number">27</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Trains the model and generates dinosaur names. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    data -- text corpus</span></span><br><span class="line"><span class="string">    ix_to_char -- dictionary that maps the index to a character</span></span><br><span class="line"><span class="string">    char_to_ix -- dictionary that maps a character to an index</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to train the model for</span></span><br><span class="line"><span class="string">    n_a -- number of units of the RNN cell</span></span><br><span class="line"><span class="string">    dino_names -- number of dinosaur names you want to sample at each iteration. </span></span><br><span class="line"><span class="string">    vocab_size -- number of unique characters found in the text, size of the vocabulary</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- learned parameters</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve n_x and n_y from vocab_size</span></span><br><span class="line">    n_x, n_y = vocab_size, vocab_size</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(n_a, n_x, n_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize loss (this is required because we want to smooth our loss, don&#x27;t worry about it)</span></span><br><span class="line">    loss = get_initial_loss(vocab_size, dino_names)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Build list of all dinosaur names (training examples).</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;dinos.txt&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        examples = f.readlines()</span><br><span class="line">    examples = [x.lower().strip() <span class="keyword">for</span> x <span class="keyword">in</span> examples]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Shuffle list of all dinosaur names</span></span><br><span class="line">    np.random.seed(<span class="number">0</span>)</span><br><span class="line">    np.random.shuffle(examples)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the hidden state of your LSTM</span></span><br><span class="line">    a_prev = np.zeros((n_a, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_iterations):</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use the hint above to define one training example (X,Y) (≈ 2 lines)</span></span><br><span class="line">        index = j % <span class="built_in">len</span>(examples)</span><br><span class="line">        X = [<span class="literal">None</span>] + [char_to_ix[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> examples[index]] </span><br><span class="line">        Y = X[<span class="number">1</span>:] + [char_to_ix[<span class="string">&quot;\n&quot;</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Perform one optimization step: Forward-prop -&gt; Backward-prop -&gt; Clip -&gt; Update parameters</span></span><br><span class="line">        <span class="comment"># Choose a learning rate of 0.01</span></span><br><span class="line">        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use a latency trick to keep the loss smooth. It happens here to accelerate the training.</span></span><br><span class="line">        loss = smooth(loss, curr_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Every 2000 Iteration, generate &quot;n&quot; characters thanks to sample() to check if the model is learning properly</span></span><br><span class="line">        <span class="keyword">if</span> j % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            print(<span class="string">&#x27;Iteration: %d, Loss: %f&#x27;</span> % (j, loss) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># The number of dinosaur names to print</span></span><br><span class="line">            seed = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> name <span class="keyword">in</span> <span class="built_in">range</span>(dino_names):</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Sample indices and print them</span></span><br><span class="line">                sampled_indices = sample(parameters, char_to_ix, seed)</span><br><span class="line">                print_sample(sampled_indices, ix_to_char)</span><br><span class="line">                </span><br><span class="line">                seed += <span class="number">1</span>  <span class="comment"># To get the same result for grading purposed, increment the seed by one. </span></span><br><span class="line">      </span><br><span class="line">            print(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Run the following cell, you should observe your model outputting random-looking characters at the first iteration. After a few thousand iterations, your model should learn to generate reasonable-looking names. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(data, ix_to_char, char_to_ix)</span><br></pre></td></tr></table></figure><pre><code>Iteration: 0, Loss: 23.087336NkzxwtdmfqoeyhsqwasjkjvuKnebKzxwtdmfqoeyhsqwasjkjvuNebZxwtdmfqoeyhsqwasjkjvuEbXwtdmfqoeyhsqwasjkjvuIteration: 2000, Loss: 27.884160LiusskeomnolxerosHmdaairusHytroligoraurusLecalosapausXusicikoraurusAbalpsamantisaurusTpraneronxerosIteration: 4000, Loss: 25.901815MivrosaurusIneeIvtroplisaurusMbaaisaurusWusichisaurusCabaselachusToraperlethosdarenitochusthiamamumamaonIteration: 6000, Loss: 24.608779OnwusceomosaurusLieeaerosaurusLxussaurusOmaXusteonosaurusEeahosaurusToreonosaurusIteration: 8000, Loss: 24.070350OnxusichepriuonKilabersaurusLutrodonOmaaerosaurusXutrchepsEdaksojeTrodiktonusIteration: 10000, Loss: 23.844446OnyusaurusKlecalosaurusLustodonOlaXusodoniaEeaeosaurusTroceosaurusIteration: 12000, Loss: 23.291971OnyxosaurusKicaLustrepiosaurusOlaagrraiansaurusYuspangosaurusEealosaurusTrognesaurusIteration: 14000, Loss: 23.382339MeutromodromurusIndaIutroinatorsaurusMacaYusteratoptititanCaTroclosaurusIteration: 16000, Loss: 23.288447MeuspsangosaurusIngaaIusosaurusMacalosaurusYushanisDaalosaurusTrpandonIteration: 18000, Loss: 22.823526PhytrolonhonygMelaMustrerasaurusPegYtronorosaurusEhalosaurusTrolomeehusIteration: 20000, Loss: 23.041871NousmofonosaurusLomaLytrognatiasaurusNgaaYtroenetiaudostarmilusEiafosaurusTroenchulunosaurusIteration: 22000, Loss: 22.728849PiutyrangosaurusMidaaMyroranisaurusPedadosaurusYtrodonEiadosaurusTrodoniomusitocorcesIteration: 24000, Loss: 22.683403MeutromeisaurusIndeceratlapsaurusJurosaurusNdaaYusicheropterusEiaeropectusTrodonasaurusIteration: 26000, Loss: 22.554523PhyusaurusLiceceronLyusichenodylusPegahusYustenhtonthosaurusElagosaurusTrodontonsaurusIteration: 28000, Loss: 22.484472OnyutimaerihusKoiaLytusaurusOlaYtroheltorusEiadosaurusTrofiashatesIteration: 30000, Loss: 22.774404PhytysLicaLysusPacalosaurusYtrochisaurusEiacosaurusTrochesaurusIteration: 32000, Loss: 22.209473MawusaurusJicaLustoiaMacaisaurusYusolenqtesaurusEeaeosaurusTrnanatraxIteration: 34000, Loss: 22.396744MavptokekusIlabaisaurusItosaurusMacaesaurusYrosaurusEiaeosaurusTrodon</code></pre><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>You can see that your algorithm has started to generate plausible dinosaur names towards the end of the training. At first, it was generating random characters, but towards the end you could see dinosaur names with cool endings. Feel free to run the algorithm even longer and play with hyperparameters to see if you can get even better results. Our implemetation generated some really cool names like <code>maconucon</code>, <code>marloralus</code> and <code>macingsersaurus</code>. Your model hopefully also learned that dinosaur names tend to end in <code>saurus</code>, <code>don</code>, <code>aura</code>, <code>tor</code>, etc.</p><p>If your model generates some non-cool names, don’t blame the model entirely—not all actual dinosaur names sound cool. (For example, <code>dromaeosauroides</code> is an actual dinosaur name and is in the training set.) But this model should give you a set of candidates from which you can pick the coolest! </p><p>This assignment had used a relatively small dataset, so that you could train an RNN quickly on a CPU. Training a model of the english language requires a much bigger dataset, and usually needs much more computation, and could run for many hours on GPUs. We ran our dinosaur name for quite some time, and so far our favoriate name is the great, undefeatable, and fierce: Mangosaurus!</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-141300.jpg" alt=""></p><h2 id="4-Writing-like-Shakespeare"><a href="#4-Writing-like-Shakespeare" class="headerlink" title="4 - Writing like Shakespeare"></a>4 - Writing like Shakespeare</h2><p>The rest of this notebook is optional and is not graded, but we hope you’ll do it anyway since it’s quite fun and informative. </p><p>A similar (but more complicated) task is to generate Shakespeare poems. Instead of learning from a dataset of Dinosaur names you can use a collection of Shakespearian poems. Using LSTM cells, you can learn longer term dependencies that span many characters in the text—e.g., where a character appearing somewhere a sequence can influence what should be a different character much much later in ths sequence. These long term dependencies were less important with dinosaur names, since the names were quite short. </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-141311.jpg" alt=""></p><caption><center> Let's become poets! </center></caption><p>We have implemented a Shakespeare poem generator with Keras. Run the following cell to load the required packages and models. This may take a few minutes. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> LambdaCallback</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model, Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Activation, Dropout, Input, Masking</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</span><br><span class="line"><span class="keyword">from</span> shakespeare_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> io</span><br></pre></td></tr></table></figure><p>To save you some time, we have already trained a model for ~1000 epochs on a collection of Shakespearian poems called <a href="shakespeare.txt"><em>“The Sonnets”</em></a>. </p><p>Let’s train the model for one more epoch. When it finishes training for an epoch—-this will also take a few minutes—-you can run <code>generate_output</code>, which will prompt asking you for an input (<code>&lt;</code>40 characters). The poem will start with your sentence, and our RNN-Shakespeare will complete the rest of the poem for you! For example, try “Forsooth this maketh no sense “ (don’t enter the quotation marks). Depending on whether you include the space at the end, your results might also differ—try it both ways, and try other inputs as well. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print_callback = LambdaCallback(on_epoch_end=on_epoch_end)</span><br><span class="line"></span><br><span class="line">model.fit(x, y, batch_size=<span class="number">128</span>, epochs=<span class="number">1</span>, callbacks=[print_callback])</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/131412/31412 [==============================] - 1278s - loss: 2.5625  &lt;keras.callbacks.History at 0x7f0ce7ea2f98&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to try with different inputs without having to re-train the model </span></span><br><span class="line">generate_output()</span><br></pre></td></tr></table></figure><p>Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: What a day!</p><p>Here is your poem: </p><p>What a day!<br>that lif how ir shi the naal be have not,<br>sered but as hise wething my meauty grace.<br>then you wey dae libe me of infeer,<br>the earting that with thing lime thise shourl’s stound be goaned.<br>.</p><p>wo true ther pas osed rute as ismebs resed<br>thing wate our frull swilt, the leatere i srand be see,<br>me you hor julkts date as head fart ever.<br>hack she morper reenson doth fow other?<br>how, his moen ound make faw</p><p>The RNN-Shakespeare model is very similar to the one you have built for dinosaur names. The only major differences are:</p><ul><li>LSTMs instead of the basic RNN to capture longer-range dependencies</li><li>The model is a deeper, stacked LSTM model (2 layer)</li><li>Using Keras instead of python to simplify the code </li></ul><p>If you want to learn more, you can also check out the Keras Team’s text generation implementation on GitHub: <a href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py">https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py.</a></p><p>Congratulations on finishing this notebook! </p><p><strong>References</strong>:</p><ul><li>This exercise took inspiration from Andrej Karpathy’s implementation: <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">https://gist.github.com/karpathy/d4dee566867f8291f086</a>. To learn more about text generation, also check out Karpathy’s <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">blog post</a>.</li><li>For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: <a href="https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py">https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py</a> </li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Numpy 建立 RNN</title>
      <link href="2019/01/22/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/1%20%E4%BD%BF%E7%94%A8%20Numpy%20%E5%BB%BA%E7%AB%8B%20RNN/"/>
      <url>2019/01/22/2019/4%20%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/Week%201/1%20%E4%BD%BF%E7%94%A8%20Numpy%20%E5%BB%BA%E7%AB%8B%20RNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是序列模型课程第一周的编程作业 Part 1</p></blockquote><a id="more"></a><h1 id="Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="Building your Recurrent Neural Network - Step by Step"></a>Building your Recurrent Neural Network - Step by Step</h1><p>Welcome to Course 5’s first assignment! In this assignment, you will implement your first Recurrent Neural Network in numpy.</p><p>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs $x^{\langle t \rangle}$ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future. </p><p><strong>Notation</strong>:</p><ul><li><p>Superscript $[l]$ denotes an object associated with the $l^{th}$ layer. </p><ul><li>Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.</li></ul></li><li><p>Superscript $(i)$ denotes an object associated with the $i^{th}$ example. </p><ul><li>Example: $x^{(i)}$ is the $i^{th}$ training example input.</li></ul></li><li><p>Superscript $\langle t \rangle$ denotes an object at the $t^{th}$ time-step. </p><ul><li>Example: $x^{\langle t \rangle}$ is the input x at the $t^{th}$ time-step. $x^{(i)\langle t \rangle}$ is the input at the $t^{th}$ timestep of example $i$.</li></ul></li><li><p>Lowerscript $i$ denotes the $i^{th}$ entry of a vector.</p><ul><li>Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$.</li></ul></li></ul><p>We assume that you are already familiar with <code>numpy</code> and/or have completed the previous courses of the specialization. Let’s get started!</p><p>Let’s first import all the packages that you will need during this assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> rnn_utils <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h2 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1 - Forward propagation for the basic Recurrent Neural Network"></a>1 - Forward propagation for the basic Recurrent Neural Network</h2><p>Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, $T_x = T_y$. </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-132425.jpg" alt=""></p><caption><center> **Figure 1**: Basic RNN model </center></caption><p>Here’s how you can implement an RNN: </p><p><strong>Steps</strong>:</p><ol><li>Implement the calculations needed for one time-step of the RNN.</li><li>Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. </li></ol><p>Let’s go!</p><h2 id="1-1-RNN-cell"><a href="#1-1-RNN-cell" class="headerlink" title="1.1 - RNN cell"></a>1.1 - RNN cell</h2><p>A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell. </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-132453.jpg" alt=""></p><caption><center> **Figure 2**: Basic RNN cell. Takes as input $x^{\langle t \rangle}$ (current input) and $a^{\langle t - 1\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\langle t \rangle}$ which is given to the next RNN cell and also used to predict $y^{\langle t \rangle}$ </center></caption><p><strong>Exercise</strong>: Implement the RNN-cell described in Figure (2).</p><p><strong>Instructions</strong>:</p><ol><li>Compute the hidden state with tanh activation: $a^{\langle t \rangle} = \tanh(W<em>{aa} a^{\langle t-1 \rangle} + W</em>{ax} x^{\langle t \rangle} + b_a)$.</li><li>Using your new hidden state $a^{\langle t \rangle}$, compute the prediction $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$. We provided you a function: <code>softmax</code>.</li><li>Store $(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$ in cache</li><li>Return $a^{\langle t \rangle}$ , $y^{\langle t \rangle}$ and cache</li></ol><p>We will vectorize over $m$ examples. Thus, $x^{\langle t \rangle}$ will have dimension $(n_x,m)$, and $a^{\langle t \rangle}$ will have dimension $(n_a,m)$. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span>(<span class="params">xt, a_prev, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters from &quot;parameters&quot;</span></span><br><span class="line">    Wax = parameters[<span class="string">&quot;Wax&quot;</span>]</span><br><span class="line">    Waa = parameters[<span class="string">&quot;Waa&quot;</span>]</span><br><span class="line">    Wya = parameters[<span class="string">&quot;Wya&quot;</span>]</span><br><span class="line">    ba = parameters[<span class="string">&quot;ba&quot;</span>]</span><br><span class="line">    by = parameters[<span class="string">&quot;by&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya, a_next) + by)   </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Waa = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">Wax = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">Wya = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">ba = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">&quot;Waa&quot;</span>: Waa, <span class="string">&quot;Wax&quot;</span>: Wax, <span class="string">&quot;Wya&quot;</span>: Wya, <span class="string">&quot;ba&quot;</span>: ba, <span class="string">&quot;by&quot;</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)</span><br><span class="line">print(<span class="string">&quot;a_next[4] = &quot;</span>, a_next[<span class="number">4</span>])</span><br><span class="line">print(<span class="string">&quot;a_next.shape = &quot;</span>, a_next.shape)</span><br><span class="line">print(<span class="string">&quot;yt_pred[1] =&quot;</span>, yt_pred[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;yt_pred.shape = &quot;</span>, yt_pred.shape)</span><br></pre></td></tr></table></figure><pre><code>a_next[4] =  [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978 -0.18887155  0.99815551  0.6531151   0.82872037]a_next.shape =  (5, 10)yt_pred[1] = [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212  0.36920224  0.9966312   0.9982559   0.17746526]yt_pred.shape =  (2, 10)</code></pre><p><strong>Expected Output</strong>: </p><table>    <tr>        <td>            **a_next[4]**:        </td>        <td>           [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978 -0.18887155  0.99815551  0.6531151   0.82872037]        </td>    </tr>        <tr>        <td>            **a_next.shape**:        </td>        <td>           (5, 10)        </td>    </tr>        <tr>        <td>            **yt[1]**:        </td>        <td>           [ 0.9888161   0.01682021  0.21140899  0.36817467  0.98988387  0.88945212  0.36920224  0.9966312   0.9982559   0.17746526]        </td>    </tr>        <tr>        <td>            **yt.shape**:        </td>        <td>           (2, 10)        </td>    </tr></table><h2 id="1-2-RNN-forward-pass"><a href="#1-2-RNN-forward-pass" class="headerlink" title="1.2 - RNN forward pass"></a>1.2 - RNN forward pass</h2><p>You can see an RNN as the repetition of the cell you’ve just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell ($a^{\langle t-1 \rangle}$) and the current time-step’s input data ($x^{\langle t \rangle}$). It outputs a hidden state ($a^{\langle t \rangle}$) and a prediction ($y^{\langle t \rangle}$) for this time-step.</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-132608.jpg" alt=""></p><caption><center> **Figure 3**: Basic RNN. The input sequence $x = (x^{\langle 1 \rangle}, x^{\langle 2 \rangle}, ..., x^{\langle T_x \rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\langle 1 \rangle}, y^{\langle 2 \rangle}, ..., y^{\langle T_x \rangle})$. </center></caption><p><strong>Exercise</strong>: Code the forward propagation of the RNN described in Figure (3).</p><p><strong>Instructions</strong>:</p><ol><li>Create a vector of zeros ($a$) that will store all the hidden states computed by the RNN.</li><li>Initialize the “next” hidden state as $a_0$ (initial hidden state).</li><li>Start looping over each time step, your incremental index is $t$ :<ul><li>Update the “next” hidden state and the cache by running <code>rnn_cell_forward</code></li><li>Store the “next” hidden state in $a$ ($t^{th}$ position) </li><li>Store the prediction in y</li><li>Add the cache to the list of caches</li></ul></li><li>Return $a$, $y$ and caches</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: rnn_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span>(<span class="params">x, a0, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize &quot;caches&quot; which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters[&quot;Wya&quot;]</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">&quot;Wya&quot;</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize &quot;a&quot; and &quot;y&quot; with zeros (≈2 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next (≈1 line)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new &quot;next&quot; hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Append &quot;cache&quot; to &quot;caches&quot; (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">3</span>,<span class="number">10</span>,<span class="number">4</span>)</span><br><span class="line">a0 = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Waa = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">Wax = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">Wya = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">ba = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">&quot;Waa&quot;</span>: Waa, <span class="string">&quot;Wax&quot;</span>: Wax, <span class="string">&quot;Wya&quot;</span>: Wya, <span class="string">&quot;ba&quot;</span>: ba, <span class="string">&quot;by&quot;</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a, y_pred, caches = rnn_forward(x, a0, parameters)</span><br><span class="line">print(<span class="string">&quot;a[4][1] = &quot;</span>, a[<span class="number">4</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;a.shape = &quot;</span>, a.shape)</span><br><span class="line">print(<span class="string">&quot;y_pred[1][3] =&quot;</span>, y_pred[<span class="number">1</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">&quot;y_pred.shape = &quot;</span>, y_pred.shape)</span><br><span class="line">print(<span class="string">&quot;caches[1][1][3] =&quot;</span>, caches[<span class="number">1</span>][<span class="number">1</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">&quot;len(caches) = &quot;</span>, <span class="built_in">len</span>(caches))</span><br></pre></td></tr></table></figure><pre><code>a[4][1] =  [-0.99999375  0.77911235 -0.99861469 -0.99833267]a.shape =  (5, 10, 4)y_pred[1][3] = [ 0.79560373  0.86224861  0.11118257  0.81515947]y_pred.shape =  (2, 10, 4)caches[1][1][3] = [-1.1425182  -0.34934272 -0.20889423  0.58662319]len(caches) =  2</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **a[4][1]**:        </td>        <td>           [-0.99999375  0.77911235 -0.99861469 -0.99833267]        </td>    </tr>        <tr>        <td>            **a.shape**:        </td>        <td>           (5, 10, 4)        </td>    </tr>        <tr>        <td>            **y[1][3]**:        </td>        <td>           [ 0.79560373  0.86224861  0.11118257  0.81515947]        </td>    </tr>        <tr>        <td>            **y.shape**:        </td>        <td>           (2, 10, 4)        </td>    </tr>        <tr>        <td>            **cache[1][1][3]**:        </td>        <td>           [-1.1425182  -0.34934272 -0.20889423  0.58662319]        </td>    </tr>        <tr>        <td>            **len(cache)**:        </td>        <td>           2        </td>    </tr></table><p>Congratulations! You’ve successfully built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output $y^{\langle t \rangle}$ can be estimated using mainly “local” context (meaning information from inputs $x^{\langle t’ \rangle}$ where $t’$ is not too far from $t$). </p><p>In the next part, you will build a more complex LSTM model, which is better at addressing vanishing gradients. The LSTM will be better able to remember a piece of information and keep it saved for many timesteps. </p><h2 id="2-Long-Short-Term-Memory-LSTM-network"><a href="#2-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="2 - Long Short-Term Memory (LSTM) network"></a>2 - Long Short-Term Memory (LSTM) network</h2><p>This following figure shows the operations of an LSTM-cell.</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-132723.jpg" alt=""></p><caption><center> **Figure 4**: LSTM-cell. This tracks and updates a "cell state" or memory variable $c^{\langle t \rangle}$ at every time-step, which can be different from $a^{\langle t \rangle}$. </center></caption><p>Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with $T_x$ time-steps. </p><h3 id="About-the-gates"><a href="#About-the-gates" class="headerlink" title="About the gates"></a>About the gates</h3><h4 id="Forget-gate"><a href="#Forget-gate" class="headerlink" title="- Forget gate"></a>- Forget gate</h4><p>For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this: </p><script type="math/tex; mode=display">\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)\tag{1}</script><p>Here, $W_f$ are weights that govern the forget gate’s behavior. We concatenate $[a^{\langle t-1 \rangle}, x^{\langle t \rangle}]$ and multiply by $W_f$. The equation above results in a vector $\Gamma_f^{\langle t \rangle}$ with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state $c^{\langle t-1 \rangle}$. So if one of the values of $\Gamma_f^{\langle t \rangle}$ is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of $c^{\langle t-1 \rangle}$. If one of the values is 1, then it will keep the information. </p><h4 id="Update-gate"><a href="#Update-gate" class="headerlink" title="- Update gate"></a>- Update gate</h4><p>Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulat for the update gate: </p><script type="math/tex; mode=display">\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^{\{t\}}] + b_u)\tag{2}</script><p>Similar to the forget gate, here $\Gamma_u^{\langle t \rangle}$ is again a vector of values between 0 and 1. This will be multiplied element-wise with $\tilde{c}^{\langle t \rangle}$, in order to compute $c^{\langle t \rangle}$.</p><h4 id="Updating-the-cell"><a href="#Updating-the-cell" class="headerlink" title="- Updating the cell"></a>- Updating the cell</h4><p>To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is: </p><script type="math/tex; mode=display">\tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)\tag{3}</script><p>Finally, the new cell state is: </p><script type="math/tex; mode=display">c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}* c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} *\tilde{c}^{\langle t \rangle} \tag{4}</script><h4 id="Output-gate"><a href="#Output-gate" class="headerlink" title="- Output gate"></a>- Output gate</h4><p>To decide which outputs we will use, we will use the following two formulas: </p><script type="math/tex; mode=display">\Gamma_o^{\langle t \rangle}=  \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)\tag{5}</script><script type="math/tex; mode=display">a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle})\tag{6}</script><p>Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the $\tanh$ of the previous state. </p><h3 id="2-1-LSTM-cell"><a href="#2-1-LSTM-cell" class="headerlink" title="2.1 - LSTM cell"></a>2.1 - LSTM cell</h3><p><strong>Exercise</strong>: Implement the LSTM cell described in the Figure (3).</p><p><strong>Instructions</strong>:</p><ol><li>Concatenate $a^{\langle t-1 \rangle}$ and $x^{\langle t \rangle}$ in a single matrix: $concat = \begin{bmatrix} a^{\langle t-1 \rangle} \ x^{\langle t \rangle} \end{bmatrix}$</li><li>Compute all the formulas 1-6. You can use <code>sigmoid()</code> (provided) and <code>np.tanh()</code>.</li><li>Compute the prediction $y^{\langle t \rangle}$. You can use <code>softmax()</code> (provided).</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_cell_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span>(<span class="params">xt, a_prev, c_prev, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep &quot;t&quot;, numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep &quot;t-1&quot;, numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep &quot;t&quot;, numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from &quot;parameters&quot;</span></span><br><span class="line">    Wf = parameters[<span class="string">&quot;Wf&quot;</span>]</span><br><span class="line">    bf = parameters[<span class="string">&quot;bf&quot;</span>]</span><br><span class="line">    Wi = parameters[<span class="string">&quot;Wi&quot;</span>]</span><br><span class="line">    bi = parameters[<span class="string">&quot;bi&quot;</span>]</span><br><span class="line">    Wc = parameters[<span class="string">&quot;Wc&quot;</span>]</span><br><span class="line">    bc = parameters[<span class="string">&quot;bc&quot;</span>]</span><br><span class="line">    Wo = parameters[<span class="string">&quot;Wo&quot;</span>]</span><br><span class="line">    bo = parameters[<span class="string">&quot;bo&quot;</span>]</span><br><span class="line">    Wy = parameters[<span class="string">&quot;Wy&quot;</span>]</span><br><span class="line">    by = parameters[<span class="string">&quot;by&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈3 lines)</span></span><br><span class="line">    concat = np.zeros((n_a + n_x, m))</span><br><span class="line">    concat[: n_a, :] = a_prev</span><br><span class="line">    concat[n_a :, :] = xt</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft * c_prev + it * cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wy, a_next) + by)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">c_prev = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Wf = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bf = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wi = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bi = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wo = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bo = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wc = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bc = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wy = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">parameters = &#123;<span class="string">&quot;Wf&quot;</span>: Wf, <span class="string">&quot;Wi&quot;</span>: Wi, <span class="string">&quot;Wo&quot;</span>: Wo, <span class="string">&quot;Wc&quot;</span>: Wc, <span class="string">&quot;Wy&quot;</span>: Wy, <span class="string">&quot;bf&quot;</span>: bf, <span class="string">&quot;bi&quot;</span>: bi, <span class="string">&quot;bo&quot;</span>: bo, <span class="string">&quot;bc&quot;</span>: bc, <span class="string">&quot;by&quot;</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a_next, c_next, yt, cache = lstm_cell_forward(xt, a_prev, c_prev, parameters)</span><br><span class="line">print(<span class="string">&quot;a_next[4] = &quot;</span>, a_next[<span class="number">4</span>])</span><br><span class="line">print(<span class="string">&quot;a_next.shape = &quot;</span>, c_next.shape)</span><br><span class="line">print(<span class="string">&quot;c_next[2] = &quot;</span>, c_next[<span class="number">2</span>])</span><br><span class="line">print(<span class="string">&quot;c_next.shape = &quot;</span>, c_next.shape)</span><br><span class="line">print(<span class="string">&quot;yt[1] =&quot;</span>, yt[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;yt.shape = &quot;</span>, yt.shape)</span><br><span class="line">print(<span class="string">&quot;cache[1][3] =&quot;</span>, cache[<span class="number">1</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">&quot;len(cache) = &quot;</span>, <span class="built_in">len</span>(cache))</span><br></pre></td></tr></table></figure><pre><code>a_next[4] =  [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482  0.76566531  0.34631421 -0.00215674  0.43827275]a_next.shape =  (5, 10)c_next[2] =  [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942  0.76449811 -0.0981561  -0.74348425 -0.26810932]c_next.shape =  (5, 10)yt[1] = [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381  0.00943007  0.12666353  0.39380172  0.07828381]yt.shape =  (2, 10)cache[1][3] = [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874  0.07651101 -1.03752894  1.41219977 -0.37647422]len(cache) =  10</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **a_next[4]**:        </td>        <td>           [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482  0.76566531  0.34631421 -0.00215674  0.43827275]        </td>    </tr>        <tr>        <td>            **a_next.shape**:        </td>        <td>           (5, 10)        </td>    </tr>        <tr>        <td>            **c_next[2]**:        </td>        <td>           [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942  0.76449811 -0.0981561  -0.74348425 -0.26810932]        </td>    </tr>        <tr>        <td>            **c_next.shape**:        </td>        <td>           (5, 10)        </td>    </tr>        <tr>        <td>            **yt[1]**:        </td>        <td>           [ 0.79913913  0.15986619  0.22412122  0.15606108  0.97057211  0.31146381  0.00943007  0.12666353  0.39380172  0.07828381]        </td>    </tr>        <tr>        <td>            **yt.shape**:        </td>        <td>           (2, 10)        </td>    </tr>    <tr>        <td>            **cache[1][3]**:        </td>        <td>           [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874  0.07651101 -1.03752894  1.41219977 -0.37647422]        </td>    </tr>        <tr>        <td>            **len(cache)**:        </td>        <td>           10        </td>    </tr></table><h3 id="2-2-Forward-pass-for-LSTM"><a href="#2-2-Forward-pass-for-LSTM" class="headerlink" title="2.2 - Forward pass for LSTM"></a>2.2 - Forward pass for LSTM</h3><p>Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of $T_x$ inputs. </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-29-132817.jpg" alt=""></p><caption><center> **Figure 4**: LSTM over multiple time-steps. </center></caption><p><strong>Exercise:</strong> Implement <code>lstm_forward()</code> to run an LSTM over $T_x$ time-steps. </p><p><strong>Note</strong>: $c^{\langle 0 \rangle}$ is initialized with zeros.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: lstm_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span>(<span class="params">x, a0, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first &quot;tanh&quot;, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first &quot;tanh&quot;, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize &quot;caches&quot;, which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and parameters[&#x27;Wy&#x27;] (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">&quot;Wy&quot;</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize &quot;a&quot;, &quot;c&quot; and &quot;y&quot; with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt, cache = lstm_cell_forward(x[:, :, t], a_next, c_next, parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new &quot;next&quot; hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t]  = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">3</span>,<span class="number">10</span>,<span class="number">7</span>)</span><br><span class="line">a0 = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Wf = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bf = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wi = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bi = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wo = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bo = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wc = np.random.randn(<span class="number">5</span>, <span class="number">5</span>+<span class="number">3</span>)</span><br><span class="line">bc = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">Wy = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">parameters = &#123;<span class="string">&quot;Wf&quot;</span>: Wf, <span class="string">&quot;Wi&quot;</span>: Wi, <span class="string">&quot;Wo&quot;</span>: Wo, <span class="string">&quot;Wc&quot;</span>: Wc, <span class="string">&quot;Wy&quot;</span>: Wy, <span class="string">&quot;bf&quot;</span>: bf, <span class="string">&quot;bi&quot;</span>: bi, <span class="string">&quot;bo&quot;</span>: bo, <span class="string">&quot;bc&quot;</span>: bc, <span class="string">&quot;by&quot;</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a, y, c, caches = lstm_forward(x, a0, parameters)</span><br><span class="line">print(<span class="string">&quot;a[4][3][6] = &quot;</span>, a[<span class="number">4</span>][<span class="number">3</span>][<span class="number">6</span>])</span><br><span class="line">print(<span class="string">&quot;a.shape = &quot;</span>, a.shape)</span><br><span class="line">print(<span class="string">&quot;y[1][4][3] =&quot;</span>, y[<span class="number">1</span>][<span class="number">4</span>][<span class="number">3</span>])</span><br><span class="line">print(<span class="string">&quot;y.shape = &quot;</span>, y.shape)</span><br><span class="line">print(<span class="string">&quot;caches[1][1[1]] =&quot;</span>, caches[<span class="number">1</span>][<span class="number">1</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;c[1][2][1]&quot;</span>, c[<span class="number">1</span>][<span class="number">2</span>][<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;len(caches) = &quot;</span>, <span class="built_in">len</span>(caches))</span><br></pre></td></tr></table></figure><pre><code>a[4][3][6] =  0.172117767533a.shape =  (5, 10, 7)y[1][4][3] = 0.95087346185y.shape =  (2, 10, 7)caches[1][1[1]] = [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139  0.41005165]c[1][2][1] -0.855544916718len(caches) =  2</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **a[4][3][6]** =        </td>        <td>           0.172117767533        </td>    </tr>        <tr>        <td>            **a.shape** =        </td>        <td>           (5, 10, 7)        </td>    </tr>        <tr>        <td>            **y[1][4][3]** =        </td>        <td>           0.95087346185        </td>    </tr>        <tr>        <td>            **y.shape** =        </td>        <td>           (2, 10, 7)        </td>    </tr>        <tr>        <td>            **caches[1][1][1]** =        </td>        <td>           [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139  0.41005165]        </td>     </tr>        <tr>        <td>            **c[1][2][1]** =        </td>        <td>           -0.855544916718        </td>    </tr>           </tr>        <tr>        <td>            **len(caches)** =        </td>        <td>           2        </td>    </tr></table><p>Congratulations! You have now implemented the forward passes for the basic RNN and the LSTM. When using a deep learning framework, implementing the forward pass is sufficient to build systems that achieve great performance. </p><h3 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations !"></a>Congratulations !</h3><p>Congratulations on completing this assignment. You now understand how recurrent neural networks work! </p><p>Lets go on to the next exercise, where you’ll use an RNN to build a character-level language model.</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras 实现 ResNet</title>
      <link href="2019/01/20/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/3%20Keras%20%E5%AE%9E%E7%8E%B0%20ResNet/"/>
      <url>2019/01/20/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/3%20Keras%20%E5%AE%9E%E7%8E%B0%20ResNet/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是 Coursera 的卷积神经网络课程的第一周编程作业 Part2</p></blockquote><a id="more"></a><h1 id="Residual-Networks"><a href="#Residual-Networks" class="headerlink" title="Residual Networks"></a>Residual Networks</h1><p>Welcome to the second assignment of this week! You will learn how to build very deep convolutional networks, using Residual Networks (ResNets). In theory, very deep networks can represent very complex functions; but in practice, they are hard to train. Residual Networks, introduced by <a href="https://arxiv.org/pdf/1512.03385.pdf">He et al.</a>, allow you to train much deeper networks than were previously practically feasible.</p><p><strong>In this assignment, you will:</strong></p><ul><li>Implement the basic building blocks of ResNets. </li><li>Put together these building blocks to implement and train a state-of-the-art neural network for image classification. </li></ul><p>This assignment will be done in Keras. </p><p>Before jumping into the problem, let’s run the cell below to load the required packages.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> resnets_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">&#x27;channels_last&#x27;</span>)</span><br><span class="line">K.set_learning_phase(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><h2 id="1-The-problem-of-very-deep-neural-networks"><a href="#1-The-problem-of-very-deep-neural-networks" class="headerlink" title="1 - The problem of very deep neural networks"></a>1 - The problem of very deep neural networks</h2><p>Last week, you built your first convolutional neural network. In recent years, neural networks have become deeper, with state-of-the-art networks going from just a few layers (e.g., AlexNet) to over a hundred layers.</p><p>The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn’t always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and “explode” to take very large values). </p><p>During training, you might therefore see the magnitude (or norm) of the gradient for the earlier layers descrease to zero very rapidly as training proceeds: </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-125528.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Vanishing gradient** <br> The speed of learning decreases very rapidly for the early layers as the network trains </center></caption>&lt;/font&gt;&lt;/font&gt;</p><p>You are now going to solve this problem by building a Residual Network!</p><h2 id="2-Building-a-Residual-Network"><a href="#2-Building-a-Residual-Network" class="headerlink" title="2 - Building a Residual Network"></a>2 - Building a Residual Network</h2><p>In ResNets, a “shortcut” or a “skip connection” allows the gradient to be directly backpropagated to earlier layers:  </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-125604.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : A ResNet block showing a **skip-connection** <br> </center></caption>&lt;/font&gt;&lt;/font&gt;</p><p>The image on the left shows the “main path” through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network. </p><p>We also saw in lecture that having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. This means that you can stack on additional ResNet blocks with little risk of harming training set performance. (There is also some evidence that the ease of learning an identity function—even more than skip connections helping with vanishing gradients—accounts for ResNets’ remarkable performance.)</p><p>Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. You are going to implement both of them. </p><h3 id="2-1-The-identity-block"><a href="#2-1-The-identity-block" class="headerlink" title="2.1 - The identity block"></a>2.1 - The identity block</h3><p>The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). To flesh out the different steps of what happens in a ResNet’s identity block, here is an alternative diagram showing the individual steps:</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-125647.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **Identity block.** Skip connection "skips over" 2 layers. </center></caption>&lt;/font&gt;&lt;/font&gt;</p><p>The upper path is the “shortcut path.” The lower path is the “main path.” In this diagram, we have also made explicit the CONV2D and ReLU steps in each layer. To speed up training we have also added a BatchNorm step. Don’t worry about this being complicated to implement—you’ll see that BatchNorm is just one line of code in Keras! </p><p>In this exercise, you’ll actually implement a slightly more powerful version of this identity block, in which the skip connection “skips over” 3 hidden layers rather than 2 layers. It looks like this: </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-125716.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Identity block.** Skip connection "skips over" 3 layers.</center></caption>&lt;/font&gt;&lt;/font&gt;</p><p>Here’re the individual steps.</p><p>First component of main path: </p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Final step: </p><ul><li>The shortcut and the input are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p><strong>Exercise</strong>: Implement the ResNet identity block. We have implemented the first component of the main path. Please read over this carefully to make sure you understand what it is doing. You should implement the rest. </p><ul><li>To implement the Conv2D step: <a href="https://keras.io/layers/convolutional/#conv2d">See reference</a></li><li>To implement BatchNorm: <a href="https://faroit.github.io/keras-docs/1.2.2/layers/normalization/">See reference</a> (axis: Integer, the axis that should be normalized (typically the channels axis))</li><li>For the activation, use:  <code>Activation(&#39;relu&#39;)(X)</code></li><li>To add the value passed forward by the shortcut: <a href="https://keras.io/layers/merge/#add">See reference</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: identity_block</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span>(<span class="params">X, f, filters, stage, block</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV&#x27;s window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">&#x27;res&#x27;</span> + <span class="built_in">str</span>(stage) + block + <span class="string">&#x27;_branch&#x27;</span></span><br><span class="line">    bn_name_base = <span class="string">&#x27;bn&#x27;</span> + <span class="built_in">str</span>(stage) + block + <span class="string">&#x27;_branch&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You&#x27;ll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">&#x27;valid&#x27;</span>, name = conv_name_base + <span class="string">&#x27;2a&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;2a&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">&#x27;same&#x27;</span>, name = conv_name_base + <span class="string">&#x27;2b&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;2b&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters = F3, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">&#x27;valid&#x27;</span>, name = conv_name_base + <span class="string">&#x27;2c&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;2c&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X_shortcut, X])</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    A_prev = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line">    X = np.random.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line">    A = identity_block(A_prev, f = <span class="number">2</span>, filters = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>], stage = <span class="number">1</span>, block = <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    test.run(tf.global_variables_initializer())</span><br><span class="line">    out = test.run([A], feed_dict=&#123;A_prev: X, K.learning_phase(): <span class="number">0</span>&#125;)</span><br><span class="line">    print(<span class="string">&quot;out = &quot;</span> + <span class="built_in">str</span>(out[<span class="number">0</span>][<span class="number">1</span>][<span class="number">1</span>][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>out = [ 0.94822985  0.          1.16101444  2.747859    0.          1.36677003]</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **out**        </td>        <td>           [ 0.94822985  0.          1.16101444  2.747859    0.          1.36677003]        </td>    </tr></table><h2 id="2-2-The-convolutional-block"><a href="#2-2-The-convolutional-block" class="headerlink" title="2.2 - The convolutional block"></a>2.2 - The convolutional block</h2><p>You’ve implemented the ResNet identity block. Next, the ResNet “convolutional block” is the other type of block. You can use this type of block when the input and output dimensions don’t match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path: </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-125820.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Convolutional block** </center></caption>&lt;/font&gt;&lt;/font&gt;</p><p>The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix $W_s$ discussed in lecture.) For example, to reduce the activation dimensions’s height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. </p><p>The details of the convolutional block are as follows. </p><p>First component of main path:</p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Shortcut path:</p><ul><li>The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li><li>The BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;1&#39;</code>. </li></ul><p>Final step: </p><ul><li>The shortcut and the main path values are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p><strong>Exercise</strong>: Implement the convolutional block. We have implemented the first component of the main path; you should implement the rest. As before, always use 0 as the seed for the random initialization, to ensure consistency with our grader.</p><ul><li><a href="https://keras.io/layers/convolutional/#conv2d">Conv Hint</a></li><li><a href="https://keras.io/layers/normalization/#batchnormalization">BatchNorm Hint</a> (axis: Integer, the axis that should be normalized (typically the features axis))</li><li>For the activation, use:  <code>Activation(&#39;relu&#39;)(X)</code></li><li><a href="https://keras.io/layers/merge/#add">Addition Hint</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: convolutional_block</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span>(<span class="params">X, f, filters, stage, block, s = <span class="number">2</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implementation of the convolutional block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV&#x27;s window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    s -- Integer, specifying the stride to be used</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">&#x27;res&#x27;</span> + <span class="built_in">str</span>(stage) + block + <span class="string">&#x27;_branch&#x27;</span></span><br><span class="line">    bn_name_base = <span class="string">&#x27;bn&#x27;</span> + <span class="built_in">str</span>(stage) + block + <span class="string">&#x27;_branch&#x27;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value</span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path </span></span><br><span class="line">    X = Conv2D(F1, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">&#x27;2a&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;2a&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(F2, (f, f), strides = (<span class="number">1</span>,<span class="number">1</span>), padding = <span class="string">&#x27;same&#x27;</span>, name = conv_name_base + <span class="string">&#x27;2b&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;2b&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (<span class="number">1</span>,<span class="number">1</span>), name = conv_name_base + <span class="string">&#x27;2c&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;2c&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class="line">    X_shortcut = Conv2D(F3, (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), name = conv_name_base + <span class="string">&#x27;1&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">&#x27;1&#x27;</span>)(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X_shortcut, X])</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> test:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    A_prev = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>])</span><br><span class="line">    X = np.random.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">6</span>)</span><br><span class="line">    A = convolutional_block(A_prev, f = <span class="number">2</span>, filters = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>], stage = <span class="number">1</span>, block = <span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    test.run(tf.global_variables_initializer())</span><br><span class="line">    out = test.run([A], feed_dict=&#123;A_prev: X, K.learning_phase(): <span class="number">0</span>&#125;)</span><br><span class="line">    print(<span class="string">&quot;out = &quot;</span> + <span class="built_in">str</span>(out[<span class="number">0</span>][<span class="number">1</span>][<span class="number">1</span>][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><pre><code>out = [ 0.09018463  1.23489773  0.46822017  0.0367176   0.          0.65516603]</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **out**        </td>        <td>           [ 0.09018463  1.23489773  0.46822017  0.0367176   0.          0.65516603]        </td>    </tr></table><h2 id="3-Building-your-first-ResNet-model-50-layers"><a href="#3-Building-your-first-ResNet-model-50-layers" class="headerlink" title="3 - Building your first ResNet model (50 layers)"></a>3 - Building your first ResNet model (50 layers)</h2><p>You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. “ID BLOCK” in the diagram stands for “Identity block,” and “ID BLOCK x3” means you should stack 3 identity blocks together.</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-125927.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 5** </u><font color='purple'>  : **ResNet-50 model** </center></caption>&lt;/font&gt;&lt;/font&gt;</p><p>The details of this ResNet-50 model are:</p><ul><li>Zero-padding pads the input with a pad of (3,3)</li><li>Stage 1:<ul><li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li><li>BatchNorm is applied to the channels axis of the input.</li><li>MaxPooling uses a (3,3) window and a (2,2) stride.</li></ul></li><li>Stage 2:<ul><li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>Stage 3:<ul><li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li></ul></li><li>Stage 4:<ul><li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li></ul></li><li>Stage 5:<ul><li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [512, 512, 2048], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li><li>The flatten doesn’t have any hyperparameters or name.</li><li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li></ul><p><strong>Exercise</strong>: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. </p><p>You’ll need to use this function: </p><ul><li>Average pooling <a href="https://keras.io/layers/pooling/#averagepooling2d">see reference</a></li></ul><p>Here’re some other functions we used in the code below:</p><ul><li>Conv2D: <a href="https://keras.io/layers/convolutional/#conv2d">See reference</a></li><li>BatchNorm: <a href="https://keras.io/layers/normalization/#batchnormalization">See reference</a> (axis: Integer, the axis that should be normalized (typically the features axis))</li><li>Zero padding: <a href="https://keras.io/layers/convolutional/#zeropadding2d">See reference</a></li><li>Max pooling: <a href="https://keras.io/layers/pooling/#maxpooling2d">See reference</a></li><li>Fully conected layer: <a href="https://keras.io/layers/core/#dense">See reference</a></li><li>Addition: <a href="https://keras.io/layers/merge/#add">See reference</a></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: ResNet50</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span>(<span class="params">input_shape = (<span class="params"><span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span></span>), classes = <span class="number">6</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">&#x27;conv1&#x27;</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">&#x27;bn_conv1&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>, <span class="number">3</span>), strides=(<span class="number">2</span>, <span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage = <span class="number">2</span>, block=<span class="string">&#x27;a&#x27;</span>, s = <span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>, <span class="number">64</span>, <span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3 (≈4 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage = <span class="number">3</span>, block=<span class="string">&#x27;a&#x27;</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>, <span class="number">128</span>, <span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4 (≈6 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage = <span class="number">4</span>, block=<span class="string">&#x27;a&#x27;</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;d&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;e&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>, <span class="number">256</span>, <span class="number">1024</span>], stage=<span class="number">4</span>, block=<span class="string">&#x27;f&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5 (≈3 lines)</span></span><br><span class="line">    X = convolutional_block(X, f = <span class="number">3</span>, filters = [<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>], stage = <span class="number">5</span>, block=<span class="string">&#x27;a&#x27;</span>, s = <span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>], stage=<span class="number">5</span>, block=<span class="string">&#x27;b&#x27;</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">512</span>, <span class="number">512</span>, <span class="number">2048</span>], stage=<span class="number">5</span>, block=<span class="string">&#x27;c&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL (≈1 line). Use &quot;X = AveragePooling2D(...)(X)&quot;</span></span><br><span class="line">    X = AveragePooling2D((<span class="number">2</span>, <span class="number">2</span>), name = <span class="string">&#x27;avg_pool&#x27;</span>)(X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># output layer</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(classes, activation=<span class="string">&#x27;softmax&#x27;</span>, name=<span class="string">&#x27;fc&#x27;</span> + <span class="built_in">str</span>(classes), kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create model</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">&#x27;ResNet50&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>Run the following code to build the model’s graph. If your implementation is not correct you will know it by checking your accuracy when running <code>model.fit(...)</code> below.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = ResNet50(input_shape = (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes = <span class="number">6</span>)</span><br></pre></td></tr></table></figure><p>As seen in the Keras Tutorial Notebook, prior training a model, you need to configure the learning process by compiling the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure><p>The model is now ready to be trained. The only thing you need is a dataset.</p><p>Let’s load the SIGNS Dataset.</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-130053.jpg" alt=""></p><p><caption><center> <u> <font color='purple'> **Figure 6** </u><font color='purple'>  : **SIGNS dataset** </center></caption>&lt;/font&gt;&lt;/font&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert training and test labels to one hot matrices</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;number of training examples = &quot;</span> + <span class="built_in">str</span>(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;number of test examples = &quot;</span> + <span class="built_in">str</span>(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X_train shape: &quot;</span> + <span class="built_in">str</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y_train shape: &quot;</span> + <span class="built_in">str</span>(Y_train.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X_test shape: &quot;</span> + <span class="built_in">str</span>(X_test.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y_test shape: &quot;</span> + <span class="built_in">str</span>(Y_test.shape))</span><br></pre></td></tr></table></figure><pre><code>number of training examples = 1080number of test examples = 120X_train shape: (1080, 64, 64, 3)Y_train shape: (1080, 6)X_test shape: (120, 64, 64, 3)Y_test shape: (120, 6)</code></pre><p>Run the following cell to train your model on 2 epochs with a batch size of 32. On a CPU it should take you around 5min per epoch. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">2</span>, batch_size = <span class="number">32</span>)</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/21080/1080 [==============================] - 243s - loss: 2.9600 - acc: 0.2630   Epoch 2/21080/1080 [==============================] - 236s - loss: 2.2507 - acc: 0.3435   &lt;keras.callbacks.History at 0x7f858f129f60&gt;</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            ** Epoch 1/2**        </td>        <td>           loss: between 1 and 5, acc: between 0.2 and 0.5, although your results can be different from ours.        </td>    </tr>    <tr>        <td>            ** Epoch 2/2**        </td>        <td>           loss: between 1 and 5, acc: between 0.2 and 0.5, you should see your loss decreasing and the accuracy increasing.        </td>    </tr></table><p>Let’s see how this model (trained on only two epochs) performs on the test set.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">preds = model.evaluate(X_test, Y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Loss = &quot;</span> + <span class="built_in">str</span>(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Test Accuracy = &quot;</span> + <span class="built_in">str</span>(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>120/120 [==============================] - 9s     Loss = 2.26055526733Test Accuracy = 0.166666666667</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **Test Accuracy**        </td>        <td>           between 0.16 and 0.25        </td>    </tr></table><p>For the purpose of this assignment, we’ve asked you to train the model only for two epochs. You can see that it achieves poor performances. Please go ahead and submit your assignment; to check correctness, the online grader will run your code only for a small number of epochs as well.</p><p>After you have finished this official (graded) part of this assignment, you can also optionally train the ResNet for more iterations, if you want. We get a lot better performance when we train for ~20 epochs, but this will take more than an hour when training on a CPU. </p><p>Using a GPU, we’ve trained our own ResNet50 model’s weights on the SIGNS dataset. You can load and run our trained model on the test set in the cells below. It may take ≈1min to load the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = load_model(<span class="string">&#x27;ResNet50.h5&#x27;</span>) </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">preds = model.evaluate(X_test, Y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Loss = &quot;</span> + <span class="built_in">str</span>(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Test Accuracy = &quot;</span> + <span class="built_in">str</span>(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>120/120 [==============================] - 7s     Loss = 0.255690792203Test Accuracy = 0.91666667064</code></pre><p>ResNet50 is a powerful model for image classification when it is trained for an adequate number of iterations. We hope you can use what you’ve learnt and apply it to your own classification problem to perform state-of-the-art accuracy.</p><p>Congratulations on finishing this assignment! You’ve now implemented a state-of-the-art image classification system! </p><h2 id="4-Test-on-your-own-image-Optional-Ungraded"><a href="#4-Test-on-your-own-image-Optional-Ungraded" class="headerlink" title="4 - Test on your own image (Optional/Ungraded)"></a>4 - Test on your own image (Optional/Ungraded)</h2><p>If you wish, you can also take a picture of your own hand and see the output of the model. To do this:</p><pre><code>1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub.2. Add your image to this Jupyter Notebook&#39;s directory, in the &quot;images&quot; folder3. Write your image&#39;s name in the following code4. Run the code and check if the algorithm is right! </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">img_path = <span class="string">&#x27;images/my_image.jpg&#x27;</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line">print(<span class="string">&#x27;Input image shape:&#x27;</span>, x.shape)</span><br><span class="line">my_image = scipy.misc.imread(img_path)</span><br><span class="line">imshow(my_image)</span><br><span class="line">print(<span class="string">&quot;class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] = &quot;</span>)</span><br><span class="line">print(model.predict(x))</span><br></pre></td></tr></table></figure><p>Input image shape: (1, 64, 64, 3)</p><p>class prediction vector [p(0), p(1), p(2), p(3), p(4), p(5)] =<br>[[ 1.  0.  0.  0.  0.  0.]]</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-131509.jpg" alt=""></p><p>You can also print a summary of your model by running the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>____________________________________________________________________________________________________Layer (type)                     Output Shape          Param #     Connected to                     ====================================================================================================input_1 (InputLayer)             (None, 64, 64, 3)     0                                            ____________________________________________________________________________________________________zero_padding2d_1 (ZeroPadding2D) (None, 70, 70, 3)     0           input_1[0][0]                    ____________________________________________________________________________________________________conv1 (Conv2D)                   (None, 32, 32, 64)    9472        zero_padding2d_1[0][0]           ____________________________________________________________________________________________________bn_conv1 (BatchNormalization)    (None, 32, 32, 64)    256         conv1[0][0]                      ____________________________________________________________________________________________________activation_4 (Activation)        (None, 32, 32, 64)    0           bn_conv1[0][0]                   ____________________________________________________________________________________________________max_pooling2d_1 (MaxPooling2D)   (None, 15, 15, 64)    0           activation_4[0][0]               ____________________________________________________________________________________________________res2a_branch2a (Conv2D)          (None, 15, 15, 64)    4160        max_pooling2d_1[0][0]            ____________________________________________________________________________________________________bn2a_branch2a (BatchNormalizatio (None, 15, 15, 64)    256         res2a_branch2a[0][0]             ____________________________________________________________________________________________________activation_5 (Activation)        (None, 15, 15, 64)    0           bn2a_branch2a[0][0]              ____________________________________________________________________________________________________res2a_branch2b (Conv2D)          (None, 15, 15, 64)    36928       activation_5[0][0]               ____________________________________________________________________________________________________bn2a_branch2b (BatchNormalizatio (None, 15, 15, 64)    256         res2a_branch2b[0][0]             ____________________________________________________________________________________________________activation_6 (Activation)        (None, 15, 15, 64)    0           bn2a_branch2b[0][0]              ____________________________________________________________________________________________________res2a_branch2c (Conv2D)          (None, 15, 15, 256)   16640       activation_6[0][0]               ____________________________________________________________________________________________________res2a_branch1 (Conv2D)           (None, 15, 15, 256)   16640       max_pooling2d_1[0][0]            ____________________________________________________________________________________________________bn2a_branch2c (BatchNormalizatio (None, 15, 15, 256)   1024        res2a_branch2c[0][0]             ____________________________________________________________________________________________________bn2a_branch1 (BatchNormalization (None, 15, 15, 256)   1024        res2a_branch1[0][0]              ____________________________________________________________________________________________________add_2 (Add)                      (None, 15, 15, 256)   0           bn2a_branch2c[0][0]                                                                                 bn2a_branch1[0][0]               ____________________________________________________________________________________________________activation_7 (Activation)        (None, 15, 15, 256)   0           add_2[0][0]                      ____________________________________________________________________________________________________res2b_branch2a (Conv2D)          (None, 15, 15, 64)    16448       activation_7[0][0]               ____________________________________________________________________________________________________bn2b_branch2a (BatchNormalizatio (None, 15, 15, 64)    256         res2b_branch2a[0][0]             ____________________________________________________________________________________________________activation_8 (Activation)        (None, 15, 15, 64)    0           bn2b_branch2a[0][0]              ____________________________________________________________________________________________________res2b_branch2b (Conv2D)          (None, 15, 15, 64)    36928       activation_8[0][0]               ____________________________________________________________________________________________________bn2b_branch2b (BatchNormalizatio (None, 15, 15, 64)    256         res2b_branch2b[0][0]             ____________________________________________________________________________________________________activation_9 (Activation)        (None, 15, 15, 64)    0           bn2b_branch2b[0][0]              ____________________________________________________________________________________________________res2b_branch2c (Conv2D)          (None, 15, 15, 256)   16640       activation_9[0][0]               ____________________________________________________________________________________________________bn2b_branch2c (BatchNormalizatio (None, 15, 15, 256)   1024        res2b_branch2c[0][0]             ____________________________________________________________________________________________________add_3 (Add)                      (None, 15, 15, 256)   0           bn2b_branch2c[0][0]                                                                                 activation_7[0][0]               ____________________________________________________________________________________________________activation_10 (Activation)       (None, 15, 15, 256)   0           add_3[0][0]                      ____________________________________________________________________________________________________res2c_branch2a (Conv2D)          (None, 15, 15, 64)    16448       activation_10[0][0]              ____________________________________________________________________________________________________bn2c_branch2a (BatchNormalizatio (None, 15, 15, 64)    256         res2c_branch2a[0][0]             ____________________________________________________________________________________________________activation_11 (Activation)       (None, 15, 15, 64)    0           bn2c_branch2a[0][0]              ____________________________________________________________________________________________________res2c_branch2b (Conv2D)          (None, 15, 15, 64)    36928       activation_11[0][0]              ____________________________________________________________________________________________________bn2c_branch2b (BatchNormalizatio (None, 15, 15, 64)    256         res2c_branch2b[0][0]             ____________________________________________________________________________________________________activation_12 (Activation)       (None, 15, 15, 64)    0           bn2c_branch2b[0][0]              ____________________________________________________________________________________________________res2c_branch2c (Conv2D)          (None, 15, 15, 256)   16640       activation_12[0][0]              ____________________________________________________________________________________________________bn2c_branch2c (BatchNormalizatio (None, 15, 15, 256)   1024        res2c_branch2c[0][0]             ____________________________________________________________________________________________________add_4 (Add)                      (None, 15, 15, 256)   0           bn2c_branch2c[0][0]                                                                                 activation_10[0][0]              ____________________________________________________________________________________________________activation_13 (Activation)       (None, 15, 15, 256)   0           add_4[0][0]                      ____________________________________________________________________________________________________res3a_branch2a (Conv2D)          (None, 8, 8, 128)     32896       activation_13[0][0]              ____________________________________________________________________________________________________bn3a_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3a_branch2a[0][0]             ____________________________________________________________________________________________________activation_14 (Activation)       (None, 8, 8, 128)     0           bn3a_branch2a[0][0]              ____________________________________________________________________________________________________res3a_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_14[0][0]              ____________________________________________________________________________________________________bn3a_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3a_branch2b[0][0]             ____________________________________________________________________________________________________activation_15 (Activation)       (None, 8, 8, 128)     0           bn3a_branch2b[0][0]              ____________________________________________________________________________________________________res3a_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_15[0][0]              ____________________________________________________________________________________________________res3a_branch1 (Conv2D)           (None, 8, 8, 512)     131584      activation_13[0][0]              ____________________________________________________________________________________________________bn3a_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3a_branch2c[0][0]             ____________________________________________________________________________________________________bn3a_branch1 (BatchNormalization (None, 8, 8, 512)     2048        res3a_branch1[0][0]              ____________________________________________________________________________________________________add_5 (Add)                      (None, 8, 8, 512)     0           bn3a_branch2c[0][0]                                                                                 bn3a_branch1[0][0]               ____________________________________________________________________________________________________activation_16 (Activation)       (None, 8, 8, 512)     0           add_5[0][0]                      ____________________________________________________________________________________________________res3b_branch2a (Conv2D)          (None, 8, 8, 128)     65664       activation_16[0][0]              ____________________________________________________________________________________________________bn3b_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3b_branch2a[0][0]             ____________________________________________________________________________________________________activation_17 (Activation)       (None, 8, 8, 128)     0           bn3b_branch2a[0][0]              ____________________________________________________________________________________________________res3b_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_17[0][0]              ____________________________________________________________________________________________________bn3b_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3b_branch2b[0][0]             ____________________________________________________________________________________________________activation_18 (Activation)       (None, 8, 8, 128)     0           bn3b_branch2b[0][0]              ____________________________________________________________________________________________________res3b_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_18[0][0]              ____________________________________________________________________________________________________bn3b_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3b_branch2c[0][0]             ____________________________________________________________________________________________________add_6 (Add)                      (None, 8, 8, 512)     0           bn3b_branch2c[0][0]                                                                                 activation_16[0][0]              ____________________________________________________________________________________________________activation_19 (Activation)       (None, 8, 8, 512)     0           add_6[0][0]                      ____________________________________________________________________________________________________res3c_branch2a (Conv2D)          (None, 8, 8, 128)     65664       activation_19[0][0]              ____________________________________________________________________________________________________bn3c_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3c_branch2a[0][0]             ____________________________________________________________________________________________________activation_20 (Activation)       (None, 8, 8, 128)     0           bn3c_branch2a[0][0]              ____________________________________________________________________________________________________res3c_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_20[0][0]              ____________________________________________________________________________________________________bn3c_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3c_branch2b[0][0]             ____________________________________________________________________________________________________activation_21 (Activation)       (None, 8, 8, 128)     0           bn3c_branch2b[0][0]              ____________________________________________________________________________________________________res3c_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_21[0][0]              ____________________________________________________________________________________________________bn3c_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3c_branch2c[0][0]             ____________________________________________________________________________________________________add_7 (Add)                      (None, 8, 8, 512)     0           bn3c_branch2c[0][0]                                                                                 activation_19[0][0]              ____________________________________________________________________________________________________activation_22 (Activation)       (None, 8, 8, 512)     0           add_7[0][0]                      ____________________________________________________________________________________________________res3d_branch2a (Conv2D)          (None, 8, 8, 128)     65664       activation_22[0][0]              ____________________________________________________________________________________________________bn3d_branch2a (BatchNormalizatio (None, 8, 8, 128)     512         res3d_branch2a[0][0]             ____________________________________________________________________________________________________activation_23 (Activation)       (None, 8, 8, 128)     0           bn3d_branch2a[0][0]              ____________________________________________________________________________________________________res3d_branch2b (Conv2D)          (None, 8, 8, 128)     147584      activation_23[0][0]              ____________________________________________________________________________________________________bn3d_branch2b (BatchNormalizatio (None, 8, 8, 128)     512         res3d_branch2b[0][0]             ____________________________________________________________________________________________________activation_24 (Activation)       (None, 8, 8, 128)     0           bn3d_branch2b[0][0]              ____________________________________________________________________________________________________res3d_branch2c (Conv2D)          (None, 8, 8, 512)     66048       activation_24[0][0]              ____________________________________________________________________________________________________bn3d_branch2c (BatchNormalizatio (None, 8, 8, 512)     2048        res3d_branch2c[0][0]             ____________________________________________________________________________________________________add_8 (Add)                      (None, 8, 8, 512)     0           bn3d_branch2c[0][0]                                                                                 activation_22[0][0]              ____________________________________________________________________________________________________activation_25 (Activation)       (None, 8, 8, 512)     0           add_8[0][0]                      ____________________________________________________________________________________________________res4a_branch2a (Conv2D)          (None, 4, 4, 256)     131328      activation_25[0][0]              ____________________________________________________________________________________________________bn4a_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4a_branch2a[0][0]             ____________________________________________________________________________________________________activation_26 (Activation)       (None, 4, 4, 256)     0           bn4a_branch2a[0][0]              ____________________________________________________________________________________________________res4a_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_26[0][0]              ____________________________________________________________________________________________________bn4a_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4a_branch2b[0][0]             ____________________________________________________________________________________________________activation_27 (Activation)       (None, 4, 4, 256)     0           bn4a_branch2b[0][0]              ____________________________________________________________________________________________________res4a_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_27[0][0]              ____________________________________________________________________________________________________res4a_branch1 (Conv2D)           (None, 4, 4, 1024)    525312      activation_25[0][0]              ____________________________________________________________________________________________________bn4a_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4a_branch2c[0][0]             ____________________________________________________________________________________________________bn4a_branch1 (BatchNormalization (None, 4, 4, 1024)    4096        res4a_branch1[0][0]              ____________________________________________________________________________________________________add_9 (Add)                      (None, 4, 4, 1024)    0           bn4a_branch2c[0][0]                                                                                 bn4a_branch1[0][0]               ____________________________________________________________________________________________________activation_28 (Activation)       (None, 4, 4, 1024)    0           add_9[0][0]                      ____________________________________________________________________________________________________res4b_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_28[0][0]              ____________________________________________________________________________________________________bn4b_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4b_branch2a[0][0]             ____________________________________________________________________________________________________activation_29 (Activation)       (None, 4, 4, 256)     0           bn4b_branch2a[0][0]              ____________________________________________________________________________________________________res4b_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_29[0][0]              ____________________________________________________________________________________________________bn4b_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4b_branch2b[0][0]             ____________________________________________________________________________________________________activation_30 (Activation)       (None, 4, 4, 256)     0           bn4b_branch2b[0][0]              ____________________________________________________________________________________________________res4b_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_30[0][0]              ____________________________________________________________________________________________________bn4b_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4b_branch2c[0][0]             ____________________________________________________________________________________________________add_10 (Add)                     (None, 4, 4, 1024)    0           bn4b_branch2c[0][0]                                                                                 activation_28[0][0]              ____________________________________________________________________________________________________activation_31 (Activation)       (None, 4, 4, 1024)    0           add_10[0][0]                     ____________________________________________________________________________________________________res4c_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_31[0][0]              ____________________________________________________________________________________________________bn4c_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4c_branch2a[0][0]             ____________________________________________________________________________________________________activation_32 (Activation)       (None, 4, 4, 256)     0           bn4c_branch2a[0][0]              ____________________________________________________________________________________________________res4c_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_32[0][0]              ____________________________________________________________________________________________________bn4c_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4c_branch2b[0][0]             ____________________________________________________________________________________________________activation_33 (Activation)       (None, 4, 4, 256)     0           bn4c_branch2b[0][0]              ____________________________________________________________________________________________________res4c_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_33[0][0]              ____________________________________________________________________________________________________bn4c_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4c_branch2c[0][0]             ____________________________________________________________________________________________________add_11 (Add)                     (None, 4, 4, 1024)    0           bn4c_branch2c[0][0]                                                                                 activation_31[0][0]              ____________________________________________________________________________________________________activation_34 (Activation)       (None, 4, 4, 1024)    0           add_11[0][0]                     ____________________________________________________________________________________________________res4d_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_34[0][0]              ____________________________________________________________________________________________________bn4d_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4d_branch2a[0][0]             ____________________________________________________________________________________________________activation_35 (Activation)       (None, 4, 4, 256)     0           bn4d_branch2a[0][0]              ____________________________________________________________________________________________________res4d_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_35[0][0]              ____________________________________________________________________________________________________bn4d_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4d_branch2b[0][0]             ____________________________________________________________________________________________________activation_36 (Activation)       (None, 4, 4, 256)     0           bn4d_branch2b[0][0]              ____________________________________________________________________________________________________res4d_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_36[0][0]              ____________________________________________________________________________________________________bn4d_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4d_branch2c[0][0]             ____________________________________________________________________________________________________add_12 (Add)                     (None, 4, 4, 1024)    0           bn4d_branch2c[0][0]                                                                                 activation_34[0][0]              ____________________________________________________________________________________________________activation_37 (Activation)       (None, 4, 4, 1024)    0           add_12[0][0]                     ____________________________________________________________________________________________________res4e_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_37[0][0]              ____________________________________________________________________________________________________bn4e_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4e_branch2a[0][0]             ____________________________________________________________________________________________________activation_38 (Activation)       (None, 4, 4, 256)     0           bn4e_branch2a[0][0]              ____________________________________________________________________________________________________res4e_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_38[0][0]              ____________________________________________________________________________________________________bn4e_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4e_branch2b[0][0]             ____________________________________________________________________________________________________activation_39 (Activation)       (None, 4, 4, 256)     0           bn4e_branch2b[0][0]              ____________________________________________________________________________________________________res4e_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_39[0][0]              ____________________________________________________________________________________________________bn4e_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4e_branch2c[0][0]             ____________________________________________________________________________________________________add_13 (Add)                     (None, 4, 4, 1024)    0           bn4e_branch2c[0][0]                                                                                 activation_37[0][0]              ____________________________________________________________________________________________________activation_40 (Activation)       (None, 4, 4, 1024)    0           add_13[0][0]                     ____________________________________________________________________________________________________res4f_branch2a (Conv2D)          (None, 4, 4, 256)     262400      activation_40[0][0]              ____________________________________________________________________________________________________bn4f_branch2a (BatchNormalizatio (None, 4, 4, 256)     1024        res4f_branch2a[0][0]             ____________________________________________________________________________________________________activation_41 (Activation)       (None, 4, 4, 256)     0           bn4f_branch2a[0][0]              ____________________________________________________________________________________________________res4f_branch2b (Conv2D)          (None, 4, 4, 256)     590080      activation_41[0][0]              ____________________________________________________________________________________________________bn4f_branch2b (BatchNormalizatio (None, 4, 4, 256)     1024        res4f_branch2b[0][0]             ____________________________________________________________________________________________________activation_42 (Activation)       (None, 4, 4, 256)     0           bn4f_branch2b[0][0]              ____________________________________________________________________________________________________res4f_branch2c (Conv2D)          (None, 4, 4, 1024)    263168      activation_42[0][0]              ____________________________________________________________________________________________________bn4f_branch2c (BatchNormalizatio (None, 4, 4, 1024)    4096        res4f_branch2c[0][0]             ____________________________________________________________________________________________________add_14 (Add)                     (None, 4, 4, 1024)    0           bn4f_branch2c[0][0]                                                                                 activation_40[0][0]              ____________________________________________________________________________________________________activation_43 (Activation)       (None, 4, 4, 1024)    0           add_14[0][0]                     ____________________________________________________________________________________________________res5a_branch2a (Conv2D)          (None, 2, 2, 512)     524800      activation_43[0][0]              ____________________________________________________________________________________________________bn5a_branch2a (BatchNormalizatio (None, 2, 2, 512)     2048        res5a_branch2a[0][0]             ____________________________________________________________________________________________________activation_44 (Activation)       (None, 2, 2, 512)     0           bn5a_branch2a[0][0]              ____________________________________________________________________________________________________res5a_branch2b (Conv2D)          (None, 2, 2, 512)     2359808     activation_44[0][0]              ____________________________________________________________________________________________________bn5a_branch2b (BatchNormalizatio (None, 2, 2, 512)     2048        res5a_branch2b[0][0]             ____________________________________________________________________________________________________activation_45 (Activation)       (None, 2, 2, 512)     0           bn5a_branch2b[0][0]              ____________________________________________________________________________________________________res5a_branch2c (Conv2D)          (None, 2, 2, 2048)    1050624     activation_45[0][0]              ____________________________________________________________________________________________________res5a_branch1 (Conv2D)           (None, 2, 2, 2048)    2099200     activation_43[0][0]              ____________________________________________________________________________________________________bn5a_branch2c (BatchNormalizatio (None, 2, 2, 2048)    8192        res5a_branch2c[0][0]             ____________________________________________________________________________________________________bn5a_branch1 (BatchNormalization (None, 2, 2, 2048)    8192        res5a_branch1[0][0]              ____________________________________________________________________________________________________add_15 (Add)                     (None, 2, 2, 2048)    0           bn5a_branch2c[0][0]                                                                                 bn5a_branch1[0][0]               ____________________________________________________________________________________________________activation_46 (Activation)       (None, 2, 2, 2048)    0           add_15[0][0]                     ____________________________________________________________________________________________________res5b_branch2a (Conv2D)          (None, 2, 2, 512)     1049088     activation_46[0][0]              ____________________________________________________________________________________________________bn5b_branch2a (BatchNormalizatio (None, 2, 2, 512)     2048        res5b_branch2a[0][0]             ____________________________________________________________________________________________________activation_47 (Activation)       (None, 2, 2, 512)     0           bn5b_branch2a[0][0]              ____________________________________________________________________________________________________res5b_branch2b (Conv2D)          (None, 2, 2, 512)     2359808     activation_47[0][0]              ____________________________________________________________________________________________________bn5b_branch2b (BatchNormalizatio (None, 2, 2, 512)     2048        res5b_branch2b[0][0]             ____________________________________________________________________________________________________activation_48 (Activation)       (None, 2, 2, 512)     0           bn5b_branch2b[0][0]              ____________________________________________________________________________________________________res5b_branch2c (Conv2D)          (None, 2, 2, 2048)    1050624     activation_48[0][0]              ____________________________________________________________________________________________________bn5b_branch2c (BatchNormalizatio (None, 2, 2, 2048)    8192        res5b_branch2c[0][0]             ____________________________________________________________________________________________________add_16 (Add)                     (None, 2, 2, 2048)    0           bn5b_branch2c[0][0]                                                                                 activation_46[0][0]              ____________________________________________________________________________________________________activation_49 (Activation)       (None, 2, 2, 2048)    0           add_16[0][0]                     ____________________________________________________________________________________________________res5c_branch2a (Conv2D)          (None, 2, 2, 512)     1049088     activation_49[0][0]              ____________________________________________________________________________________________________bn5c_branch2a (BatchNormalizatio (None, 2, 2, 512)     2048        res5c_branch2a[0][0]             ____________________________________________________________________________________________________activation_50 (Activation)       (None, 2, 2, 512)     0           bn5c_branch2a[0][0]              ____________________________________________________________________________________________________res5c_branch2b (Conv2D)          (None, 2, 2, 512)     2359808     activation_50[0][0]              ____________________________________________________________________________________________________bn5c_branch2b (BatchNormalizatio (None, 2, 2, 512)     2048        res5c_branch2b[0][0]             ____________________________________________________________________________________________________activation_51 (Activation)       (None, 2, 2, 512)     0           bn5c_branch2b[0][0]              ____________________________________________________________________________________________________res5c_branch2c (Conv2D)          (None, 2, 2, 2048)    1050624     activation_51[0][0]              ____________________________________________________________________________________________________bn5c_branch2c (BatchNormalizatio (None, 2, 2, 2048)    8192        res5c_branch2c[0][0]             ____________________________________________________________________________________________________add_17 (Add)                     (None, 2, 2, 2048)    0           bn5c_branch2c[0][0]                                                                                 activation_49[0][0]              ____________________________________________________________________________________________________activation_52 (Activation)       (None, 2, 2, 2048)    0           add_17[0][0]                     ____________________________________________________________________________________________________avg_pool (AveragePooling2D)      (None, 1, 1, 2048)    0           activation_52[0][0]              ____________________________________________________________________________________________________flatten_1 (Flatten)              (None, 2048)          0           avg_pool[0][0]                   ____________________________________________________________________________________________________fc6 (Dense)                      (None, 6)             12294       flatten_1[0][0]                  ====================================================================================================Total params: 23,600,006Trainable params: 23,546,886Non-trainable params: 53,120____________________________________________________________________________________________________</code></pre><p>Finally, run the code below to visualize your ResNet50. You can also download a .png picture of your model by going to “File -&gt; Open…-&gt; model.png”.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(model, to_file=<span class="string">&#x27;model.png&#x27;</span>)</span><br><span class="line">SVG(model_to_dot(model).create(prog=<span class="string">&#x27;dot&#x27;</span>, <span class="built_in">format</span>=<span class="string">&#x27;svg&#x27;</span>))</span><br></pre></td></tr></table></figure><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-output_39_0.svg" alt=""></p><font color='blue'>**What you should remember:**- Very deep "plain" networks don't work in practice because they are hard to train due to vanishing gradients.  - The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function. - There are two main type of blocks: The identity block and the convolutional block. - Very deep Residual Networks are built by stacking these blocks together.</font><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>This notebook presents the ResNet algorithm due to He et al. (2015). The implementation here also took significant inspiration and follows the structure given in the github repository of Francois Chollet: </p><ul><li>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition (2015)</a></li><li>Francois Chollet’s github repository: <a href="https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py">https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Keras 实现 Happy House</title>
      <link href="2019/01/19/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/2%20Keras-Happy%20House/"/>
      <url>2019/01/19/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/2%20Keras-Happy%20House/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是卷积神经网络课程第二周的编程作业 Part1，用 Keras 解决 Happy House 问题。</p></blockquote><a id="more"></a><h1 id="Keras-tutorial-the-Happy-House"><a href="#Keras-tutorial-the-Happy-House" class="headerlink" title="Keras tutorial - the Happy House"></a>Keras tutorial - the Happy House</h1><p>Welcome to the first assignment of week 2. In this assignment, you will:</p><ol><li>Learn to use Keras, a high-level neural networks API (programming framework), written in Python and capable of running on top of several lower-level frameworks including TensorFlow and CNTK. </li><li>See how you can in a couple of hours build a deep learning algorithm.</li></ol><p>Why are we using Keras? Keras was developed to enable deep learning engineers to build and experiment with different models very quickly. Just as TensorFlow is a higher-level framework than Python, Keras is an even higher-level framework and provides additional abstractions. Being able to go from idea to result with the least possible delay is key to finding good models. However, Keras is more restrictive than the lower-level frameworks, so there are some very complex models that you can implement in TensorFlow but not (without more difficulty) in Keras. That being said, Keras will work fine for many common models. </p><p>In this exercise, you’ll work on the “Happy House” problem, which we’ll explain below. Let’s load the required packages and solve the problem of the Happy House!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">&#x27;channels_last&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><p><strong>Note</strong>: As you can see, we’ve imported a lot of functions from Keras. You can use them easily just by calling them directly in the notebook. Ex: <code>X = Input(...)</code> or <code>X = ZeroPadding2D(...)</code>.</p><h2 id="1-The-Happy-House"><a href="#1-The-Happy-House" class="headerlink" title="1 - The Happy House"></a>1 - The Happy House</h2><p>For your next vacation, you decided to spend a week with five of your friends from school. It is a very convenient house with many things to do nearby. But the most important benefit is that everybody has commited to be happy when they are in the house. So anyone wanting to enter the house must prove their current state of happiness.</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-103154.jpg"></p><caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **the Happy House**</font></font></center></caption><p>As a deep learning expert, to make sure the “Happy” rule is strictly applied, you are going to build an algorithm which that uses pictures from the front door camera to check if the person is happy or not. The door should open only if the person is happy. </p><p>You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled. </p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-103318.jpg" alt=""></p><p>Run the following code to normalize the dataset and learn about its shapes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">Y_train = Y_train_orig.T</span><br><span class="line">Y_test = Y_test_orig.T</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;number of training examples = &quot;</span> + <span class="built_in">str</span>(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;number of test examples = &quot;</span> + <span class="built_in">str</span>(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X_train shape: &quot;</span> + <span class="built_in">str</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y_train shape: &quot;</span> + <span class="built_in">str</span>(Y_train.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X_test shape: &quot;</span> + <span class="built_in">str</span>(X_test.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y_test shape: &quot;</span> + <span class="built_in">str</span>(Y_test.shape))</span><br></pre></td></tr></table></figure><pre><code>number of training examples = 600number of test examples = 150X_train shape: (600, 64, 64, 3)Y_train shape: (600, 1)X_test shape: (150, 64, 64, 3)Y_test shape: (150, 1)</code></pre><p><strong>Details of the “Happy” dataset</strong>:</p><ul><li>Images are of shape (64,64,3)</li><li>Training: 600 pictures</li><li>Test: 150 pictures</li></ul><p>It is now time to solve the “Happy” Challenge.</p><h2 id="2-Building-a-model-in-Keras"><a href="#2-Building-a-model-in-Keras" class="headerlink" title="2 - Building a model in Keras"></a>2 - Building a model in Keras</h2><p>Keras is very good for rapid prototyping. In just a short time you will be able to build a model that achieves outstanding results.</p><p>Here is an example of a model in Keras:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">input_shape</span>):</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = <span class="string">&#x27;conv0&#x27;</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">&#x27;bn0&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">&#x27;max_pool&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name=<span class="string">&#x27;fc&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you&#x27;ll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">&#x27;HappyModel&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>Note that Keras uses a different convention with variable names than we’ve previously used with numpy and TensorFlow. In particular, rather than creating and assigning a new variable on each step of forward propagation such as <code>X</code>, <code>Z1</code>, <code>A1</code>, <code>Z2</code>, <code>A2</code>, etc. for the computations for the different layers, in Keras code each line above just reassigns <code>X</code> to a new value using <code>X = ...</code>. In other words, during each step of forward propagation, we are just writing the latest value in the commputation into the same variable <code>X</code>. The only exception was <code>X_input</code>, which we kept separate and did not overwrite, since we needed it at the end to create the Keras model instance (<code>model = Model(inputs = X_input, ...)</code> above). </p><p><strong>Exercise</strong>: Implement a <code>HappyModel()</code>. This assignment is more open-ended than most. We suggest that you start by implementing a model using the architecture we suggest, and run through the rest of this assignment using that as your initial model. But after that, come back and take initiative to try out other model architectures. For example, you might take inspiration from the model above, but then vary the network architecture and hyperparameters however you wish. You can also use other functions such as <code>AveragePooling2D()</code>, <code>GlobalMaxPooling2D()</code>, <code>Dropout()</code>. </p><p><strong>Note</strong>: You have to be careful with your data’s shapes. Use what you’ve learned in the videos to make sure your convolutional, pooling and fully-connected layers are adapted to the volumes you’re applying it to.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: HappyModel</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span>(<span class="params">input_shape</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Feel free to use the suggested outline in the text above to get started, and run through the whole</span></span><br><span class="line">    <span class="comment"># exercise (including the later portions of this notebook) once. The come back also try out other</span></span><br><span class="line">    <span class="comment"># network architectures as well. </span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>, (<span class="number">7</span>, <span class="number">7</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = <span class="string">&#x27;conv0&#x27;</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">&#x27;bn0&#x27;</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>), name=<span class="string">&#x27;max_pool&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>, name=<span class="string">&#x27;fc&#x27;</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you&#x27;ll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs = X_input, outputs = X, name=<span class="string">&#x27;HappyModel&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>You have now built a function to describe your model. To train and test this model, there are four steps in Keras:</p><ol><li>Create the model by calling the function above</li><li>Compile the model by calling <code>model.compile(optimizer = &quot;...&quot;, loss = &quot;...&quot;, metrics = [&quot;accuracy&quot;])</code></li><li>Train the model on train data by calling <code>model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)</code></li><li>Test the model on test data by calling <code>model.evaluate(x = ..., y = ...)</code></li></ol><p>If you want to know more about <code>model.compile()</code>, <code>model.fit()</code>, <code>model.evaluate()</code> and their arguments, refer to the official <a href="https://keras.io/models/model/">Keras documentation</a>.</p><p><strong>Exercise</strong>: Implement step 1, i.e. create the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">happyModel = HappyModel(X_train.shape[<span class="number">1</span>:])</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p><strong>Exercise</strong>: Implement step 2, i.e. compile the model to configure the learning process. Choose the 3 arguments of <code>compile()</code> wisely. Hint: the Happy Challenge is a binary classification problem.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">happyModel.<span class="built_in">compile</span>(optimizer = <span class="string">&quot;adam&quot;</span>, loss = <span class="string">&quot;binary_crossentropy&quot;</span>, metrics = [<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p><strong>Exercise</strong>: Implement step 3, i.e. train the model. Choose the number of epochs and the batch size.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">happyModel.fit(x = X_train, y = Y_train, epochs = <span class="number">40</span>, batch_size = <span class="number">12</span>)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><pre><code>Epoch 1/40600/600 [==============================] - 13s - loss: 1.2111 - acc: 0.7100    Epoch 2/40600/600 [==============================] - 13s - loss: 0.2560 - acc: 0.9133    Epoch 3/40600/600 [==============================] - 12s - loss: 0.1751 - acc: 0.9250    Epoch 4/40600/600 [==============================] - 12s - loss: 0.1252 - acc: 0.9533    Epoch 5/40600/600 [==============================] - 12s - loss: 0.0996 - acc: 0.9650    Epoch 6/40600/600 [==============================] - 12s - loss: 0.1118 - acc: 0.9633    Epoch 7/40600/600 [==============================] - 12s - loss: 0.0950 - acc: 0.9667    Epoch 8/40600/600 [==============================] - 13s - loss: 0.1658 - acc: 0.9417    Epoch 9/40600/600 [==============================] - 14s - loss: 0.2170 - acc: 0.9317    Epoch 10/40600/600 [==============================] - 14s - loss: 0.0665 - acc: 0.9817    Epoch 11/40600/600 [==============================] - 13s - loss: 0.0562 - acc: 0.9833    Epoch 12/40600/600 [==============================] - 12s - loss: 0.2736 - acc: 0.9117    Epoch 13/40600/600 [==============================] - 13s - loss: 0.1018 - acc: 0.9683    Epoch 14/40600/600 [==============================] - 12s - loss: 0.1452 - acc: 0.9550    Epoch 15/40600/600 [==============================] - 12s - loss: 0.0710 - acc: 0.9767    Epoch 16/40600/600 [==============================] - 12s - loss: 0.0879 - acc: 0.9767    Epoch 17/40600/600 [==============================] - 12s - loss: 0.1838 - acc: 0.9467    Epoch 18/40600/600 [==============================] - 12s - loss: 0.0782 - acc: 0.9683    Epoch 19/40600/600 [==============================] - 12s - loss: 0.0608 - acc: 0.9800    Epoch 20/40600/600 [==============================] - 12s - loss: 0.0380 - acc: 0.9800    Epoch 21/40600/600 [==============================] - 12s - loss: 0.0810 - acc: 0.9733    Epoch 22/40600/600 [==============================] - 12s - loss: 0.0534 - acc: 0.9850    Epoch 23/40600/600 [==============================] - 12s - loss: 0.2090 - acc: 0.9550    Epoch 24/40600/600 [==============================] - 12s - loss: 0.1350 - acc: 0.9733    Epoch 25/40600/600 [==============================] - 12s - loss: 0.0848 - acc: 0.9717    Epoch 26/40600/600 [==============================] - 12s - loss: 0.0567 - acc: 0.9767    Epoch 27/40600/600 [==============================] - 12s - loss: 0.1254 - acc: 0.9650    Epoch 28/40600/600 [==============================] - 13s - loss: 0.1934 - acc: 0.9467    Epoch 29/40600/600 [==============================] - 12s - loss: 0.1330 - acc: 0.9583    Epoch 30/40600/600 [==============================] - 12s - loss: 0.0894 - acc: 0.9817    Epoch 31/40600/600 [==============================] - 12s - loss: 0.0653 - acc: 0.9833    Epoch 32/40600/600 [==============================] - 12s - loss: 0.1109 - acc: 0.9717    Epoch 33/40600/600 [==============================] - 13s - loss: 0.1199 - acc: 0.9717    Epoch 34/40600/600 [==============================] - 13s - loss: 0.0435 - acc: 0.9883    Epoch 35/40600/600 [==============================] - 12s - loss: 0.0651 - acc: 0.9800    Epoch 36/40600/600 [==============================] - 12s - loss: 0.0425 - acc: 0.9867    Epoch 37/40600/600 [==============================] - 12s - loss: 0.0853 - acc: 0.9883    Epoch 38/40600/600 [==============================] - 12s - loss: 0.0405 - acc: 0.9950    Epoch 39/40600/600 [==============================] - 12s - loss: 0.0551 - acc: 0.9817    Epoch 40/40600/600 [==============================] - 12s - loss: 0.0470 - acc: 0.9917    &lt;keras.callbacks.History at 0x7f6bad97bd68&gt;</code></pre><p>Note that if you run <code>fit()</code> again, the <code>model</code> will continue to train with the parameters it has already learnt instead of reinitializing them.</p><p><strong>Exercise</strong>: Implement step 4, i.e. test/evaluate the model.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">preds = happyModel.evaluate(X_test, Y_test)</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">print()</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Loss = &quot;</span> + <span class="built_in">str</span>(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Test Accuracy = &quot;</span> + <span class="built_in">str</span>(preds[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>150/150 [==============================] - 1s     Loss = 0.136449843446Test Accuracy = 0.960000003974</code></pre><p>If your <code>happyModel()</code> function worked, you should have observed much better than random-guessing (50%) accuracy on the train and test sets.</p><p>To give you a point of comparison, our model gets around <strong>95% test accuracy in 40 epochs</strong> (and 99% train accuracy) with a mini batch size of 16 and “adam” optimizer. But our model gets decent accuracy after just 2-5 epochs, so if you’re comparing different models you can also train a variety of models on just a few epochs and see how they compare. </p><p>If you have not yet achieved a very good accuracy (let’s say more than 80%), here’re some things you can play around with to try to achieve it:</p><ul><li>Try using blocks of CONV-&gt;BATCHNORM-&gt;RELU such as:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), strides = (<span class="number">1</span>, <span class="number">1</span>), name = <span class="string">&#x27;conv0&#x27;</span>)(X)</span><br><span class="line">X = BatchNormalization(axis = <span class="number">3</span>, name = <span class="string">&#x27;bn0&#x27;</span>)(X)</span><br><span class="line">X = Activation(<span class="string">&#x27;relu&#x27;</span>)(X)</span><br></pre></td></tr></table></figure>until your height and width dimensions are quite low and your number of channels quite large (≈32 for example). You are encoding useful information in a volume with a lot of channels. You can then flatten the volume and use a fully-connected layer.</li><li>You can use MAXPOOL after such blocks. It will help you lower the dimension in height and width.</li><li>Change your optimizer. We find Adam works well. </li><li>If the model is struggling to run and you get memory issues, lower your batch_size (12 is usually a good compromise)</li><li>Run on more epochs, until you see the train accuracy plateauing. </li></ul><p>Even if you have achieved a good accuracy, please feel free to keep playing with your model to try to get even better results. </p><p><strong>Note</strong>: If you perform hyperparameter tuning on your model, the test set actually becomes a dev set, and your model might end up overfitting to the test (dev) set. But just for the purpose of this assignment, we won’t worry about that here.</p><h2 id="3-Conclusion"><a href="#3-Conclusion" class="headerlink" title="3 - Conclusion"></a>3 - Conclusion</h2><p>Congratulations, you have solved the Happy House challenge! </p><p>Now, you just need to link this model to the front-door camera of your house. We unfortunately won’t go into the details of how to do that here. </p><p><font color='blue'><br><strong>What we would like you to remember from this assignment:</strong></p><ul><li>Keras is a tool we recommend for rapid prototyping. It allows you to quickly try out different model architectures. Are there any applications of deep learning to your daily life that you’d like to implement using Keras? </li><li>Remember how to code a model in Keras and the four steps leading to the evaluation of your model on the test set. Create-&gt;Compile-&gt;Fit/Train-&gt;Evaluate/Test.</li></ul><h2 id="4-Test-with-your-own-image-Optional"><a href="#4-Test-with-your-own-image-Optional" class="headerlink" title="4 - Test with your own image (Optional)"></a>4 - Test with your own image (Optional)</h2><p>Congratulations on finishing this assignment. You can now take a picture of your face and see if you could enter the Happy House. To do that:</p><pre><code>1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub.2. Add your image to this Jupyter Notebook&#39;s directory, in the &quot;images&quot; folder3. Write your image&#39;s name in the following code4. Run the code and check if the algorithm is right (0 is unhappy, 1 is happy)!</code></pre><p>The training/test sets were quite similar; for example, all the pictures were taken against the same background (since a front door camera is always mounted in the same position). This makes the problem easier, but a model trained on this data may or may not work on your own data. But feel free to give it a try! </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">img_path = <span class="string">&#x27;images/my_image.jpg&#x27;</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">imshow(img)</span><br><span class="line"></span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">print(happyModel.predict(x))</span><br></pre></td></tr></table></figure><pre><code>[[  1.00774433e-09]]</code></pre><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-104740.jpg" alt=""></p><h2 id="5-Other-useful-functions-in-Keras-Optional"><a href="#5-Other-useful-functions-in-Keras-Optional" class="headerlink" title="5 - Other useful functions in Keras (Optional)"></a>5 - Other useful functions in Keras (Optional)</h2><p>Two other basic features of Keras that you’ll find useful are:</p><ul><li><code>model.summary()</code>: prints the details of your layers in a table with the sizes of its inputs/outputs</li><li><code>plot_model()</code>: plots your graph in a nice layout. You can even save it as “.png” using SVG() if you’d like to share it on social media ;). It is saved in “File” then “Open…” in the upper bar of the notebook.</li></ul><p>Run the following code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">happyModel.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================input_7 (InputLayer)         (None, 64, 64, 3)         0         _________________________________________________________________zero_padding2d_2 (ZeroPaddin (None, 70, 70, 3)         0         _________________________________________________________________conv0 (Conv2D)               (None, 64, 64, 32)        4736      _________________________________________________________________bn0 (BatchNormalization)     (None, 64, 64, 32)        128       _________________________________________________________________activation_9 (Activation)    (None, 64, 64, 32)        0         _________________________________________________________________max_pool (MaxPooling2D)      (None, 32, 32, 32)        0         _________________________________________________________________flatten_5 (Flatten)          (None, 32768)             0         _________________________________________________________________fc (Dense)                   (None, 1)                 32769     =================================================================Total params: 37,633Trainable params: 37,569Non-trainable params: 64_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_model(happyModel, to_file=<span class="string">&#x27;HappyModel.png&#x27;</span>)</span><br><span class="line">SVG(model_to_dot(happyModel).create(prog=<span class="string">&#x27;dot&#x27;</span>, <span class="built_in">format</span>=<span class="string">&#x27;svg&#x27;</span>))</span><br></pre></td></tr></table></figure><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-output_23_0.svg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一些经典 CNN</title>
      <link href="2019/01/17/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/1%20%E4%B8%80%E4%BA%9B%E7%BB%8F%E5%85%B8%20CNN/"/>
      <url>2019/01/17/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%202/1%20%E4%B8%80%E4%BA%9B%E7%BB%8F%E5%85%B8%20CNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在计算机视觉的发展过程中，出现了很多经典的卷积神经网络模型，它们对后来的研究有很大影响，这篇文章简要谈谈 LeNet-5、AlexNet 和 VGGNet.</p></blockquote><a id="more"></a><h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p>LeNet-5 是由 LeCun 在 1998 年的一篇论文中提出的，与今天的一些神经网络相比，LeNet-5 是一个小型神经网络，它只有大约 60000 个参数，而今天经常会有包含千万到亿量级参数的神经网络，我们先来看一下 LeNet-5 的模型：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-01-061526.png" alt=""></p><p>如图所示，输入是一张维度为 (32, 32, 1) 的图片，在这个例子中该图片是一张手写体数字的灰度图片，内容为数字 7，由于这是一张灰度图，因此通道数为 1，下面依次对该网络每层进行说明：</p><ul><li>第一层：<ul><li>6 个大小为 5 × 5 的过滤器，Padding = 1，Stride = 1. 输出维度为 (28, 28, 6) 的图片。</li><li>均值池化层，过滤器大小为 2 × 2，Stride = 2. 输出维度为 (14, 14, 6) 的图片。</li></ul></li><li>第二层：<ul><li>6 个大小为 5 × 5 的过滤器，Padding = 1，Stride = 1. 输出维度为 (10, 10, 6) 的图片。</li><li>均值池化层，过滤器大小为 2 × 2，Stride = 2. 输出维度为 (5, 5, 6) 的图片。</li></ul></li><li>第三层：全连接层，共 120 个结点</li><li>第四层：全连接层，共 84 个结点</li><li>第五层：输出层，输出预测值</li></ul><p><strong>Note:</strong> 由于池化层并不包含任何参数，因此将其与前一个卷积层共算作一层。</p><p>这个网络可以完成手写体数字识别的任务，输入一张大小为 32 × 32 的灰度图，最终输出对图中数字的预测，其中池化层用的是均值池化，而目前更常用的最大池化，另外，现在输出层通常使用 softmax 函数，而当时使用的是另一种函数。</p><p>该网络中用到结构在现今仍非常常用，即先使用一层或几层卷积层，再使用一层池化层，然后再使用一层或几层卷积层，再使用一层池化层，最后再使用几层全连接层，最后输出结果。这种结构方式在今天也非常常见。</p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>与 LeNet-5 只有大约 60000 个参数不同，AlexNet 有大约 6 千万个参数，不过这二者拥有很相似的结构，只是 AlexNet 拥有更多的隐藏神经元，在更大的数据上训练，这使得它有更好的性能，它的结构如下</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-01-064438.png" alt=""></p><ul><li>第一层：<ul><li>96 个大小为 11 × 11 的过滤器，Padding = 1，Stride = 4. 输出维度为 (55, 55, 96).</li><li>最大池化层，过滤器大小为 3 × 3，Stride = 2. 输出维度为 (27, 27, 96).</li></ul></li><li>第二层：<ul><li>256 个大小为 5 × 5 的过滤器，Padding = same，Stride = 1. 输出维度为 (27, 27, 256).</li><li>最大池化层，过滤器大小为 3 × 3，Stride = 2. 输出维度为 (13, 13, 256).</li></ul></li><li>第三层：384 个大小为 3 × 3 的过滤器，Padding = same，Stride = 1. 输出维度为 (13, 13, 384).</li><li>第四层：384 个大小为 3 × 3 的过滤器，Padding = same，Stride = 1. 输出维度为 (13, 13, 384).</li><li>第五层：<ul><li>256 个大小为 3 × 3 的过滤器，Padding = same，Stride = 1. 输出维度为 (13, 13, 256).</li><li>最大池化层：过滤器大小为 3 × 3，Stride = 2. 输出维度为 (6, 6, 256).</li></ul></li><li>第六层：全连接层，共 4096 个结点</li><li>第七层：全连接层，共 4096 个结点</li><li>第八层：输出层，使用 softmax 输出预测类别</li></ul><p>另一个与 LeNet-5 不同之处是，AlexNet 的激活函数使用了 ReLU，而非 LeNet-5 使用的 sigmoid 或者 tanh，这也使得其性能更好一些。</p><p>实际上上面列出的结构是原始 AlexNet 的简化，原始的 AlexNet 中还有局部响应归一层，但后来并不太常用，因此这里做了简化。</p><h3 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h3><p>VGG-16 中的 16 是指该网络中有 16 层带有权重的层，这是一个很大的神经网络，这导致该网络中的参数多达 1 亿 3 千 8 百万个。但是 VGG 的结构非常统一，该网络中的所有卷积层的过滤器大小都是 3 × 3，Stride 都是 1，Padding 都是 same，而池化层过滤器的大小全为 2 × 2，Stride 全为 2。结构方面，总是现有几层卷积层，然后是池化层，再来几层卷积层，再来一个池化层。由于该网络层数很多，结构图稍微简化了一些：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-01-073535.png" alt=""></p><p>其中，</p><ul><li>[CONV 64] × 2 是指两层各有 64 个过滤器的卷积层</li><li>POOL 指池化层</li><li>FC 指全连接层</li></ul><p>可以看到，虽然 VGG-16 是一个比较大的网络，但是由于其结构统一，因此受到了许多研究者的青睐，另外还有 VGG-19，指拥有 19 层带有权重的层的 VGG，但由于其性能与 VGG-16 差不多，因此大多数人还是选择用 VGG-16。</p><p>以上是对三个经典卷积神经网络的简单介绍，文末附上 AlexNet 和 VGG 的相关论文。</p><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">[Krizhevsky et al., 2012. ImageNet classification with deep convolutional neural networks]</a></p><p><a href="https://arxiv.org/pdf/1409.1556.pdf">[Simonyan &amp; Zisserman 2015. Very deep convolutional networks for large-scale image recognition]</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transfer Learning 识别狗猫品种</title>
      <link href="2019/01/16/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/Transfer%20Learning%20%E8%AF%86%E5%88%AB%E5%8A%A8%E7%89%A9%E5%93%81%E7%A7%8D/"/>
      <url>2019/01/16/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/Transfer%20Learning%20%E8%AF%86%E5%88%AB%E5%8A%A8%E7%89%A9%E5%93%81%E7%A7%8D/</url>
      
        <content type="html"><![CDATA[<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-03-%E4%B8%8B%E8%BD%BD.png" alt=""></p><a id="more"></a><h3 id="导入需要的包"><a href="#导入需要的包" class="headerlink" title="导入需要的包"></a>导入需要的包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure><h3 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h3><p>数据集含 7393 张图片，37 个不同猫狗品种</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bs = <span class="number">64</span> <span class="comment"># batch-size</span></span><br><span class="line"><span class="comment"># 下载数据集</span></span><br><span class="line">path = untar_data(URLs.PETS)</span><br><span class="line">path_anno = path/<span class="string">&#x27;annotations&#x27;</span></span><br><span class="line">path_img = path/<span class="string">&#x27;images&#x27;</span></span><br><span class="line"></span><br><span class="line">fnames = get_image_files(path_img)</span><br><span class="line"><span class="comment"># 利用正则表达式获得类别，组装成训练集和验证集</span></span><br><span class="line">pat = re.<span class="built_in">compile</span>(<span class="string">r&#x27;/([^/]+)_\d+.jpg$&#x27;</span>)</span><br><span class="line">data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transfrom(), size=<span class="number">224</span>, bs=bs, num_workers=<span class="number">0</span>).normalize(imagenet_stats)</span><br><span class="line"><span class="comment"># 查看</span></span><br><span class="line">data.show_batch(rows=<span class="number">3</span>, figsize=(<span class="number">7</span>,<span class="number">6</span>))</span><br></pre></td></tr></table></figure><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-03-%E4%B8%8B%E8%BD%BD.png" alt=""></p><h3 id="使用-ResNet34-迁移学习"><a href="#使用-ResNet34-迁移学习" class="headerlink" title="使用 ResNet34 迁移学习"></a>使用 ResNet34 迁移学习</h3><p>使用本地 CPU 迭代 4 次需大约 2 hours，使用 GCP 仅需 2 mins</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 ResNet34</span></span><br><span class="line">learn = create_cnn(data, models.resnet34, metrics=error_rate)</span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">learn.fit_one_cycle(<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>Total time: 02:10</p><div class="table-container"><table><thead><tr><th>epoch</th><th>train_loss</th><th>valid_loss</th><th>error_rate</th></tr></thead><tbody><tr><td>1</td><td>1.363533</td><td>0.374314</td><td>0.110961</td></tr><tr><td>2</td><td>0.551215</td><td>0.287073</td><td>0.094723</td></tr><tr><td>3</td><td>0.346619</td><td>0.271395</td><td>0.081867</td></tr><tr><td>4</td><td>0.249408</td><td>0.254900</td><td>0.079838</td></tr></tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型</span></span><br><span class="line">learn.save(<span class="string">&#x27;stage-1&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h3><p><strong>losses</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassficationInterpretation.from_learner(learn)</span><br><span class="line"><span class="comment"># losses 从大到小存至 list</span></span><br><span class="line">losses, idxs = interp.top_losses()</span><br><span class="line"><span class="comment"># losses 最大的 9 张图</span></span><br><span class="line">interp.plot_top_losses(<span class="number">9</span>, fjgsize=(<span class="number">15</span>, <span class="number">11</span>))</span><br></pre></td></tr></table></figure><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-03-081750.jpg" alt=""></p><p><strong>confusion matrix</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制 confusion matrix</span></span><br><span class="line">interp.plot_confusion_matrix(figsize=(<span class="number">12</span>, <span class="number">12</span>), dpi=<span class="number">60</span>)</span><br></pre></td></tr></table></figure><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-03-082753.jpg" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 混淆次数最多的几个品种</span></span><br><span class="line">interp.most_confused(min_val=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><blockquote><p>[(‘american_pit_bull_terrier’, ‘staffordshire_bull_terrier’, 10),<br> (‘Russian_Blue’, ‘British_Shorthair’, 5),<br> (‘american_pit_bull_terrier’, ‘american_bulldog’, 5),<br> (‘american_pit_bull_terrier’, ‘miniature_pinscher’, 4),<br> (‘Egyptian_Mau’, ‘Bengal’, 3),<br> (‘american_bulldog’, ‘staffordshire_bull_terrier’, 3),<br> (‘english_cocker_spaniel’, ‘english_setter’, 3)]</p></blockquote><p>看来斯塔福郡斗牛梗和美国比特斗牛犬最难分辨，是有点像</p><div class="table-container"><table><thead><tr><th>美国比特斗牛犬</th><th>斯塔福郡斗牛梗</th></tr></thead><tbody><tr><td><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-03-083904.jpg" alt=""></td><td><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-02-03-084247.jpg" alt=""></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow 实现手势识别</title>
      <link href="2019/01/16/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/3%20TensorFlow%20%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/"/>
      <url>2019/01/16/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/3%20TensorFlow%20%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是 Coursera 的卷积神经网络课程的第一周编程作业 Part2</p></blockquote><a id="more"></a><h1 id="Convolutional-Neural-Networks-Application"><a href="#Convolutional-Neural-Networks-Application" class="headerlink" title="Convolutional Neural Networks: Application"></a>Convolutional Neural Networks: Application</h1><p>Welcome to Course 4’s second assignment! In this notebook, you will:</p><ul><li>Implement helper functions that you will use when implementing a TensorFlow model</li><li>Implement a fully functioning ConvNet using TensorFlow </li></ul><p><strong>After this assignment you will be able to:</strong></p><ul><li>Build and train a ConvNet in TensorFlow for a classification problem </li></ul><p>We assume here that you are already familiar with TensorFlow. If you are not, please refer the <em>TensorFlow Tutorial</em> of the third week of Course 2 (“<em>Improving deep neural networks</em>“).</p><h2 id="1-0-TensorFlow-model"><a href="#1-0-TensorFlow-model" class="headerlink" title="1.0 - TensorFlow model"></a>1.0 - TensorFlow model</h2><p>In the previous assignment, you built helper functions using numpy to understand the mechanics behind convolutional neural networks. Most practical applications of deep learning today are built using programming frameworks, which have many built-in functions you can simply call. </p><p>As usual, we will start by loading in the packages. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> cnn_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>Run the next cell to load the “SIGNS” dataset you are going to use.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (signs)</span></span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure><p>As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhu55wd4jj311s0k6t9t.jpg" alt=""></p><p>The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of <code>index</code> below and re-run to see different examples. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">99</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;y = &quot;</span> + <span class="built_in">str</span>(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure><pre><code>y = 5</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhu77plnoj3073070q2u.jpg" alt=""></p><p>In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.</p><p>To get started, let’s examine the shapes of your data. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;number of training examples = &quot;</span> + <span class="built_in">str</span>(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;number of test examples = &quot;</span> + <span class="built_in">str</span>(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X_train shape: &quot;</span> + <span class="built_in">str</span>(X_train.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y_train shape: &quot;</span> + <span class="built_in">str</span>(Y_train.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X_test shape: &quot;</span> + <span class="built_in">str</span>(X_test.shape))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y_test shape: &quot;</span> + <span class="built_in">str</span>(Y_test.shape))</span><br><span class="line">conv_layers = &#123;&#125;</span><br></pre></td></tr></table></figure><pre><code>number of training examples = 1080number of test examples = 120X_train shape: (1080, 64, 64, 3)Y_train shape: (1080, 6)X_test shape: (120, 64, 64, 3)Y_test shape: (120, 6)</code></pre><h3 id="1-1-Create-placeholders"><a href="#1-1-Create-placeholders" class="headerlink" title="1.1 - Create placeholders"></a>1.1 - Create placeholders</h3><p>TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.</p><p><strong>Exercise</strong>: Implement the function below to create placeholders for the input image X and the output Y. You should not define the number of training examples for the moment. To do so, you could use “None” as the batch size, it will give you the flexibility to choose it later. Hence X should be of dimension <strong>[None, n_H0, n_W0, n_C0]</strong> and Y should be of dimension <strong>[None, n_y]</strong>.  <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder">Hint</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: create_placeholders</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span>(<span class="params">n_H0, n_W0, n_C0, n_y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype &quot;float&quot;</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype &quot;float&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    X = tf.placeholder(tf.float32, (<span class="literal">None</span>, n_H0, n_W0, n_C0), name = <span class="string">&#x27;X&#x27;</span>)</span><br><span class="line">    Y = tf.placeholder(tf.float32, (<span class="literal">None</span>, n_y), name = <span class="string">&#x27;Y&#x27;</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X, Y = create_placeholders(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;X = &quot;</span> + <span class="built_in">str</span>(X))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;Y = &quot;</span> + <span class="built_in">str</span>(Y))</span><br></pre></td></tr></table></figure><pre><code>X = Tensor(&quot;X:0&quot;, shape=(?, 64, 64, 3), dtype=float32)Y = Tensor(&quot;Y:0&quot;, shape=(?, 6), dtype=float32)</code></pre><p><strong>Expected Output</strong></p><table> <tr><td>    X = Tensor("Placeholder:0", shape=(?, 64, 64, 3), dtype=float32)</td></tr><tr><td>    Y = Tensor("Placeholder_1:0", shape=(?, 6), dtype=float32)</td></tr></table><h3 id="1-2-Initialize-parameters"><a href="#1-2-Initialize-parameters" class="headerlink" title="1.2 - Initialize parameters"></a>1.2 - Initialize parameters</h3><p>You will initialize weights/filters $W1$ and $W2$ using <code>tf.contrib.layers.xavier_initializer(seed = 0)</code>. You don’t need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.</p><p><strong>Exercise:</strong> Implement initialize_parameters(). The dimensions for each group of filters are provided below. Reminder - to initialize a parameter $W$ of shape [1,2,3,4] in Tensorflow, use:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = tf.get_variable(<span class="string">&quot;W&quot;</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], initializer = ...)</span><br></pre></td></tr></table></figure><br><a href="https://www.tensorflow.org/api_docs/python/tf/get_variable">More Info</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: initialize_parameters</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># so that your &quot;random&quot; numbers match ours</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines of code)</span></span><br><span class="line">    W1 = tf.get_variable(<span class="string">&quot;W1&quot;</span>, [<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>))</span><br><span class="line">    W2 = tf.get_variable(<span class="string">&quot;W2&quot;</span>, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">16</span>], initializer = tf.contrib.layers.xavier_initializer(seed = <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">&quot;W1&quot;</span>: W1,</span><br><span class="line">                  <span class="string">&quot;W2&quot;</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess_test:</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess_test.run(init)</span><br><span class="line">    print(<span class="string">&quot;W1 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W1&quot;</span>].<span class="built_in">eval</span>()[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]))</span><br><span class="line">    print(<span class="string">&quot;W2 = &quot;</span> + <span class="built_in">str</span>(parameters[<span class="string">&quot;W2&quot;</span>].<span class="built_in">eval</span>()[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>W1 = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 -0.06847463  0.05245192]W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 -0.22779644 -0.1601823  -0.16117483 -0.10286498]</code></pre><p><strong> Expected Output:</strong></p><table>     <tr>        <td>        W1 =         </td>        <td>[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 <br> -0.06847463  0.05245192]        </td>    </tr>    <tr>        <td>        W2 =         </td>        <td>[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 <br> -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 <br> -0.22779644 -0.1601823  -0.16117483 -0.10286498]        </td>    </tr></table><h3 id="1-2-Forward-propagation"><a href="#1-2-Forward-propagation" class="headerlink" title="1.2 - Forward propagation"></a>1.2 - Forward propagation</h3><p>In TensorFlow, there are built-in functions that carry out the convolution steps for you.</p><ul><li><p><strong>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input $X$ and a group of filters $W1$, this function convolves $W1$’s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d">here</a></p></li><li><p><strong>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool">here</a></p></li><li><p><strong>tf.nn.relu(Z1):</strong> computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu">here.</a></p></li><li><p><strong>tf.contrib.layers.flatten(P)</strong>: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten">here.</a></p></li><li><p><strong>tf.contrib.layers.fully_connected(F, num_outputs):</strong> given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected">here.</a></p></li></ul><p>In the last function above (<code>tf.contrib.layers.fully_connected</code>), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. </p><p><strong>Exercise</strong>: </p><p>Implement the <code>forward_propagation</code> function below to build the following model: <code>CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</code>. You should use the functions above. </p><p>In detail, we will use the following parameters for all the steps:</p><pre><code> - Conv2D: stride 1, padding is &quot;SAME&quot; - ReLU - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is &quot;SAME&quot; - Conv2D: stride 1, padding is &quot;SAME&quot; - ReLU - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is &quot;SAME&quot; - Flatten the previous output. - FULLYCONNECTED (FC) layer: Apply a fully connected layer without an non-linear activation function. Do not call the softmax here. This will result in 6 neurons in the output layer, which then get passed later to a softmax. In TensorFlow, the softmax and cost function are lumped together into a single function, which you&#39;ll call in a different function when computing the cost. </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: forward_propagation</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span>(<span class="params">X, parameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;W2&quot;</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary &quot;parameters&quot; </span></span><br><span class="line">    W1 = parameters[<span class="string">&#x27;W1&#x27;</span>]</span><br><span class="line">    W2 = parameters[<span class="string">&#x27;W2&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># CONV2D: stride of 1, padding &#x27;SAME&#x27;</span></span><br><span class="line">    Z1 = tf.nn.conv2d(X, W1, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 8x8, sride 8, padding &#x27;SAME&#x27;</span></span><br><span class="line">    P1 = tf.nn.max_pool(A1, ksize = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">    <span class="comment"># CONV2D: filters W2, stride 1, padding &#x27;SAME&#x27;</span></span><br><span class="line">    Z2 = tf.nn.conv2d(P1, W2, strides = [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">    <span class="comment"># RELU</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    <span class="comment"># MAXPOOL: window 4x4, stride 4, padding &#x27;SAME&#x27;</span></span><br><span class="line">    P2 = tf.nn.max_pool(A2, ksize = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>], padding = <span class="string">&#x27;SAME&#x27;</span>)</span><br><span class="line">    <span class="comment"># FLATTEN</span></span><br><span class="line">    P2 = tf.contrib.layers.flatten(P2)</span><br><span class="line">    <span class="comment"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span></span><br><span class="line">    <span class="comment"># 6 neurons in output layer. Hint: one of the arguments should be &quot;activation_fn=None&quot; </span></span><br><span class="line">    Z3 = tf.contrib.layers.fully_connected(P2, <span class="number">6</span>, activation_fn = <span class="literal">None</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X, Y = create_placeholders(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    a = sess.run(Z3, &#123;X: np.random.randn(<span class="number">2</span>,<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>), Y: np.random.randn(<span class="number">2</span>,<span class="number">6</span>)&#125;)</span><br><span class="line">    print(<span class="string">&quot;Z3 = &quot;</span> + <span class="built_in">str</span>(a))</span><br></pre></td></tr></table></figure><pre><code>Z3 = [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064] [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]</code></pre><p><strong>Expected Output</strong>:</p><table>     <td>     Z3 =    </td>    <td>    [[-0.44670227 -1.57208765 -1.53049231 -2.31013036 -1.29104376  0.46852064] <br> [-0.17601591 -1.57972014 -1.4737016  -2.61672091 -1.00810647  0.5747785 ]]    </td></table><h3 id="1-3-Compute-cost"><a href="#1-3-Compute-cost" class="headerlink" title="1.3 - Compute cost"></a>1.3 - Compute cost</h3><p>Implement the compute cost function below. You might find these two functions helpful: </p><ul><li><strong>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):</strong> computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits">here.</a></li><li><strong>tf.reduce_mean:</strong> computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/reduce_mean">here.</a></li></ul><p><strong> Exercise</strong>: Compute the cost below using the function above.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: compute_cost </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span>(<span class="params">Z3, Y</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- &quot;true&quot; labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line of code)</span></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    X, Y = create_placeholders(<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">6</span>)</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    a = sess.run(cost, &#123;X: np.random.randn(<span class="number">4</span>,<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>), Y: np.random.randn(<span class="number">4</span>,<span class="number">6</span>)&#125;)</span><br><span class="line">    print(<span class="string">&quot;cost = &quot;</span> + <span class="built_in">str</span>(a))</span><br></pre></td></tr></table></figure><pre><code>cost = 2.91034</code></pre><p><strong>Expected Output</strong>: </p><table>    <td>     cost =    </td>     <td>     2.91034    </td> </table><h2 id="1-4-Model"><a href="#1-4-Model" class="headerlink" title="1.4 Model"></a>1.4 Model</h2><p>Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset. </p><p>You have implemented <code>random_mini_batches()</code> in the Optimization programming assignment of course 2. Remember that this function returns a list of mini-batches. </p><p><strong>Exercise</strong>: Complete the function below. </p><p>The model below should:</p><ul><li>create placeholders</li><li>initialize parameters</li><li>forward propagate</li><li>compute the cost</li><li>create an optimizer</li></ul><p>Finally you will create a session and run a for loop  for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function. <a href="https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer">Hint for initializing the variables</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span>(<span class="params">X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">          num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = <span class="literal">True</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    ops.reset_default_graph()                         <span class="comment"># to be able to rerun the model without overwriting tf variables</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                             <span class="comment"># to keep results consistent (tensorflow seed)</span></span><br><span class="line">    seed = <span class="number">3</span>                                          <span class="comment"># to keep results consistent (numpy seed)</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape             </span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]                            </span><br><span class="line">    costs = []                                        <span class="comment"># To keep track of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create Placeholders of the correct shape</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Forward propagation: Build the forward propagation in the tensorflow graph</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Cost function: Add cost function to tensorflow graph</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize all the variables globally</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">     </span><br><span class="line">    <span class="comment"># Start the session to compute the tensorflow graph</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Run the initialization</span></span><br><span class="line">        sess.run(init)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Do the training loop</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line"></span><br><span class="line">            minibatch_cost = <span class="number">0.</span></span><br><span class="line">            num_minibatches = <span class="built_in">int</span>(m / minibatch_size) <span class="comment"># number of minibatches of size minibatch_size in the train set</span></span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Select a minibatch</span></span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                <span class="comment"># IMPORTANT: The line that runs the graph on a minibatch.</span></span><br><span class="line">                <span class="comment"># Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span></span><br><span class="line">                <span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line">                _ , temp_cost = sess.run([optimizer, cost], feed_dict = &#123;X:minibatch_X, Y:minibatch_Y&#125;)</span><br><span class="line">                <span class="comment">### END CODE HERE ###</span></span><br><span class="line">                </span><br><span class="line">                minibatch_cost += temp_cost / num_minibatches</span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">            <span class="comment"># Print the cost every epoch</span></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span> (<span class="string">&quot;Cost after epoch %i: %f&quot;</span> % (epoch, minibatch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="literal">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">        plt.xlabel(<span class="string">&#x27;iterations (per tens)&#x27;</span>)</span><br><span class="line">        plt.title(<span class="string">&quot;Learning rate =&quot;</span> + <span class="built_in">str</span>(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        predict_op = tf.argmax(Z3, <span class="number">1</span>)</span><br><span class="line">        correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">&quot;float&quot;</span>))</span><br><span class="line">        print(accuracy)</span><br><span class="line">        train_accuracy = accuracy.<span class="built_in">eval</span>(&#123;X: X_train, Y: Y_train&#125;)</span><br><span class="line">        test_accuracy = accuracy.<span class="built_in">eval</span>(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">        print(<span class="string">&quot;Train Accuracy:&quot;</span>, train_accuracy)</span><br><span class="line">        print(<span class="string">&quot;Test Accuracy:&quot;</span>, test_accuracy)</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure><p>Run the following cell to train your model for 100 epochs. Check if your cost after epoch 0 and 5 matches our output. If not, stop the cell and go back to your code!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_, _, parameters = model(X_train, Y_train, X_test, Y_test)</span><br></pre></td></tr></table></figure><pre><code>Cost after epoch 0: 1.917929Cost after epoch 5: 1.506757Cost after epoch 10: 0.955359Cost after epoch 15: 0.845802Cost after epoch 20: 0.701174Cost after epoch 25: 0.571977Cost after epoch 30: 0.518435Cost after epoch 35: 0.495806Cost after epoch 40: 0.429827Cost after epoch 45: 0.407291Cost after epoch 50: 0.366394Cost after epoch 55: 0.376922Cost after epoch 60: 0.299491Cost after epoch 65: 0.338870Cost after epoch 70: 0.316400Cost after epoch 75: 0.310413Cost after epoch 80: 0.249549Cost after epoch 85: 0.243457Cost after epoch 90: 0.200031Cost after epoch 95: 0.175452</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhuwonvrlj30b007qt8n.jpg" alt=""></p><pre><code>Tensor(&quot;Mean_1:0&quot;, shape=(), dtype=float32)Train Accuracy: 0.940741Test Accuracy: 0.783333</code></pre><p><strong>Expected output</strong>: although it may not match perfectly, your expected output should be close to ours and your cost value should decrease.</p><table> <tr>    <td>     **Cost after epoch 0 =**    </td>    <td>       1.917929    </td> </tr><tr>    <td>     **Cost after epoch 5 =**    </td>    <td>       1.506757    </td> </tr><tr>    <td>     **Train Accuracy   =**    </td>    <td>       0.940741    </td> </tr> <tr>    <td>     **Test Accuracy   =**    </td>    <td>       0.783333    </td> </tr> </table><p>Congratulations! You have finised the assignment and built a model that recognizes SIGN language with almost 80% accuracy on the test set. If you wish, feel free to play around with this dataset further. You can actually improve its accuracy by spending more time tuning the hyperparameters, or using regularization (as this model clearly has a high variance). </p><p>Once again, here’s a thumbs up for your work! </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fname = <span class="string">&quot;images/thumbs_up.jpg&quot;</span></span><br><span class="line">image = np.array(ndimage.imread(fname, flatten=<span class="literal">False</span>))</span><br><span class="line">my_image = scipy.misc.imresize(image, size=(<span class="number">64</span>,<span class="number">64</span>))</span><br><span class="line">plt.imshow(my_image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x7fd6345b2fd0&gt;</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhux9z66kj3073070dfr.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 Numpy 建立 CNN</title>
      <link href="2019/01/14/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/2%20%E4%BD%BF%E7%94%A8%20Numpy%20%E5%BB%BA%E7%AB%8B%20CNN/"/>
      <url>2019/01/14/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/2%20%E4%BD%BF%E7%94%A8%20Numpy%20%E5%BB%BA%E7%AB%8B%20CNN/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这是 Coursera 的卷积神经网络课程的第一周编程作业 Part1</p></blockquote><a id="more"></a><h1 id="Convolutional-Neural-Networks-Step-by-Step"><a href="#Convolutional-Neural-Networks-Step-by-Step" class="headerlink" title="Convolutional Neural Networks: Step by Step"></a>Convolutional Neural Networks: Step by Step</h1><p>Welcome to Course 4’s first assignment! In this assignment, you will implement convolutional (CONV) and pooling (POOL) layers in numpy, including both forward propagation and (optionally) backward propagation. </p><p><strong>Notation</strong>:</p><ul><li>Superscript $[l]$ denotes an object of the $l^{th}$ layer. <ul><li>Example: $a^{[4]}$ is the $4^{th}$ layer activation. $W^{[5]}$ and $b^{[5]}$ are the $5^{th}$ layer parameters.</li></ul></li></ul><ul><li>Superscript $(i)$ denotes an object from the $i^{th}$ example. <ul><li>Example: $x^{(i)}$ is the $i^{th}$ training example input.</li></ul></li></ul><ul><li>Lowerscript $i$ denotes the $i^{th}$ entry of a vector.<ul><li>Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the activations in layer $l$, assuming this is a fully connected (FC) layer.</li></ul></li></ul><ul><li>$n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer. If you want to reference a specific layer $l$, you can also write $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. </li><li>$n<em>{H</em>{prev}}$, $n<em>{W</em>{prev}}$ and $n<em>{C</em>{prev}}$ denote respectively the height, width and number of channels of the previous layer. If referencing a specific layer $l$, this could also be denoted $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. </li></ul><p>We assume that you are already familiar with <code>numpy</code> and/or have completed the previous courses of the specialization. Let’s get started!</p><h2 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1 - Packages"></a>1 - Packages</h2><p>Let’s first import all the packages that you will need during this assignment. </p><ul><li><a href="www.numpy.org">numpy</a> is the fundamental package for scientific computing with Python.</li><li><a href="http://matplotlib.org">matplotlib</a> is a library to plot graphs in Python.</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="2-Outline-of-the-Assignment"><a href="#2-Outline-of-the-Assignment" class="headerlink" title="2 - Outline of the Assignment"></a>2 - Outline of the Assignment</h2><p>You will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed:</p><ul><li>Convolution functions, including:<ul><li>Zero Padding</li><li>Convolve window </li><li>Convolution forward</li><li>Convolution backward (optional)</li></ul></li><li>Pooling functions, including:<ul><li>Pooling forward</li><li>Create mask </li><li>Distribute value</li><li>Pooling backward (optional)</li></ul></li></ul><p>This notebook will ask you to implement these functions from scratch in <code>numpy</code>. In the next notebook, you will use the TensorFlow equivalents of these functions to build the following model:</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhu3zqhm3j30xk0bet8v.jpg" alt=""></p><p><strong>Note</strong> that for every forward function, there is its corresponding backward equivalent. Hence, at every step of your forward module you will store some parameters in a cache. These parameters are used to compute gradients during backpropagation. </p><h2 id="3-Convolutional-Neural-Networks"><a href="#3-Convolutional-Neural-Networks" class="headerlink" title="3 - Convolutional Neural Networks"></a>3 - Convolutional Neural Networks</h2><p>Although programming frameworks make convolutions easy to use, they remain one of the hardest concepts to understand in Deep Learning. A convolution layer transforms an input volume into an output volume of different size, as shown below. </p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhueimdv1j30qe0d6dfy.jpg" alt=""></p><p>In this part, you will build every step of the convolution layer. You will first implement two helper functions: one for zero padding and the other for computing the convolution function itself. </p><h3 id="3-1-Zero-Padding"><a href="#3-1-Zero-Padding" class="headerlink" title="3.1 - Zero-Padding"></a>3.1 - Zero-Padding</h3><p>Zero-padding adds zeros around the border of an image:</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhufpxnmhj30vw0qamz5.jpg" alt=""></p><caption>    <center>         <u>             <font color='purple'> **Figure 1: Zero-Padding**<br>Image (3 channels, RGB) with a padding of 2. </font>        </u>    </center></caption><p>The main benefits of padding are the following:</p><ul><li><p>It allows you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the “same” convolution, in which the height/width is exactly preserved after one layer. </p></li><li><p>It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.</p></li></ul><p><strong>Exercise</strong>: Implement the following function, which pads all the images of a batch of examples X with zeros. <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html">Use np.pad</a>. Note if you want to pad the array “a” of shape $(5,5,5,5,5)$ with <code>pad = 1</code> for the 2nd dimension, <code>pad = 3</code> for the 4th dimension and <code>pad = 0</code> for the rest, you would do:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.pad(a, ((<span class="number">0</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">0</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">&#x27;constant&#x27;</span>, constant_values = (..,..))</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: zero_pad</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span>(<span class="params">X, pad</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span></span><br><span class="line"><span class="string">    as illustrated in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span></span><br><span class="line"><span class="string">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line)</span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>, <span class="number">0</span>), (pad, pad), (pad, pad), (<span class="number">0</span>, <span class="number">0</span>)), <span class="string">&#x27;constant&#x27;</span>, constant_values = (<span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">x_pad = zero_pad(x, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x.shape =&quot;</span>, x.shape)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x_pad.shape =&quot;</span>, x_pad.shape)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x[1,1] =&quot;</span>, x[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;x_pad[1,1] =&quot;</span>, x_pad[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">fig, axarr = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">axarr[<span class="number">0</span>].set_title(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">axarr[<span class="number">0</span>].imshow(x[<span class="number">0</span>,:,:,<span class="number">0</span>])</span><br><span class="line">axarr[<span class="number">1</span>].set_title(<span class="string">&#x27;x_pad&#x27;</span>)</span><br><span class="line">axarr[<span class="number">1</span>].imshow(x_pad[<span class="number">0</span>,:,:,<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>x.shape = (4, 3, 3, 2)x_pad.shape = (4, 7, 7, 2)x[1,1] = [[ 0.90085595 -0.68372786] [-0.12289023 -0.93576943] [-0.26788808  0.53035547]]x_pad[1,1] = [[ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.]]&lt;matplotlib.image.AxesImage at 0x7f0faa2777f0&gt;</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhungq9a4j309404umx9.jpg" alt=""></p><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **x.shape**:        </td>        <td>           (4, 3, 3, 2)        </td>    </tr>        <tr>        <td>            **x_pad.shape**:        </td>        <td>           (4, 7, 7, 2)        </td>    </tr>        <tr>        <td>            **x[1,1]**:        </td>        <td>           [[ 0.90085595 -0.68372786] [-0.12289023 -0.93576943] [-0.26788808  0.53035547]]        </td>    </tr>        <tr>        <td>            **x_pad[1,1]**:        </td>        <td>           [[ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.] [ 0.  0.]]        </td>    </tr></table><h3 id="3-2-Single-step-of-convolution"><a href="#3-2-Single-step-of-convolution" class="headerlink" title="3.2 - Single step of convolution"></a>3.2 - Single step of convolution</h3><p>In this part, implement a single step of convolution, in which you apply the filter to a single position of the input. This will be used to build a convolutional unit, which: </p><ul><li>Takes an input volume </li><li>Applies a filter at every position of the input</li><li>Outputs another volume (usually of different size)</li></ul><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-Convolution_schematic.gif" style="width:500px;height:300px;"></p><caption>    <center> <u> <font color='purple'> **Figure 2: Convolution operation**<br> with a filter of 2x2 and a stride of 1 (stride = amount you move the window each time you slide)</font> </u>    </center></caption><p>In a computer vision application, each value in the matrix on the left corresponds to a single pixel value, and we convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up and adding a bias. In this first step of the exercise, you will implement a single step of convolution, corresponding to applying a filter to just one of the positions to get a single real-valued output. </p><p>Later in this notebook, you’ll apply this function to multiple positions of the input to implement the full convolutional operation. </p><p><strong>Exercise</strong>: Implement conv_single_step(). <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html">Hint</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_single_step</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span>(<span class="params">a_slice_prev, W, b</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span></span><br><span class="line"><span class="string">    of the previous layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Element-wise product between a_slice and W. Do not add the bias yet.</span></span><br><span class="line">    s = np.multiply(a_slice_prev, W)</span><br><span class="line">    <span class="comment"># Sum over all entries of the volume s.</span></span><br><span class="line">    Z = np.<span class="built_in">sum</span>(s)</span><br><span class="line">    <span class="comment"># Add bias b to Z. Cast b to a float() so that Z results in a scalar value.</span></span><br><span class="line">    Z = Z + np.<span class="built_in">float</span>(b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">a_slice_prev = np.random.randn(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">W = np.random.randn(<span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">Z = conv_single_step(a_slice_prev, W, b)</span><br><span class="line">print(<span class="string">&quot;Z =&quot;</span>, Z)</span><br></pre></td></tr></table></figure><pre><code>Z = -6.99908945068</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **Z**        </td>        <td>            -6.99908945068        </td>    </tr></table><h3 id="3-3-Convolutional-Neural-Networks-Forward-pass"><a href="#3-3-Convolutional-Neural-Networks-Forward-pass" class="headerlink" title="3.3 - Convolutional Neural Networks - Forward pass"></a>3.3 - Convolutional Neural Networks - Forward pass</h3><p>In the forward pass, you will take many filters and convolve them on the input. Each ‘convolution’ gives you a 2D matrix output. You will then stack these outputs to get a 3D volume: </p><center><video width="620" height="440" src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-conv_kiank.mp4" type="video/mp4" controls></video></center><p><strong>Exercise</strong>: Implement the function below to convolve the filters W on an input activation A_prev. This function takes as input A_prev, the activations output by the previous layer (for a batch of m inputs), F filters/weights denoted by W, and a bias vector denoted by b, where each filter has its own (single) bias. Finally you also have access to the hyperparameters dictionary which contains the stride and the padding. </p><p><strong>Hint</strong>: </p><ol><li>To select a 2x2 slice at the upper left corner of a matrix “a_prev” (shape (5,5,3)), you would do:<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a_slice_prev = a_prev[<span class="number">0</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">2</span>,:]</span><br></pre></td></tr></table></figure>This will be useful when you will define <code>a_slice_prev</code> below, using the <code>start/end</code> indexes you will define.</li><li>To define a_slice you will need to first define its corners <code>vert_start</code>, <code>vert_end</code>, <code>horiz_start</code> and <code>horiz_end</code>. This figure may be helpful for you to find how each of the corner can be defined using h, w, f and s in the code below.</li></ol><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhurvqf17j30s80ksq3s.jpg" alt=""></p><caption>    <center> <u> <font color='purple'> **Figure 3: Definition of a slice using vertical and horizontal start/end (with a 2x2 filter)** <br> This figure shows only a single channel. </u> </font>    </center></caption><p><strong>Reminder</strong>:<br>The formulas relating the output shape of the convolution to the input shape is:</p><script type="math/tex; mode=display">n_H = \lfloor \frac{n_{H_{prev}} - f + 2 \times pad}{stride} \rfloor +1</script><script type="math/tex; mode=display">n_W = \lfloor \frac{n_{W_{prev}} - f + 2 \times pad}{stride} \rfloor +1</script><script type="math/tex; mode=display">n_C = \text{number of filters used in the convolution}</script><p>For this exercise, we won’t worry about vectorization, and will just implement everything with for-loops.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: conv_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span>(<span class="params">A_prev, W, b, hparameters</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the forward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing &quot;stride&quot; and &quot;pad&quot;</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward() function</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev&#x27;s shape (≈1 line)  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W&#x27;s shape (≈1 line)</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from &quot;hparameters&quot; (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">&quot;stride&quot;</span>]</span><br><span class="line">    pad = hparameters[<span class="string">&quot;pad&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span></span><br><span class="line">    n_H = <span class="built_in">int</span>((n_H_prev - f + <span class="number">2</span>*pad) / stride) + <span class="number">1</span></span><br><span class="line">    n_W = <span class="built_in">int</span>((n_W_prev - f + <span class="number">2</span>*pad) / stride) + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize the output volume Z with zeros. (≈1 line)</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create A_prev_pad by padding A_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                               <span class="comment"># loop over the batch of training examples</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i, :, :, :]                               <span class="comment"># Select ith training example&#x27;s padded activation</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                           <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):                       <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_C):                   <span class="comment"># loop over channels (= #filters) of the output volume</span></span><br><span class="line">                    <span class="comment"># Find the corners of the current &quot;slice&quot; (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span></span><br><span class="line">                    Z[i, h, w, c] = conv_single_step(a_slice_prev, W[:, :, :, c], b[:, :, :, c])</span><br><span class="line">                                        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save information in &quot;cache&quot; for the backprop</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">10</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">W = np.random.randn(<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">8</span>)</span><br><span class="line">b = np.random.randn(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">8</span>)</span><br><span class="line">hparameters = &#123;<span class="string">&quot;pad&quot;</span> : <span class="number">2</span>,</span><br><span class="line">               <span class="string">&quot;stride&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line">Z, cache_conv = conv_forward(A_prev, W, b, hparameters)</span><br><span class="line">print(<span class="string">&quot;Z&#x27;s mean =&quot;</span>, np.mean(Z))</span><br><span class="line">print(<span class="string">&quot;Z[3,2,1] =&quot;</span>, Z[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">print(<span class="string">&quot;cache_conv[0][1][2][3] =&quot;</span>, cache_conv[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>][<span class="number">3</span>])</span><br></pre></td></tr></table></figure><pre><code>Z&#39;s mean = 0.0489952035289Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437  5.18531798  8.75898442]cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]</code></pre><p><strong>Expected Output</strong>:</p><table>    <tr>        <td>            **Z's mean**        </td>        <td>            0.0489952035289        </td>    </tr>    <tr>        <td>            **Z[3,2,1]**        </td>        <td>            [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437  5.18531798  8.75898442]        </td>    </tr>    <tr>        <td>            **cache_conv[0][1][2][3]**        </td>        <td>            [-0.20075807  0.18656139  0.41005165]        </td>    </tr></table><p>Finally, CONV layer should also contain an activation, in which case we would add the following line of code:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolve the window to get back one output neuron</span></span><br><span class="line">Z[i, h, w, c] = ...</span><br><span class="line"><span class="comment"># Apply activation</span></span><br><span class="line">A[i, h, w, c] = activation(Z[i, h, w, c])</span><br></pre></td></tr></table></figure><p>You don’t need to do it here. </p><h2 id="4-Pooling-layer"><a href="#4-Pooling-layer" class="headerlink" title="4 - Pooling layer"></a>4 - Pooling layer</h2><p>The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are: </p><ul><li><p>Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.</p></li><li><p>Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.</p></li></ul><table><td><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhuu5nd58j31720nadgp.jpg" style="width:500px;height:300px;"><td><td><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhuuzrvabj31620my3zh.jpg" style="width:500px;height:300px;"><td></table><p>These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size $f$. This specifies the height and width of the fxf window you would compute a max or average over. </p><h3 id="4-1-Forward-Pooling"><a href="#4-1-Forward-Pooling" class="headerlink" title="4.1 - Forward Pooling"></a>4.1 - Forward Pooling</h3><p>Now, you are going to implement MAX-POOL and AVG-POOL, in the same function. </p><p><strong>Exercise</strong>: Implement the forward pass of the pooling layer. Follow the hints in the comments below.</p><p><strong>Reminder</strong>:<br>As there’s no padding, the formulas binding the output shape of the pooling to the input shape is:</p><script type="math/tex; mode=display">n_H = \lfloor \frac{n_{H_{prev}} - f}{stride} \rfloor +1</script><script type="math/tex; mode=display">n_W = \lfloor \frac{n_{W_{prev}} - f}{stride} \rfloor +1</script><script type="math/tex; mode=display">n_C = n_{C_{prev}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: pool_forward</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span>(<span class="params">A_prev, hparameters, mode = <span class="string">&quot;max&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the forward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing &quot;f&quot; and &quot;stride&quot;</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string (&quot;max&quot; or &quot;average&quot;)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from the input shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from &quot;hparameters&quot;</span></span><br><span class="line">    f = hparameters[<span class="string">&quot;f&quot;</span>]</span><br><span class="line">    stride = hparameters[<span class="string">&quot;stride&quot;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Define the dimensions of the output</span></span><br><span class="line">    n_H = <span class="built_in">int</span>(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = <span class="built_in">int</span>(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize output matrix A</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))              </span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                         <span class="comment"># loop over the training examples</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                     <span class="comment"># loop on the vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):                 <span class="comment"># loop on the horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span> (n_C):            <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current &quot;slice&quot; (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)</span></span><br><span class="line">                    a_prev_slice = A_prev[i, :, :, c]</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">&quot;max&quot;</span>:</span><br><span class="line">                        A[i, h, w, c] = np.<span class="built_in">max</span>(a_prev_slice[vert_start:vert_end, horiz_start:horiz_end])</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">&quot;average&quot;</span>:</span><br><span class="line">                        A[i, h, w, c] = np.mean(a_prev_slice[vert_start:vert_end, horiz_start:horiz_end])</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Store the input and hparameters in &quot;cache&quot; for pool_backward()</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">hparameters = &#123;<span class="string">&quot;stride&quot;</span> : <span class="number">2</span>, <span class="string">&quot;f&quot;</span>: <span class="number">3</span>&#125;</span><br><span class="line"></span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line">print(<span class="string">&quot;mode = max&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;A =&quot;</span>, A)</span><br><span class="line">print()</span><br><span class="line">A, cache = pool_forward(A_prev, hparameters, mode = <span class="string">&quot;average&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;mode = average&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;A =&quot;</span>, A)</span><br></pre></td></tr></table></figure><pre><code>mode = maxA = [[[[ 1.74481176  0.86540763  1.13376944]]] [[[ 1.13162939  1.51981682  2.18557541]]]]mode = averageA = [[[[ 0.02105773 -0.20328806 -0.40389855]]] [[[-0.22154621  0.51716526  0.48155844]]]]</code></pre><p><strong>Expected Output:</strong></p><table>    <tr>    <td>    A  =    </td>        <td>         [[[[ 1.74481176  0.86540763  1.13376944]]] [[[ 1.13162939  1.51981682  2.18557541]]]]        </td>    </tr>    <tr>    <td>    A  =    </td>        <td>         [[[[ 0.02105773 -0.20328806 -0.40389855]]] [[[-0.22154621  0.51716526  0.48155844]]]]        </td>    </tr></table><p>Congratulations! You have now implemented the forward passes of all the layers of a convolutional network. </p><p>The remainer of this notebook is optional, and will not be graded.</p><h2 id="5-Backpropagation-in-convolutional-neural-networks-OPTIONAL-UNGRADED"><a href="#5-Backpropagation-in-convolutional-neural-networks-OPTIONAL-UNGRADED" class="headerlink" title="5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)"></a>5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)</h2><p>In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers don’t need to bother with the details of the backward pass. The backward pass for convolutional networks is complicated. If you wish however, you can work through this optional portion of the notebook to get a sense of what backprop in a convolutional network looks like. </p><p>When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in convolutional neural networks you can to calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are not trivial and we did not derive them in lecture, but we briefly presented them below.</p><h3 id="5-1-Convolutional-layer-backward-pass"><a href="#5-1-Convolutional-layer-backward-pass" class="headerlink" title="5.1 - Convolutional layer backward pass"></a>5.1 - Convolutional layer backward pass</h3><p>Let’s start by implementing the backward pass for a CONV layer. </p><h4 id="5-1-1-Computing-dA"><a href="#5-1-1-Computing-dA" class="headerlink" title="5.1.1 - Computing dA:"></a>5.1.1 - Computing dA:</h4><p>This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:</p><script type="math/tex; mode=display">dA += \sum _{h=0} ^{n_H} \sum_{w=0} ^{n_W} W_c \times dZ_{hw} \tag{1}</script><p>Where $W<em>c$ is a filter and $dZ</em>{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices. </p><p>In code, inside the appropriate for-loops, this formula translates into:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure></p><h4 id="5-1-2-Computing-dW"><a href="#5-1-2-Computing-dW" class="headerlink" title="5.1.2 - Computing dW:"></a>5.1.2 - Computing dW:</h4><p>This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:</p><script type="math/tex; mode=display">dW_c  += \sum _{h=0} ^{n_H} \sum_{w=0} ^ {n_W} a_{slice} \times dZ_{hw}  \tag{2}</script><p>Where $a<em>{slice}$ corresponds to the slice which was used to generate the acitivation $Z</em>{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. </p><p>In code, inside the appropriate for-loops, this formula translates into:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure></p><h4 id="5-1-3-Computing-db"><a href="#5-1-3-Computing-db" class="headerlink" title="5.1.3 - Computing db:"></a>5.1.3 - Computing db:</h4><p>This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:</p><script type="math/tex; mode=display">db = \sum_h \sum_w dZ_{hw} \tag{3}</script><p>As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. </p><p>In code, inside the appropriate for-loops, this formula translates into:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure><br><strong>Exercise</strong>: Implement the <code>conv_backward</code> function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span>(<span class="params">dZ, cache</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from &quot;cache&quot;</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev&#x27;s shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W&#x27;s shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from &quot;hparameters&quot;</span></span><br><span class="line">    stride = hparameters[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    pad = hparameters[<span class="string">&#x27;pad&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ&#x27;s shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = dZ.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           </span><br><span class="line">    dW = np.zeros((f, f, n_C_prev, n_C))</span><br><span class="line">    db = np.zeros((<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current &quot;slice&quot;</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter&#x27;s parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:, :, :, c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example&#x27;s dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">dA, dW, db = conv_backward(Z, cache_conv)</span><br><span class="line">print(<span class="string">&quot;dA_mean =&quot;</span>, np.mean(dA))</span><br><span class="line">print(<span class="string">&quot;dW_mean =&quot;</span>, np.mean(dW))</span><br><span class="line">print(<span class="string">&quot;db_mean =&quot;</span>, np.mean(db))</span><br></pre></td></tr></table></figure><pre><code>dA_mean = 1.45243777754dW_mean = 1.72699145831db_mean = 7.83923256462</code></pre><p><strong> Expected Output: </strong></p><table>    <tr>        <td>            **dA_mean**        </td>        <td>            1.45243777754        </td>    </tr>    <tr>        <td>            **dW_mean**        </td>        <td>            1.72699145831        </td>    </tr>    <tr>        <td>            **db_mean**        </td>        <td>            7.83923256462        </td>    </tr></table><h2 id="5-2-Pooling-layer-backward-pass"><a href="#5-2-Pooling-layer-backward-pass" class="headerlink" title="5.2 Pooling layer - backward pass"></a>5.2 Pooling layer - backward pass</h2><p>Next, let’s implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. </p><h3 id="5-2-1-Max-pooling-backward-pass"><a href="#5-2-1-Max-pooling-backward-pass" class="headerlink" title="5.2.1 Max pooling - backward pass"></a>5.2.1 Max pooling - backward pass</h3><p>Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called <code>create_mask_from_window()</code> which does the following: </p><script type="math/tex; mode=display">X = \begin{bmatrix}1 && 3 \\4 && 2\end{bmatrix} \quad \rightarrow  \quad M =\begin{bmatrix}0 && 0 \\1 && 0\end{bmatrix}\tag{4}</script><p>As you can see, this function creates a “mask” matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). You’ll see later that the backward pass for average pooling will be similar to this but using a different mask.  </p><p><strong>Exercise</strong>: Implement <code>create_mask_from_window()</code>. This function will be helpful for pooling backward.<br>Hints:</p><ul><li><a href="">np.max()</a> may be helpful. It computes the maximum of an array.</li><li>If you have a matrix X and a scalar x: <code>A = (X == x)</code> will return a matrix A of the same size as X such that:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A[i,j] &#x3D; True if X[i,j] &#x3D; x</span><br><span class="line">A[i,j] &#x3D; False if X[i,j] !&#x3D; x</span><br></pre></td></tr></table></figure></li><li>Here, you don’t need to consider cases where there are several maxima in a matrix.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈1 line)</span></span><br><span class="line">    mask = (x == np.<span class="built_in">max</span>(x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">x = np.random.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">mask = create_mask_from_window(x)</span><br><span class="line">print(<span class="string">&#x27;x = &#x27;</span>, x)</span><br><span class="line">print(<span class="string">&quot;mask = &quot;</span>, mask)</span><br></pre></td></tr></table></figure><pre><code>x =  [[ 1.62434536 -0.61175641 -0.52817175] [-1.07296862  0.86540763 -2.3015387 ]]mask =  [[ True False False] [False False False]]</code></pre><p><strong>Expected Output:</strong> </p><table> <tr> <td>**x =**</td><td>[[ 1.62434536 -0.61175641 -0.52817175] <br> [-1.07296862  0.86540763 -2.3015387 ]]  </td></tr><tr> <td>**mask =**</td><td>[[ True False False] <br> [False False False]]</td></tr></table><p>Why do we keep track of the position of the max? It’s because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will “propagate” the gradient back to this particular input value that had influenced the cost. </p><h3 id="5-2-2-Average-pooling-backward-pass"><a href="#5-2-2-Average-pooling-backward-pass" class="headerlink" title="5.2.2 - Average pooling - backward pass"></a>5.2.2 - Average pooling - backward pass</h3><p>In max pooling, for each input window, all the “influence” on the output came from a single input value—the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.</p><p>For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you’ll use for the backward pass will look like: </p><script type="math/tex; mode=display">dZ = 1 \quad \rightarrow  \quad dZ =\begin{bmatrix}1/4 && 1/4 \\1/4 && 1/4\end{bmatrix}\tag{5}</script><p>This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average. </p><p><strong>Exercise</strong>: Implement the function below to equally distribute a value dz through a matrix of dimension shape. <a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html">Hint</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span>(<span class="params">dz, shape</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = <span class="number">1</span> / (n_H * n_W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the &quot;average&quot; value (≈1 line)</span></span><br><span class="line">    a = np.ones((n_H, n_W)) * dz * average</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = distribute_value(<span class="number">2</span>, (<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">print(<span class="string">&#x27;distributed value =&#x27;</span>, a)</span><br></pre></td></tr></table></figure><pre><code>distributed value = [[ 0.5  0.5] [ 0.5  0.5]]</code></pre><p><strong>Expected Output</strong>: </p><table> <tr> <td>distributed_value =</td><td>[[ 0.5  0.5]<br\> [ 0.5  0.5]]</td></tr></table><h3 id="5-2-3-Putting-it-together-Pooling-backward"><a href="#5-2-3-Putting-it-together-Pooling-backward" class="headerlink" title="5.2.3 Putting it together: Pooling backward"></a>5.2.3 Putting it together: Pooling backward</h3><p>You now have everything you need to compute backward propagation on a pooling layer.</p><p><strong>Exercise</strong>: Implement the <code>pool_backward</code> function in both modes (<code>&quot;max&quot;</code> and <code>&quot;average&quot;</code>). You will once again use 4 for-loops (iterating over training examples, height, width, and channels). You should use an <code>if/elif</code> statement to see if the mode is equal to <code>&#39;max&#39;</code> or <code>&#39;average&#39;</code>. If it is equal to ‘average’ you should use the <code>distribute_value()</code> function you implemented above to create a matrix of the same shape as <code>a_slice</code>. Otherwise, the mode is equal to ‘<code>max</code>‘, and you will create a mask with <code>create_mask_from_window()</code> and multiply it by the corresponding value of dZ.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span>(<span class="params">dA, cache, mode = <span class="string">&quot;max&quot;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Implements the backward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer&#x27;s input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string (&quot;max&quot; or &quot;average&quot;)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from cache (≈1 line)</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from &quot;hparameters&quot; (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">&#x27;stride&#x27;</span>]</span><br><span class="line">    f = hparameters[<span class="string">&#x27;f&#x27;</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev&#x27;s shape and dA&#x27;s shape (≈2 lines)</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape</span><br><span class="line">    m, n_H, n_W, n_C = dA.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev with zeros (≈1 line)</span></span><br><span class="line">    dA_prev = np.zeros((A_prev.shape))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select training example from A_prev (≈1 line)</span></span><br><span class="line">        a_prev = A_prev[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current &quot;slice&quot; (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the backward propagation in both modes.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">&quot;max&quot;</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Use the corners and &quot;c&quot; to define the current slice from a_prev (≈1 line)</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                        <span class="comment"># Create the mask from a_prev_slice (≈1 line)</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        <span class="comment"># Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">&quot;average&quot;</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Get the value a from dA (≈1 line)</span></span><br><span class="line">                        da = dA[i, h, w, c]</span><br><span class="line">                        <span class="comment"># Define the shape of the filter as fxf (≈1 line)</span></span><br><span class="line">                        shape = (f, f)</span><br><span class="line">                        <span class="comment"># Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line">    <span class="comment">### END CODE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">A_prev = np.random.randn(<span class="number">5</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">hparameters = &#123;<span class="string">&quot;stride&quot;</span> : <span class="number">1</span>, <span class="string">&quot;f&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line">A, cache = pool_forward(A_prev, hparameters)</span><br><span class="line">dA = np.random.randn(<span class="number">5</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">dA_prev = pool_backward(dA, cache, mode = <span class="string">&quot;max&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;mode = max&quot;</span>)</span><br><span class="line">print(<span class="string">&#x27;mean of dA = &#x27;</span>, np.mean(dA))</span><br><span class="line">print(<span class="string">&#x27;dA_prev[1,1] = &#x27;</span>, dA_prev[<span class="number">1</span>,<span class="number">1</span>])  </span><br><span class="line">print()</span><br><span class="line">dA_prev = pool_backward(dA, cache, mode = <span class="string">&quot;average&quot;</span>)</span><br><span class="line">print(<span class="string">&quot;mode = average&quot;</span>)</span><br><span class="line">print(<span class="string">&#x27;mean of dA = &#x27;</span>, np.mean(dA))</span><br><span class="line">print(<span class="string">&#x27;dA_prev[1,1] = &#x27;</span>, dA_prev[<span class="number">1</span>,<span class="number">1</span>]) </span><br></pre></td></tr></table></figure><pre><code>mode = maxmean of dA =  0.145713902729dA_prev[1,1] =  [[ 0.          0.        ] [ 5.05844394 -1.68282702] [ 0.          0.        ]]mode = averagemean of dA =  0.145713902729dA_prev[1,1] =  [[ 0.08485462  0.2787552 ] [ 1.26461098 -0.25749373] [ 1.17975636 -0.53624893]]</code></pre><p><strong>Expected Output</strong>: </p><p>mode = max:</p><table> <tr> <td>**mean of dA =**</td><td>0.145713902729  </td></tr><tr> <td>**dA_prev[1,1] =** </td><td>[[ 0.          0.        ] <br> [ 5.05844394 -1.68282702] <br> [ 0.          0.        ]]</td></tr></table><p>mode = average</p><table> <tr> <td>**mean of dA =**</td><td>0.145713902729  </td></tr><tr> <td>**dA_prev[1,1] =** </td><td>[[ 0.08485462  0.2787552 ] <br> [ 1.26461098 -0.25749373] <br> [ 1.17975636 -0.53624893]]</td></tr></table><h3 id="Congratulations"><a href="#Congratulations" class="headerlink" title="Congratulations !"></a>Congratulations !</h3><p>Congratulation on completing this assignment. You now understand how convolutional neural networks work. You have implemented all the building blocks of a neural network. In the next assignment you will implement a ConvNet using TensorFlow.</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN 部分理论</title>
      <link href="2019/01/13/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/1%20CNN%20%E9%83%A8%E5%88%86%E7%90%86%E8%AE%BA/"/>
      <url>2019/01/13/2019/3%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/Week%201/1%20CNN%20%E9%83%A8%E5%88%86%E7%90%86%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<blockquote><p>卷积神经网络是深度学习在计算机视觉领域应用的基础，这篇文章简要谈谈 Padding、Stride、Pooling Layer 等内容。</p></blockquote><a id="more"></a><h2 id="Padding（填充）"><a href="#Padding（填充）" class="headerlink" title="Padding（填充）"></a>Padding（填充）</h2><p>如果没有填充，那么每经过一次卷积操作，图像的大小都会变小，比如一个大小为 $3 × 3$ 的过滤器在一个大小为 $6 × 6$ 的图像上进行卷积，会得到一个大小为 $4 × 4$ 的图像。事实上，假设原始图像大小为 $n × n$，过滤器大小为 $f × f$，那么卷积后的图像大小为 $(n-f+1) × (n-f+1)$，很明显，这会导致图像的大小在几次卷积操作后就会越来越小。</p><p>没有填充的另一个缺点是，位于图像边缘的像素在卷积操作时只会被用到一次，而更接近图像中心的像素却会被用到多次，这使我们丢失了许多靠近图像边界的信息。</p><p>所以为了解决以上两个问题，一个方法是在进行卷积操作前对图像进行填充。同样用大小为 $6×6$ 的图像为例，如果在该图像周围填充一圈像素点，那么该图像的大小就会变为 $8×8$，再用一个大小为 $3×3$ 的过滤器对该图像进行卷积后，得到的图像的大小为 $6×6$，这就使得卷积后的图像大小和原始图像（填充之前）的大小一样了。</p><p>我们用 p 来表示填充的像素圈数，此时卷积后的图像大小为 $(n+2p-f+1)×(n+2p-f+1)$，合理设置 p 的值，即可保证卷积后的图像大小不变，同时也提高了原始图像（填充前）边界信息的利用率。</p><p>如果希望卷积后图像大小不变，即 $n+2p-f+1=n$，解得 $p = \cfrac{f-1}{2}$ 即可。这里的 $f$ 在绝大多数情况都是奇数而不是偶数，这样有两个好处：</p><ol><li>如果 $f$ 是偶数，将得到不对称的填充，因为这里的 p 无法除尽；</li><li>$f$ 为奇数可以保证过滤器有一个中心像素点，可以称之为中心像素，这样可以方便我们描述过滤器的位置。</li></ol><p>当然用偶数的 $f$ 也可以得到不错的结果，但一般情况下人们都会把 $f$ 设置成奇数。</p><h2 id="Stride（步幅）"><a href="#Stride（步幅）" class="headerlink" title="Stride（步幅）"></a>Stride（步幅）</h2><p>没有引入 Stride（步幅）时，过滤器每次只移动一个像素点距离。引入 Stride 后用 s 表示步幅，此时过滤器每次移动 s 个像素点距离。比如一个大小为 $3×3$ 的过滤器在一个大小为 $7×7$ 的图像上进行步幅为 2 的卷积操作后，得到的图像大小为 $3×3$。</p><p>这里的关系式为，如果原始图像大小为 $n×n$，过滤器大小为 $f×f$，填充为 p，步幅为 s，那么卷积后的图像大小为 $(\cfrac{n+2p-f}{s}+1) × (\cfrac{n+2p-f}{s}+1)$。</p><p>一个问题是，如果式子中的分数除不尽怎么办？办法是对 $(\cfrac{n+2p-f}{s}+1)$ 向下取整，因为如果过滤器移动到了其一部分超出图像边界的地方，那么这里是不进行卷积运算的。</p><h2 id="在三维图像上进行卷积运算"><a href="#在三维图像上进行卷积运算" class="headerlink" title="在三维图像上进行卷积运算"></a>在三维图像上进行卷积运算</h2><p>一副彩色图像有 RGB 三个通道，一个 $6×6$ 大小的图像维度应该是 $6×6×3$，这里的 3 便对应三个通道，对这样的图像进行卷积运算时，用到的过滤器的通道数必须与其一致，比如这里可以使用<font color=red><strong>一个</strong></font>维度为 $3×3×3$ 的过滤器，得到卷积后的图像维度为 $4×4$。如果使用 $x$ 个该维度的过滤器，那么卷积后的图像维度应为 $4×4×x$。</p><p>具体该如何运算呢？先以一个过滤器为例。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzi8h72nd8j31hk0u0q6a.jpg" alt=""></p><p>方便起见，这里不讨论 Padding 和 Stride。如上图所示，先将过滤器的三个通道分别于原图像左上角贴合，然后将这 27 个位置的元素对应相乘再相加，得到结果，填入输出图像的最左上角元素，然后移动过滤器继续运算，直到移动到原图像右下角为止。</p><p>如果有多个过滤器，比如两个，如图所示：</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzi8xyeat8j31ha0u0tcq.jpg" alt=""></p><p>那么按照刚才的计算方法，会得到两个维度为 $4×4$ 的图像，再将这两幅二维图像叠在一起便得到了维度为 $4×4×2$ 的图像。对于其他数量的过滤器，方法也是一样的。</p><h2 id="Pooling-layer-池化层"><a href="#Pooling-layer-池化层" class="headerlink" title="(Pooling layer) 池化层"></a>(Pooling layer) 池化层</h2><h3 id="Max-pooling-最大池化"><a href="#Max-pooling-最大池化" class="headerlink" title="Max pooling (最大池化)"></a>Max pooling (最大池化)</h3><p>假设原始图像大小为 $4×4$，过滤器大小为 $2×2$，步幅为 2，不填充，那么经过池化层后的图像大小为 $2×2$，过滤器移动的方式和之前一样，只不过这里进行的运算不是卷积，而是取最大值，具体操作看这张图：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzj1bvjd39j31hf0u0q61.jpg" alt=""></p><p>如上图所示，原始图像被分成了四个区域，这四个区域实际就是过滤器进行操作的四个区域，在每个区域里，取最大值赋给输出图像的相应位置。</p><p>如果考虑输入图像的通道数目，比如 $n×n×n_c$，其中 $n_c$ 代表通道数目，那么输出图像的通道数目也是 $n_c$，具体过程就是在输入图像的每个通道都进行池化操作。</p><h3 id="Average-pooling-平均池化"><a href="#Average-pooling-平均池化" class="headerlink" title="Average pooling (平均池化)"></a>Average pooling (平均池化)</h3><p>顾名思义，与 Max Pooling 不同，Average pooling 在运算时取过滤器覆盖区域的平均像素值，再赋给输出图像的相应位置。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzj1chskjtj31hb0u00v0.jpg" alt=""></p><h3 id="关于超参数的选择"><a href="#关于超参数的选择" class="headerlink" title="关于超参数的选择"></a>关于超参数的选择</h3><ul><li>f 和 s 都取 2 或者 f 取 3，s 取 2，这会使得输入图像的长宽近似变为一半</li><li>p 一般取 0，即不作任何填充</li><li>Max pooling 比 Average pooling 用得更多些</li></ul><h3 id="卷积神经网络的优势"><a href="#卷积神经网络的优势" class="headerlink" title="卷积神经网络的优势"></a>卷积神经网络的优势</h3><h4 id="Parameter-sharing-参数共享"><a href="#Parameter-sharing-参数共享" class="headerlink" title="Parameter sharing (参数共享)"></a>Parameter sharing (参数共享)</h4><p>如果一个过滤器是专门用来检测垂直边缘的，那么对于输入图像的每一部分，该过滤器都能起作用，换句话说，该过滤器中的参数对于输入图像的每一部分都是有作用的，这使得一个用于检测某种特征（如垂直边界）的过滤器能够被用在输入图像的任何一个部分，这同时使得神经网络的参数大大减少，除了减少运算量，在一定程度上也能防止过拟合的发生。</p><h4 id="Sparsity-of-connections-连接的稀疏性"><a href="#Sparsity-of-connections-连接的稀疏性" class="headerlink" title="Sparsity of connections (连接的稀疏性)"></a>Sparsity of connections (连接的稀疏性)</h4><p>在每一层，每个输出值都只和输入图像中某一部分的输入值有关。这可以实现 translation invarience（平移无关性）。</p><p>关于平移无关性可以参考<a href="https://stats.stackexchange.com/questions/208936/what-is-translation-invariance-in-computer-vision-and-convolutional-neural-netwo#">这篇在 StackExchange 上的讨论</a></p><p>-</p><p>代码练习链接：</p><ul><li><a href="http://www.liuhdme.com/2019/01/10/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">卷积神经网络——代码部分</a></li><li><a href="http://www.liuhdme.com/2019/01/12/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8/">手势识别</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> Computer Vision </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Computer Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning 策略</title>
      <link href="2019/01/11/2019/2%20%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/Week%201/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5/"/>
      <url>2019/01/11/2019/2%20%E7%BB%93%E6%9E%84%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/Week%201/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AD%96%E7%95%A5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇文章简要谈谈实际项目中的一些策略，这些策略有助于找出项目中出现的问题以及准确快速地找出解决问题的方法。</p></blockquote><a id="more"></a><h3 id="为什么需要机器学习策略"><a href="#为什么需要机器学习策略" class="headerlink" title="为什么需要机器学习策略"></a>为什么需要机器学习策略</h3><p>如果想提高一个深度学习系统的准确率，有很多方法：</p><ul><li>收集更多数据</li><li>使训练集的数据更多样化</li><li>训练更长时间</li><li>尝试不同的优化算法</li><li>尝试更大的神经网络</li><li>尝试更小的神经网络</li><li>使用正则化</li><li>改变神经网络的结构</li><li>…</li></ul><p>如果一一尝试这些方法，不仅会话费大量时间，有时得到的效果也非常微小，因此我们需要对具体情况进行仔细分析，从上述方法中找出最关键的方法。</p><h3 id="正交化"><a href="#正交化" class="headerlink" title="正交化"></a>正交化</h3><p>一些非常有经验的深度学习研究人员非常清楚哪些超参数应该调整，哪些不该调整，这里“该不该”的标准就是，如果一个超参数调整后只会影响系统某一方面的性能，那么这个超参数就是应该调整的。比如，之前的文章讲过，数据集应该被分成训练集、开发集合测试集，那么也许你会发现系统在训练集上和开发集上表现良好，而在测试集上表现很差，那么我们就希望能通过调整几个超参数来提高系统在测试集上的表现，而不影响其在训练集和开发集上的表现。这种调整一个或几个超参数以达到只影响某一方面的性能的特性就叫做<strong>正交化</strong>。</p><p>这里有一些可能出现的问题以及具有正交化这种特点的处理办法：</p><ol><li>如果训练集的准确率较低，你可以尝试使用更大的神经网络、其他的优化算法；</li><li>如果开发集的准确率较低，你可以尝试使用正则化、更大的训练集；</li><li>如果测试集的准确率较低，你可以尝试使用更大的开发集；</li><li>如果你的系统在测试集的表现没有问题，却在实际应用中表现不理想，你可以尝试改变开发集中的数据，或是换一个损失函数。</li></ol><h3 id="单一的量化评估指标"><a href="#单一的量化评估指标" class="headerlink" title="单一的量化评估指标"></a>单一的量化评估指标</h3><p>无论你在尝试调整超参数，或是选择不同的机器学习算法，亦或是在构建机器学习系统时尝试不同的配置项，你都会发现如果你有一个单一的量化评估指标，这会让你很轻松地知道新的方法比上一次的方法是更好还是更糟，这会让整个进程加快很多，让我们来看一个例子。</p><p>你构建了一个猫分类器 A，然后调整了一些超参数或别的配置项，又得到了一个猫分类器 B，这时如果你想知道两个分类器哪个更好，你可以考察两个分类器的精确率和召回率。所谓精确率就是一个分类器认为是猫的图片中真正是猫的比例，而召回率是指对于所有是猫的图片，你的分类器识别出来了多少张。你可能会发现分类器 A 的精确率很高，而召回率很低，分类器 B 的精确率很低，而召回率很高，这就让你十分苦恼，究竟应该选择哪个分类器才好呢？</p><p>出现这种尴尬局面的原因就在于你使用了两个评估指标，因此，与其使用两个指标，倒不如只使用一个，而这个新的指标可以兼顾精确率和召回率，在机器学习中，一种方法是使用 F1 Score，你可以把它理解为是精确率和召回率的平均值，它的计算公式为</p><script type="math/tex; mode=display">F1 = \cfrac{2}{\frac{1}{P} + \frac{1}{R}}</script><p>其中 P 是精确率，R 是召回率，实际上这计算的二者的调和平均值，但你仍然可以理解为这是一种平均计算。</p><p>那么在这个例子中，你只需要考察两个分类器中哪个的 F1 分数更高，那么选择这个更高的就可以了。</p><h3 id="满足与优化指标（Satisficing-Matrics-and-Optimizing-Matrics）"><a href="#满足与优化指标（Satisficing-Matrics-and-Optimizing-Matrics）" class="headerlink" title="满足与优化指标（Satisficing Matrics and Optimizing Matrics）"></a>满足与优化指标（Satisficing Matrics and Optimizing Matrics）</h3><p>虽然单一的评估指标看上去很不错，但想找到这种单一的指标并不是很容易，比如：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-30-071738.png" alt=""></p><p>你很难决定到底应该如何把 F1 分数和运行时间适当的结合起来，如果只是求平均值的话，这样看起来似乎有些过于刻意了，而且当指标很多时，如果把他们结合起来的方法不当，那么即使你得到了一个单一评估指标，这个指标的作用可能也不见得有多好。</p><p>怎么办呢？这里还有一些方法，即<strong>满足指标</strong>与<strong>优化指标</strong>，什么意思呢，我们仍然沿用上图中的例子。</p><p>假设你希望得到的分类器的运行时间必须小于 100 毫秒，在满足这个条件下，F1 分数越高越好，那么你就可以轻松选择分类器 B，而不用再纠结如何把 F1 分数和运行时间结合起来。这个例子中，运行时间就是我们所说的满足指标，意味着它必须要满足一个最低的值，如果低于了这个值，那我们就不予考虑，比如这里的分类器 C，它的运行时间远远超过了 100 毫秒，即使它的的 F1 分数还不错，我们也不会考虑它。这里的 F1 分数就是优化指标，意味着在满足了满足指标的条件下，它越高越好。</p><p>可能你一共有 N 个关心的指标，那么你可以只选择其中的一个作为优化指标，剩下的 N-1 个作为满足指标，那么你只需要找到那个满足了所有 N-1 个满足指标的分类器中，优化指标最高的那个分类器就可以了。</p><p>当然了，这些指标一般是用在你的训练集/开发集/测试集中的，因此你必须要把你数据集构建成这三部分，下面我们就谈谈如何进行这样的构建。</p><h3 id="训练集-开发集-测试集分布"><a href="#训练集-开发集-测试集分布" class="headerlink" title="训练集/开发集/测试集分布"></a>训练集/开发集/测试集分布</h3><p>你构建训练集、开发集、测试集的方法将会对构建系统的进展情况产生巨大的影响，这里我们先重点谈谈如何设置开发集和测试集。</p><p>开发集有时也被称为交叉验证集，它和测试集有什么作用呢？比如你在训练集上训练了很多不同模型，那么你就可以用开发集来评估这些模型的表现情况，然后就可以选择一个最好的模型，再使用测试集来评估这个模型。</p><p>举一个更加具体的例子。你正在构建一个猫分类器，你想在一下国家和地区推广你的分类器：</p><ul><li>美国</li><li>英国</li><li>欧洲其他地区</li><li>南美</li><li>印度</li><li>中国</li><li>亚洲其他地区</li><li>澳大利亚</li></ul><p>那么如何设置你的开发集和测试集呢？有一种想法是，你可已选择前四个国家和地区中的猫片作为开发集，后四个国家和地区中的猫片作为测试集。事实证明，这是一个非常糟糕的想法 :) 因为在这个例子中，你的开发集和测试集中的数据来自于不同的分布。如果把这个过程比作瞄准——射击的话，就好比你或你的团队花了数月的时间瞄准一个目标，最后射击的时候却打向了另外一个目标，很难想象你们的结果会好到哪里去。</p><p>合适的做法是，把你们收集到的所有地区的数据混合起来并随机打乱，再分成开发集和测试集，这样开发集和测试集就都拥有了上述八个国家和地区的数据，即开发集和测试集中的数据具有了相同的分布。</p><p>这里还有一个问题，即如何选择开发集和测试集的大小呢，我们下面讨论这个问题。</p><h3 id="开发集和测试集的大小"><a href="#开发集和测试集的大小" class="headerlink" title="开发集和测试集的大小"></a>开发集和测试集的大小</h3><p>建立开发集和测试集的方式随着深度学习时代的到来而发生着变化。机器学习中有一条经验法则：将你拥有的所有数据 按照 7 : 3 的比例分割成训练集和测试集，如果也需要建立开发集，则可以将 60% 的数据作为训练集，20% 作为开发集，20% 作为测试集。</p><p>在早期的机器学习时代，这曾经是非常合理的做法，尤其是在数据规模不大的时候。比如如果你总共只有几百个或几千个甚至几万个样例，使用 70/30 或 60/20/20 的经验法则是非常合理的。但是在现代机器学习中，我们往往需要处理更大量的数据。假如你有 100 万个训练样例，一种合理的做法其实是，使用 98% 的数据作为训练集，1% 作为开发集，1% 作为测试集。</p><p>这样做的原因其实很简单，假如你有一百万个样例，那么其中的 1% 就已经有 1 万个样例了，这对于开发集或测试集来说已经足够了。</p><p>测试集的作用是在系统开发完成后，帮我们最终评估系统性能的，所以测试集的大小只要能保证对系统的评估结构的高置信度即可，因此除非你需要对系统最终的性能有十分精确的测量，你并不需要你的测试集拥有上百万个数据。</p><p>有时候你可能并不需要对最终的系统评估有很高的置信度，那么在在这种情况下没有测试集也是可以的。</p><p>不过我觉得还是有一个测试集要安心一些，防止数据在开发集上过拟合得太厉害。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>学习率衰减</title>
      <link href="2019/01/08/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6%20%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/"/>
      <url>2019/01/08/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/6%20%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F/</url>
      
        <content type="html"><![CDATA[<blockquote><p>学习率衰减（learning rate decay）同样也可以让我们的学习算法运行地更快，它能保证损失函数最终摆动时处在离最优值很近的范围内。</p></blockquote><a id="more"></a><h3 id="为什么需要学习率衰减"><a href="#为什么需要学习率衰减" class="headerlink" title="为什么需要学习率衰减"></a>为什么需要学习率衰减</h3><p>前面我们讲过，mini-batch 梯度下降算法可以提高更新权重的速度，让我们及时看到损失函数的情况，但是每个损失函数并不会一直下降，而是在保证整体趋势减小的情况下略微波动，如果用一个等高线图来表示就是这样的：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-31-111554.png" alt=""></p><p>上图的中心点为最优值点，我们可以看到损失函数渐渐接近最优值，但最后却在最优值附近摆动，这是因为学习率的大小一直不变，如果我们可以让学习率随着损失函数接近最优值或是随着迭代次数的增加而慢慢减小的话，就可以得到下图中的绿线：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-31-112320.png" alt=""></p><p>可以看到，随着损失函数接近最优值，摆动的幅度也在减小，从而保证最后损失函数在离最优值更近的范围内摆动。</p><h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><p>我们的目标是让学习率 $\alpha$ 随着迭代次数的增加而逐渐减小。先明确迭代这个概念，所谓迭代就是让训练集中的所有数据都输入一次网络，这就是一次迭代。如果训练集被分成了许多个 mini batches，那一次迭代就是所有 mini batches 中的数据都依次输入进网络并更新了权重。因此我们可以这样定义学习率：</p><script type="math/tex; mode=display">\alpha = \cfrac{1}{1 + decayRate · epochNum} · \alpha_0</script><p>其中，</p><ul><li>decayRate 是衰减率，衰减率越大，学习率就减小得越快</li><li>epochNum 是迭代次数</li><li>$\alpha_0$ 是初始学习率，初始学习率可以设置得大一些，这样可以保证算法一开始的学习速度不会太慢</li></ul><p>从这个式子可以明显看出，随着迭代次数 epochNum 的增大，$\alpha$ 就会变小。</p><p>decayRate 和 $\alpha_0$ 都是超参数，我们需要在实践中调整这些值以得到满意的结果。</p><p>当然还有一些别的方法实现学习率衰减，比如：</p><script type="math/tex; mode=display">\alpha = decayRate^{epochNum} · \alpha_0</script><script type="math/tex; mode=display">\alpha = \cfrac{k}{\sqrt{epochNum}} · \alpha_0</script><p>还有人会选择手动调整 $\alpha$ 的值，比如每个几小时或几天就去手工调整学习率的值，这种方法也是可行的。</p><p>以上就是对学习率衰减的简要介绍，其基本思想就是让学习率随着迭代次数的增加而减小，以保证损失函数在离最优值比较近的区域内摆动。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adam 优化算法</title>
      <link href="2019/01/07/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5%20Adam%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>2019/01/07/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/5%20Adam%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>在深度学习的发展过程中出现过很多优化算法，但是有些优化算法并不能被非常广泛地使用，因此出现了一些质疑优化算法的声音。但是当有人尝试将动量梯度下降和 RMSprop 结合起来后，人们发现这种新的优化算法效果非常好而且在很多问题上的表现都不错，后来便广泛地使用了起来。</p></blockquote><a id="more"></a><p>Adam 算法本质上是将动量梯度下降算法和 RMSprop 结合了起来。</p><p>如果读过前几篇文章，那么对下面的算法描述一定不陌生：</p><blockquote><p>第 t 次迭代：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;①. 在当前的 mini-batch 上计算 dW, db<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;②. $v<em>{dW} = β_1 · v</em>{dW} + (1 - β) · dW$，$v<em>{db} = β_1 · v</em>{db} + (1 - β) · db$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;③. $s<em>{dW} = β_2 · s</em>{dW} + (1 - β) · dW^2$，$s<em>{db} = β_2 · s</em>{db} + (1 - β) · db^2$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;④. $v^{corrected}<em>{dW} = \cfrac{v</em>{dW}}{1 - \beta<em>1^t}$，$v^{corrected}</em>{db} = \cfrac{v<em>{db}}{1 - \beta_1^t}$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⑤. $s^{corrected}</em>{dW} = \cfrac{s<em>{dW}}{1 - \beta_1^t}$，$s^{corrected}</em>{db} = \cfrac{s<em>{db}}{1 - \beta_1^t}$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⑥. $W := W - α · \cfrac{v</em>{dW}^{corrected}}{\sqrt{s<em>{dW}^{corredted}} + \epsilon}$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;⑦. $b := b - α · \cfrac{v</em>{db}^{corrected}}{\sqrt{s_{db}^{corredted}} + \epsilon}$  </p></blockquote><p>这实际上就是将前面几篇文章里的算法放在了一起，其中为了区分清楚，动量梯度下降中的参数 $\beta$ 替换成了 $\beta_1$，即上面的 ②；RMSprop 中的参数 $\beta$ 替换成了 $\beta_2$，即上面的 ③。</p><p>另外，④、⑤ 两步的目的是消除偏差，可以参考 <a href="">指数加权平均</a> 这篇文章。</p><p>⑥、⑦ 两步中的 $\epsilon$ 是一个非常小的数，其目的是防止分子除以 0，因此加上了一个非常小的数以防止这种情况出现。</p><p>这个算法中有很多超参数：</p><ul><li>$\beta_1$：动量梯度下降中的参数，通常取 0.9</li><li>$\beta_2$：RMSprop 中的参数，通常取 0.999</li><li>$\alpha$：学习率，这个参数需要在实践中调整</li><li>$\epsilon$：防止分母为 0 的参数，通常取 $10^{-8}$</li></ul><p>Adam 算法这个名字的由来是 Adaptive Moment Estimation，即自适应矩估计，因为动量梯度下降计算的是导数的粗略平均值，即一阶矩，而 RMSprop 计算的是导数平方的粗略平均值，即二阶矩，这就是 Adam 优化算法名字的由来。</p><p>以上就是对 Adam 优化算法的简要介绍，Adam 在很多场合都可以使用，它被嵌入在了许多深度学习框架之中，可以直接使用。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RMSprop</title>
      <link href="2019/01/06/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4%20RMSprop/"/>
      <url>2019/01/06/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/4%20RMSprop/</url>
      
        <content type="html"><![CDATA[<blockquote><p>RMSprop 翻译成中文是“均方根传递”，它也能加速算法学习的速度。</p></blockquote><a id="more"></a><p>仍然使用上篇文章中的图：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-31-084515.png" alt=""></p><p>在此，我们假设 W 为水平方向的参数，b 为竖直方向的参数。从上图可以看出，更新 W 时的步伐过小，而更新 b 的步伐过大，这是 dW 过小和 db 过大造成的，如果我们可以增大 dW 和减小 db，就可以使上图蓝线更快地向右行进，而减少上下振动。下面就来实现这个目的。</p><p>回忆一下，在动量梯度下降算法中，算法描述如下：</p><blockquote><p>第 t 次迭代：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在当前的 mini-batch 上计算 dW, db<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v<em>{dW} = β · v</em>{dW} + (1 - β) · dW$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v<em>{db} = β · v</em>{db} + (1 - β) · db$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$W := W - α · v<em>{dW}$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$b := b - α · v</em>{db}$</p></blockquote><p>RMSprop 的算法描述与其十分相似：</p><blockquote><p>第 t 次迭代：<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在当前的 mini-batch 上计算 dW, db<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s<em>{dW} = β · s</em>{dW} + (1 - β) · dW^2$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$s<em>{db} = β · s</em>{db} + (1 - β) · db^2$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$W := W - α · \cfrac{dW}{\sqrt{s<em>{dW}}}$<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$b := b - α · \cfrac{db}{\sqrt{s</em>{db}}}$</p></blockquote><p><strong>Note:</strong> 为了区分清楚，RMSprop 中使用指数滑动平均时用的是 S 而不是 V。</p><p>从 RMSprop 的算法描述可以看到，计算滑动指数平均时，$(1 - \beta)$ 后面的项是 $dW^2$ 和 $db^2$，如此一来，假如 dW 本身较小（比如小于 1），平方后就会更小，$s<em>{dW}$ 也会变小；如果 db 较大（比如大于 1），平方后就会更大，$s</em>{db}$ 也会更大。即我们让小的更小，大的更大，这么做是为了更新权重时做准备。</p><p>观察更新 W 的式子，我们发现 dW 下面有一个 $\sqrt{s<em>{dW}}$，由于在前面的操作中，我们把 $s</em>{dW}$ 变得很小，因此这里 dW 除以一个很小的数就会变大，相当于加大了更新 W 的步伐。对于 b，则相当于减小了在 b 方向上的步伐。最终的效果就是，水平方向前进更快，而竖直方向的振荡变小。</p><p>这便是 RMSprop 算法，当然在实践中我们不可能只有两个参数，但原理对于更多的参数也是一样的，即增大过小的导数，减小过大的导数。</p><p>顺便一提，RMSprop 翻译成中文是均方根传递。</p><p>现在我们介绍了动量梯度下降算法和 RMSprop，如果将二者结合起来会怎么样呢？下篇文章就来介绍一下 Adam 梯度下降算法。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动量梯度下降算法</title>
      <link href="2019/01/05/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3%20%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>2019/01/05/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/3%20%E5%8A%A8%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>上篇文章介绍了指数加权平均，这篇文章介绍在此基础上介绍一下动量梯度下降算法。</p></blockquote><a id="more"></a><p>所谓动量梯度下降算法，简言之就计算梯度的指数加权平均，然后使用这个梯度来更新权重，下面我们来详细解释这句话。</p><p>我们在使用梯度下降算法更新权重时，希望损失函数能减小直到最优值。我们可以在一副等高线图中，画出损失函数随着迭代次数增加而减小的路径，即如下图所示：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-30-104854.png" alt=""></p><p>图中红点为最优点，蓝线为损失函数的减小路径，从图中左侧出发，逐渐靠近最优点。不过我们可以发现，这条路径看起来十分曲折，虽然整体趋势是向右的，但在竖直方向有太多波动，这直接造成了两个负面影响：</p><ol><li>增加了梯度下降的次数，增加了训练时间</li><li>无法使用较大的学习率</li></ol><p>如果使用了较大的学习率，可能会出现下图中紫线的情况：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-30-105539.png" alt=""></p><p>即虽然增大了向右的步伐，同时也增大了上下的步伐，导致损失函数值反而越来越大，因此为了避免振荡过大，我们只能选择较小的学习率。</p><p>为了使其步伐能在水平方向更大，而在竖直方向更小，可以使用之前提到的指数滑动平均。</p><p>我们说过，运用了指数滑动平均后，$v_t$ 相当于粗略计算了前 $\frac{1}{1 - \beta}$ 个数据的平均值，如果我们对导数进行指数滑动平均操作，就会有以下结果：</p><ul><li>竖直方向的振动几乎消失</li><li>水平方向的步伐逐渐加大</li></ul><p>即如下图红线所示</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-31-074100.png" alt=""></p><p>这正好是我们想看到的结果，为什么会这样呢？下面来分析一下。观察上图中的蓝线，我们发现竖直方向的振动大致可以抵消，即每两次上下方向的振动长度大致相等，因此如果对其去平均值，结果就会很接近 0，这就是“竖直方向的振动几乎消失”的原因，而蓝线水平方向的路径都是向右的，对其取平均值不会使其减小，而是随着已经行进的路径增多而变大，这就是“水平方向的步伐逐渐加大”的原因。综上，得到上图中的红线。</p><p>算法描述如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">第 t 次迭代：</span><br><span class="line">在当前的 mini-batch 上计算 dW, db</span><br><span class="line">v_dW &#x3D; β * v_dW + (1 - β) * dW</span><br><span class="line">v_db &#x3D; β * v_db + (1 - β) * db</span><br><span class="line">W -&#x3D; α * v_dW, b -&#x3D; α * v_db</span><br></pre></td></tr></table></figure><p>上面的描述中，$\alpha$ 和 $\beta$ 都是需要调整的超参数，$\beta$ 通常会取 0.9 左右。</p><p>以上就是对动量梯度下降算法的简单介绍，它几乎总是要优于不适用动量的梯度下降算法，不过除此外，还有一些其他的方法也能加速你的训练速度，接下来几篇文章会谈谈 RMSprop 和 Adam 梯度下降算法以及学习率衰减。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>指数加权平均</title>
      <link href="2019/01/04/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2%20%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/"/>
      <url>2019/01/04/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/2%20%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87/</url>
      
        <content type="html"><![CDATA[<blockquote><p>有一些算法比梯度下降算法更有效，为了学习这些算法，我们需要先了解一个概念——指数加权平均（Exponentially weighted averages）</p></blockquote><a id="more"></a><p>我会先讲指数加权平均的具体做法，然后再讲这么做的原因。</p><h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p>假设我们收集了一个在北半球的地区一年中每天的温度，像这样：</p><ul><li>t(1) = 4°C</li><li>t(2) = 9°C</li><li>t(3) = 6°C</li><li>…</li><li>t(180) = 15°C</li><li>…</li></ul><p>将这些温度作出一张图是这样的：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-063740.png" alt=""></p><p>可以发现，一年中开头和结尾气温较低，中间气温较高，整体趋势是这样的，但是噪声很大。</p><p>下面我们来计算指数加权平均：</p><ul><li>$v_0 = 0$</li><li>$v_1 = 0.9 × v_0 + 0.1 × t(1) = 0.4$</li><li>$v_2 = 0.9 × v_1 + 0.1 × t(2) = 1.26$</li><li>$v_3 = 0.9 × v_2 + 0.1 × t(3) = 1.734$</li><li>…</li></ul><p>如果按照这个公式一直计算下去，将得到的结果也画出来，可以得到下面的结果：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-064821.png" alt=""></p><p>其中红色的线就是利用指数加权平均计算出的结果，比之前的结果要平滑了很多。</p><p>具体来讲，在这个例子中，指数加权平均的一般公式为</p><script type="math/tex; mode=display">v_t = \beta × v_{t-1} + (1 - \beta) × \theta(t)</script><p>这里的 $\beta $ 是一个超参数，上面的例子中 $\beta = 0.9$.</p><p>稍后会讲到，这里的 $v_t$ 相当于粗略计算了前 $\cfrac{1}{1 - \beta}$ 天的平均气温，即在这个例子中，$v_t$ 相当于前 10 天的平均气温。</p><p>让我们把 $\beta$ 的值设置得更接近 1，比如说 0.98，那此时的 $v_t$ 便相当于粗略计算了前50 天的平均气温，也将其画出来，便得到了下面的绿线：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-065810.png" alt=""></p><p>可以发现，当 $\beta$ 更大时，得到的曲线更加光滑，因为我们对更多天数的温度做了平均处理，因此曲线就波动更小，更加光滑。但另一方面，曲线会右移，因为这时我们在一个更大的窗口内计算平均值，这导致温度变化时曲线会适应地更加缓慢，这就造成了一些延迟。</p><p>如果将 $/beta$ 减小呢，比如取 $\beta = 0.5$，那么这时 $v_t$ 只相当于前两天的平均天气，将曲线画出来就如下面的黄线所示：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-070348.png" alt=""></p><p>可以发现，由于只对前两天的温度进行平均计算，曲线的波动会更大，有更多的噪声，但是也能更快适应温度的变化。</p><p>总之，使用 $v<em>t = \beta × v</em>{t-1} + (1 - \beta) × \theta(t)$ 这个公式就能实现指数加权平均，在统计学中，这也被称为指数加权滑动平均，但我们可以简称为指数加权平均，通过调整 $beta$ 这个参数，我们可以得到一些略微不同的结果，在这个温度的例子中，这条红色的线要更好一些。</p><p>下面我们来理解一下指数加权平均的本质以及它为什么有效。</p><h3 id="为什么要用指数加权平均"><a href="#为什么要用指数加权平均" class="headerlink" title="为什么要用指数加权平均"></a>为什么要用指数加权平均</h3><p>先看一下这几个式子：</p><blockquote><p>$v<em>{100} = 0.9·v</em>{99} + 0.1 · \theta_{100}$&nbsp;&nbsp;&nbsp;· · · · · · &nbsp;&nbsp;&nbsp;①</p><p>$v<em>{99} = 0.9·v</em>{98} + 0.1 · \theta_{99}$&nbsp;&nbsp;&nbsp;· · · · · · &nbsp;&nbsp;&nbsp;②</p><p>$v<em>{98} = 0.9·v</em>{97} + 0.1 · \theta_{98}$&nbsp;&nbsp;&nbsp;· · · · · · &nbsp;&nbsp;&nbsp;③</p><p>…</p></blockquote><p>将 ③ 式代入 ② 式，再将 ② 式代入 ① 式可以得到：</p><script type="math/tex; mode=display">v_{100} = 0.1·\theta_{100} + 0.1 · 0.9 · \theta_{99} + 0.1 · 0.9^2 · \theta_{98}</script><p>如果继续向下递推，最终可以得到下式</p><script type="math/tex; mode=display">v_{100} = 0.1·(\theta_{100} + 0.9 · \theta_{99} + 0.9^2 · \theta_{98} + ··· + 0.9^{99} · \theta_1)</script><p>从这个式子中可以发现，运用指数加权平均后，第 100 天的温度受到了前面所有天温度的影响，随着向前的天数减小，影响也减小，总之，这就使得处理后的数据更加平滑，噪声更少。</p><p>但是为什么说 $\beta=0.9$ 时，这里的 $v_{100}$ 是对前 10 天气温平均值的粗略计算呢？</p><p>观察 $v<em>{100} = 0.1·(\theta</em>{100} + 0.9 · \theta<em>{99} + 0.9^2 · \theta</em>{98} + ··· + 0.9^{99} · \theta<em>1)$ 中的系数可以发现，$\theta</em>{90}$ 前面的系数应为 $0.9^{10} ≈ 0.34$，由于这些项系数呈指数级减少，从该项开始，后面项的系数逐渐减小到了可以忽略不计的地步，因此我们如果只考察从 $\theta<em>{91}$ 到 $\theta</em>{100}$，并把 $0.1$ 写作分数的形式，就有</p><script type="math/tex; mode=display">v_{100} ≈ \cfrac{\theta_{100} + 0.9 · \theta_{99} + 0.9^2 · \theta_{98} + ··· + 0.9^{9} · \theta_{91}}{10} ≈ \cfrac{\theta_{100} + \theta_{99} + ··· + \theta_{91}}{10}</script><p>这便是对前 10 天平均气温的粗略计算。</p><p>事实上有，</p><script type="math/tex; mode=display">\lim_{x \to 0}(1-x)^\frac{1}{x} = \frac{1}{e} ≈ 0.37</script><p>因此在上面的例子中，可以认为 $\theta<em>{90}$ 的系数 $0.9^{10} = (1-0.1)^{\frac{1}{0.1}} ≈ 0.34 ≈ \cfrac{1}{e}$，这有什么意义呢？这可以说明在计算 $v</em>{100}$ 选择保留 $\theta<em>{91}$ 至 $\theta</em>{100}$ 而去掉 $\theta<em>{90}$ 以及之后的项是有道理和规律的，因为只要 $\beta$ 接近 1，就有一个接近 0 的数 $\epsilon = 1 - \beta$ 使得 $\beta^{\frac{1}{1-\beta}} = (1-\epsilon)^{\frac{1}{\epsilon}} ≈ \cfrac{1}{e} ≈ 0.37$ 可以忽略不计，也就是说系数为 $\beta^\frac{1}{1-\beta}$ 及其之后的项都可丢弃，不参与 $v</em>{100}$ 的计算，从而参与计算的只有系数为 $\beta^\frac{1}{1-\beta}$ 之前的项，而这些项的数目正好为 $\frac{1}{1-\beta}$，因此最后计算的便是前 $\frac{1}{1-\beta}$ 天温度的平均值。比如在上面的例子中 $\frac{1}{1-\beta} = 10$，$v_{100}$ 便是前 10 天温度平均值的粗略计算。</p><p>所以对于计算 $v_{t}$，更一般的式子为</p><script type="math/tex; mode=display">\begin{align}v_{t} & = \cfrac{\theta_t + \beta · \theta_{t-1} + \beta^2 · \theta_{t-2} + ··· + \beta^{t-1} · \theta_1}{\cfrac{1}{1-\beta}}\\& ≈ \cfrac{\theta_t + \beta · \theta_{t-1} + \beta^2 · \theta_{t-2} + ··· + \beta^{\frac{1}{1-\beta}-1} · \theta_{t-\frac{1}{1-\beta}+1}}{\cfrac{1}{1-\beta}}\end{align}</script><p>最终结果就是对前 $\cfrac{1}{1-\beta}$ 天气温平均值的粗略计算。</p><p>如果对于具体原理仍感迷惑也没关系，因为这并不影响实际运用。</p><p>当然，这里是用气温举的例子，以上算法和理论对其他类型的数据也是成立的。</p><h3 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h3><p>在之前的例子中，如果取 $\beta = 0.98$，我们应该得到下图中的绿线：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-065810.png" alt=""></p><p>但实际上我们得到的是下面这条紫线：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-30-100747.png" alt=""></p><p>可以发现，绿线和紫线在后半部分基本贴合，但在开始阶段，紫线要低于绿线，下面我们来解释为什么会出现这种情况以及如何应对。</p><p>回想一下指数加权平均的过程：</p><ul><li>$v_0 = 0$</li><li>$v_1 = 0.98 × v_0 + 0.02 × t(1) = 0.08$</li><li>$v_2 = 0.98 × v_1 + 0.02 × t(2) = 0.2584$</li><li>…</li></ul><p>我们把 $v_0$ 设置为 0，所以 $v_1$ 其实等于 $0.02 × t(1)$，远远小于第一天的真实温度，这就是一个偏差，亦即紫线一开始比较低的原因，我们可以通过偏差修正的方法来解决这个问题。</p><p>所谓偏差修正，就是在计算 $v_t$ 的时候增加一个步骤：</p><script type="math/tex; mode=display">v_t := \frac{v_t}{1 - \beta^t}</script><p>注意观察这个式子，在这个例子中，当 t = 1 时，$1 - \beta^t$ 的值为 0.02，故应用上面这个式子后相当于使 $v_t$ 放大了 50 倍。随着 t 增大，$1 - \beta^t$ 增大，且最后会接近于 1，这时 $v_t$ 基本就不再变化了。</p><p>总结一下，偏差修正就是找一个方法，在开始阶段放大 $v_t$ ，使上图中的紫线在开始阶段与绿线进行拟合。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mini-Batch 梯度下降</title>
      <link href="2019/01/03/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1%20Mini-Batch%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
      <url>2019/01/03/2019/1%20%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/1%20Mini-Batch%20%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</url>
      
        <content type="html"><![CDATA[<blockquote><p>这篇文章简要介绍什么是 mini-batch 梯度下降算法及其特点</p></blockquote><a id="more"></a><p>在训练网络时，如果训练数据非常庞大，那么把所有训练数据都输入一次神经网络需要非常长的时间，另外，这些数据可能根本无法一次性装入内存。人们在实践中发现并证明了另外一种方法，这种方法可以解决这些问题，更快地完成训练。</p><p>假如我们有 m = 500000 对训练数据 X Y，把这些数据按照每组 1000 对分成 500 组，即 X 被分成：</p><pre><code>- X&#123;1&#125; = 0    ...    1000- X&#123;2&#125; = 1001    ...    2000- ...- X&#123;500&#125; = 499001    ...    500000</code></pre><p>Y 的分法和 X 相同，但要保证 X 和 Y 的对应关系不变。</p><p>这就将 m 对训练数据分成了 500 组 <strong>mini batches</strong>，每组都有 1000 对数据，我们可以这样形容第 t 组训练数据数据：\(X{t}, Y{t}\)</p><p>在之前的梯度下降算法中，我们使用整个数据集来运行梯度下降算法，但现在，我们在每组 mini-batch 上运行梯度下降算法，即在这个例子中，我们只需要将 1000 对训练数据，就可以更新一次网络中的权重，而在之前，我们需要全部 500000 对训练数据才能更新一次权重。这种新的梯度下降就叫做 <strong>mini-batch 梯度下降</strong>，而之前的梯度下降其实叫做 <strong>batch 梯度下降</strong>。</p><p>虽然 mini-batch 梯度下降算法可以更快地更新权重，但是损失函数并不会在每次更新权重后都下降，不过总体趋势仍然是下降的，下面是两种梯度下降算法的对比：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-27-140347.png" alt=""></p><blockquote><p>左图是 batch 梯度下降，横轴为迭代次数，纵轴为损失函数值，注意，这里的一次迭代是指所有的训练数据都参与权重的一次更新。</p><p>右图是 mini-batch 梯度下降，横轴为用于更新权重的 mini-batch 数，纵轴为损失函数值，可以明显发现，虽然损失函数不是每次都下降，但整体趋势是下降的。</p></blockquote><p>这里关于 mini-batch size 有几种选择：</p><ul><li>m =&gt; 得到 <strong>batch 梯度下降算法</strong></li><li>1 =&gt; 得到<strong>随机梯度下降算法（SGD）</strong></li><li>1~m 之间 =&gt; <strong>mini-batch 梯度下降算法</strong></li></ul><p>这里说一下随机梯度下降算法，如果每输入一个训练数据后，都更新一次权重，这种梯度下降就叫随机梯度下降，它的好处在于权重的更新非常快，但缺点在于噪声非常大，损失函数变化的方向非常多，虽然最后能到达最优值附近，但不会停下来，而是在最优值附近摆动，看起来是这样的：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-061118.png" alt=""></p><p>图中紫色的线对应随机梯度下降中的损失函数，蓝色的线对应 batch 梯度下降中的损失函数，而 mini-batch 梯度下降的损失函数变化过程介于这二者之间，如下图绿线所示：</p><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2019-01-28-061328.png" alt=""></p><p>mini-batch 梯度下降中的损失函数最后也会在最优值附近摆动，但幅度比 SGD 要小，同时也可以通过学习率衰减来进一步减小摆动幅度。</p><p>下面是一些在选择 mini-batch size 时的经验：</p><ul><li>如果训练集数据较少（比如少于 2000 个样本），使用 batch 梯度下降就可以了，即令 mini-batch size = m</li><li>mini-batch size 最好设置成 2 的整数次幂，比如 32、64、128、256 等</li><li>确保每个 mini batch 都可以放入内存</li></ul><p>mini-batch size 是一个超参数，需要在实际项目中寻找合适的值</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
          <category> Deep Learning </category>
          
          <category> 优化算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>世界电影大数据分析</title>
      <link href="2018/07/07/2018/%E7%AD%94%E8%BE%A9%E6%96%87%E6%A1%A3/"/>
      <url>2018/07/07/2018/%E7%AD%94%E8%BE%A9%E6%96%87%E6%A1%A3/</url>
      
        <content type="html"><![CDATA[<p>学校组织了为期 20 天的实训，老实说，这次实训的收获真比我想象中大很多，这篇文章是我们小组完成的实训项目的报告。</p><a id="more"></a><h1 id="数据源地址"><a href="#数据源地址" class="headerlink" title="数据源地址"></a>数据源地址</h1><p><a href="http://wangjunle23.blog.163.com/blog/static/117838171201411304124368/">http://wangjunle23.blog.163.com/blog/static/117838171201411304124368/</a></p><h1 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h1><p>该数据集（ML20M）描述了来自 MOVELELN 的5星级评级和免费文本标记活动。它包含20000263个评级和465564个标签，横跨27278部电影。这些数据是由138493 个用户在 1995 年和 2015 年 3 月 31 日之间创建的。这个数据集是在 2015 年 3 月 31 日生成的，并在 2016 年 10 月 17 日更新以更新 Link .CSV 。用户随机选择纳入。所有选定的用户至少有 20 部电影，没有人口统计信息包括在内，每个用户由 ID 表示，并且不提供其他信息。</p><h1 id="数据字典"><a href="#数据字典" class="headerlink" title="数据字典"></a>数据字典</h1><div class="table-container"><table><thead><tr><th><strong>编号</strong></th><th><strong>名称</strong></th><th><strong>数据类型</strong></th><th><strong>数据格式</strong></th></tr></thead><tbody><tr><td>1</td><td>标签</td><td>string</td><td>字符串格式</td></tr><tr><td>2</td><td>标签Id</td><td>int</td><td>普通整数格式</td></tr><tr><td>3</td><td>标签相关性</td><td>float</td><td>普通浮点数格式</td></tr><tr><td>4</td><td>电影Id</td><td>int</td><td>普通整数格式</td></tr><tr><td>5</td><td>Imdb网站上的Id号</td><td>int</td><td>普通整数格式</td></tr><tr><td>6</td><td>Themoviedb网站上的Id号</td><td>int</td><td>普通整数格式</td></tr><tr><td>7</td><td>电影名</td><td>string</td><td>字符串格式</td></tr><tr><td>8</td><td>电影流派</td><td>string</td><td>字符串格式</td></tr><tr><td>9</td><td>用户Id</td><td>int</td><td>普通整数格式</td></tr><tr><td>10</td><td>评级分数</td><td>float</td><td>普通浮点数格式</td></tr><tr><td>11</td><td>时间戳</td><td>int</td><td>普通整数格式</td></tr></tbody></table></div><h1 id="项目分析要点"><a href="#项目分析要点" class="headerlink" title="项目分析要点"></a>项目分析要点</h1><h2 id="关于流派"><a href="#关于流派" class="headerlink" title="关于流派"></a>关于流派</h2><h3 id="每种流派电影的数量"><a href="#每种流派电影的数量" class="headerlink" title="每种流派电影的数量"></a>每种流派电影的数量</h3><p>根据文件movies进行处理，分析所有电影流派的电影数，比较哪种流派电影多，哪种电影流派少。</p><h4 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h4><p>主要语句如下：</p><ol><li><p><strong>将预先处理过的数据采集到HDFS中</strong>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put &#x2F;home&#x2F;cloudera&#x2F;Desktop&#x2F;movies.txt &#x2F;data&#x2F;movies &#x2F;&#x2F; 将目标文件拷贝在虚拟机的data目录下</span><br></pre></td></tr></table></figure></li></ol><ol><li><p><strong>创建相应的数据库表，将数据导入数据库表中：</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> movies(</span><br><span class="line">    movieId <span class="type">int</span>,</span><br><span class="line">    title String,</span><br><span class="line">    genres STRING</span><br><span class="line">)</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span><span class="string">&#x27;&#123;&#x27;</span></span><br><span class="line">LINES TERMINATED <span class="keyword">BY</span>&quot;\n&quot;</span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location &quot;/data/movies&quot;;</span><br></pre></td></tr></table></figure></li></ol><h4 id="利用HUE对movies数据库表中的数据进行绘制。"><a href="#利用HUE对movies数据库表中的数据进行绘制。" class="headerlink" title="利用HUE对movies数据库表中的数据进行绘制。"></a>利用HUE对movies数据库表中的数据进行绘制。</h4><ol><li><p><strong>SQL查询语句</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Comedy&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Comedy,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Action&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Action,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Adventure&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Adventure,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Animation&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Animation,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Children&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Children,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Drama&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Drama,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Romance&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Romance,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Crime&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Crime,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Thriller&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Thriller,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Horror&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Horror,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Mystery&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Mystery,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Sci-Fi&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)SciFi,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;IMAX&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)IMAX,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Documentary&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Documentary,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Musical&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Musical,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Fantasy&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)Fantasy,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;War&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)War,</span><br><span class="line">       <span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> genres rlike &quot;Film-Noir&quot; <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">end</span>)FilmNoir <span class="keyword">from</span> movies;</span><br></pre></td></tr></table></figure></li></ol><ol><li><strong>查询结果</strong></li></ol><p><img src="http://oom3nz471.bkt.clouddn.com/2.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/3.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/4.jpg" alt="img"> </p><h4 id="分析结果"><a href="#分析结果" class="headerlink" title="分析结果"></a><strong>分析结果</strong></h4><p>从各流派电影数图来看，电影数最多的是Drama类，电影数最少的是IMAX类。我们猜测，IMAX技术出现比较晚，受年代限制所以这类电影数量比较少，除IMAX类之外最少的是Film-Noir（黑色电影）类。</p><h3 id="各流派得分随时间的变化"><a href="#各流派得分随时间的变化" class="headerlink" title="各流派得分随时间的变化"></a>各流派得分随时间的变化</h3><h4 id="分析思路"><a href="#分析思路" class="headerlink" title="分析思路"></a>分析思路</h4><p>工具：Spark</p><p>这个角度稍微复杂了一些，但主体思路不难，大致为如下流程：</p><ol><li><p>首先，我将 moives.csv 表和 ratings.csv 表用他们共有的元素 movieId 建立起联系，得到如下所示的列表：</p><blockquote><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhvwnt2zyj308b0a8gm4.jpg" alt=""></p></blockquote><p>其中，为了方便，value 中的第一个元素的第二个元素为该电影诞生年份距 1890 年的以 20 年为一个单位的数目，写一个用来解析年份的函数并调用即可。</p></li><li><p>然后，通过另外一个解析函数将每一行的流派拆开并和后面剩下的元素单独组合，形成以“流派 + 年份”为 Key，分数为 Value 的 RDD（丢弃 movieId，之后用不到这个元素了），再通过 ReduceByKey 将相同的 Key 组合，Value 相加，同时统计次数，可形成以下形式的列表：</p><blockquote><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhvwx0uqej304b02owed.jpg" alt=""></p></blockquote></li><li><p>算出每一项的平均分后，将年份单独作为 Key，用 ReduceByKey 将 Key 相同的组合在一起，Value 依次相加，组合成该年代下的“（流派，得分）”列表，即如下所示：</p><blockquote><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fzhvxctgy0j30960873yd.jpg" alt=""></p></blockquote><p>每个年代的流派和得分信息便可得出了，之后作出图表即可进行分析</p></li></ol><h4 id="图表分析"><a href="#图表分析" class="headerlink" title="图表分析"></a>图表分析</h4><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhvy3h7n6j30zk0g4jx9.jpg" alt=""></p><p>结论：上图包含了很多信息，可以从很多角度进行分析。比如，恐怖片的质量看上去越来越差强人意了，犯罪主题的电影似乎也越来越不合人的胃口，但又比恐怖片好一些。纪录片似乎质量一直不错，枪战片的质量在上世纪七八十年代似乎很不错，另外，似乎所有流派的电影质量都有所下滑，这也许是人们已经厌倦了现代的好莱坞式商业电影大行于世导致的。</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> lines1 = sc.textFile(<span class="string">&quot;D:/BigData/input/ratings.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> lines2 = sc.textFile(<span class="string">&quot;D:/BigData/input/movies.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> movieIds_ratings = lines1.map(line =&gt; (line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>), line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">2</span>).toDouble))</span><br><span class="line">    <span class="keyword">val</span> ratings = movieIds_ratings.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> movieIds = lines1.map(line =&gt; (line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> movie_num = movieIds.reduceByKey((v1, v2) =&gt; v1 + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result1 = ratings.join(movie_num).map(x =&gt; (x._1, (x._2._1 / x._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>).toDouble))</span><br><span class="line"></span><br><span class="line">    lines2.map(line =&gt; (line.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">0</span>), line.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">2</span>), line.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">1</span>))).foreach(x =&gt; (parseYear(x._1, x._2, x._3)))</span><br><span class="line">    sc.parallelize(ls_year).join(result1).map(x =&gt; (x._2._1._2, x._2._1._1, x._2._2)).foreach(x =&gt; parse(x._1, x._2, x._3))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> decade_genre_rating = sc.parallelize(ls_decadeGenreRating)</span><br><span class="line">      .map(x =&gt; (x._1, (x._2, <span class="number">1</span>)))</span><br><span class="line">      .reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + <span class="number">1</span>))</span><br><span class="line">      .map(x =&gt; (x._1, (x._2._1 / x._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>)))</span><br><span class="line">      .map(x =&gt; (x._1._1, (x._1._2, x._2)))</span><br><span class="line">      .reduceByKey((x, y) =&gt; (x._1 + <span class="string">&quot; &quot;</span> + x._2 + <span class="string">&quot;|&quot;</span>, y._1 + <span class="string">&quot; &quot;</span> + y._2))</span><br><span class="line">    decade_genre_rating.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> ls_decadeGenreRating = <span class="type">List</span>(((<span class="string">&quot;Messi&quot;</span>, <span class="string">&quot;Barca&quot;</span>), <span class="number">1.0</span>))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse</span></span>(str:<span class="type">String</span>, genres:<span class="type">String</span>, rating:<span class="type">Double</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (a &lt;- genres.split(<span class="string">&quot;\\|&quot;</span>)) &#123;</span><br><span class="line">      <span class="keyword">val</span> t = <span class="type">Tuple2</span>((str, a), rating)</span><br><span class="line">      ls_decadeGenreRating = t :: ls_decadeGenreRating</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> ls_year = <span class="type">List</span>((<span class="string">&quot;Messi&quot;</span>, (<span class="string">&quot;Barca&quot;</span>, <span class="string">&quot;Barca&quot;</span>)))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseYear</span></span>(str1:<span class="type">String</span>, str2:<span class="type">String</span>, str3:<span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str3.split(<span class="string">&quot;`&quot;</span>).length == <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> decade = (<span class="type">Integer</span>.parseInt(str3.split(<span class="string">&quot;`&quot;</span>)(<span class="number">1</span>).substring(<span class="number">0</span>, <span class="number">4</span>)) - <span class="number">1890</span>) / <span class="number">20</span></span><br><span class="line">      <span class="keyword">val</span> t = <span class="type">Tuple2</span>(str1, (str2, decade.toString))</span><br><span class="line">      ls_year = t :: ls_year</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="各流派内电影得分榜top10"><a href="#各流派内电影得分榜top10" class="headerlink" title="各流派内电影得分榜top10"></a>各流派内电影得分榜top10</h3><p>用户每天都在对“看过”的电影进行“很差”到“力荐”的评价，数据源中包含20000263条用户评价，其中最高为5星。我们根据每部影片看过的人对该影片所得的评价，通过hive分析出各个种类电影 Top 10，如果你想看某种类型的电影，可以在我们的排行榜中挑选出我们精选的高分电影！</p><h4 id="分析思路-1"><a href="#分析思路-1" class="headerlink" title="分析思路"></a><strong>分析思路</strong></h4><ol><li><p>数据采集</p><p>此次分析要用到数据源中的两个表movies和ratings。两个首先将预先处理过的数据采集到hdfs的/data/input/rating 和/data/input/movies中，接着创建相应的数据表ratings和movies，并将目标文件的数据导入数据库表。</p></li><li><p>数据处理</p><p>在ratings表中根据电影id分组，算出每个电影用户对它的评分的均分，作为此部电影的评分，并将结果保留进avg_rating表</p><p>在movies表中通过 find_in_set 筛选出指定流派的所有电影，并将查询结果和avg_rating表根据电影id为key进行内联接，联接后的数据为该流派所有电影和对应的分数，接着用order by desc排序，并限制显示前十条，即该流派高分电影top 10。</p></li></ol><h4 id="Hive代码"><a href="#Hive代码" class="headerlink" title="Hive代码"></a>Hive代码</h4><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>  ratings</span><br><span class="line">( userId <span class="type">int</span>,</span><br><span class="line">  movieId <span class="type">int</span>,</span><br><span class="line">  rating <span class="keyword">double</span>, </span><br><span class="line">  <span class="type">time</span> <span class="type">timestamp</span></span><br><span class="line"> ) </span><br><span class="line"> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">  lines Terminated <span class="keyword">by</span> &quot;\n&quot;</span><br><span class="line">  STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">  location &quot;/data/input/rating/&quot;;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>  movies</span><br><span class="line">( movieId <span class="type">int</span>,</span><br><span class="line">  title string, </span><br><span class="line">  genres string</span><br><span class="line"> ) </span><br><span class="line"></span><br><span class="line"> comment &quot;Change in the number of movies&quot; </span><br><span class="line"> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;&#123;&#x27;</span> </span><br><span class="line">  lines Terminated <span class="keyword">by</span> &quot;\n&quot;</span><br><span class="line">  STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">  location &quot;/data/input/movie/&quot;;</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>根据用户评分表算出每部电影评分的均分，结果保留进均分表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> avg_rating</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> movieid,<span class="built_in">avg</span>(rating) <span class="keyword">from</span> ratings <span class="keyword">group</span> <span class="keyword">by</span> movieid;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> avg_rating change _c1 avgrating <span class="type">decimal</span>(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span><span class="keyword">alter</span> <span class="keyword">table</span> avg_rating change <span class="keyword">column</span> avgrating <span class="keyword">at</span> <span class="type">decimal</span>(<span class="number">4</span>,<span class="number">3</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><ol><li><p>在movies表中筛选出指定类型所有电影，将此查询结果与均分表进行内连接，并且对连接后的结果根据分数排序，筛选前十，得出各个流派电影评分top10（下面以Adventure为例）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.avgrating, m.title</span><br><span class="line"><span class="keyword">from</span> avg_rating a</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> movies <span class="keyword">where</span> find_in_set(<span class="string">&#x27;Adventure&#x27;</span>, genres)<span class="operator">!=</span><span class="number">0</span>)m </span><br><span class="line"><span class="keyword">on</span>(a.movieid <span class="operator">=</span> m.movieid) </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> avgrating <span class="keyword">desc</span> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="图表"><a href="#图表" class="headerlink" title="图表"></a>图表</h4><p>以下节选部分类型排行榜top10</p><ol><li><p><strong>冒险片Adventure</strong> <strong>排行榜</strong></p><p> <img src="https://ws4.sinaimg.cn/large/006tNc79gy1fzhw0t2momj30ld09vgm3.jpg" alt=""></p></li><li><p><strong>喜剧片Comedy排行榜</strong></p><p> <img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhw14dnkmj30kd0a2gm2.jpg" alt=""></p></li><li><p><strong>爱情片Romance排行榜</strong></p><p> <img src="https://ws4.sinaimg.cn/large/006tNc79gy1fzhw1wzhguj30l60a074q.jpg" alt=""></p></li><li><p><strong>恐怖片**</strong>Thriller<strong>**排行榜</strong></p><p> <img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhw25koq7j30ma0a0wf2.jpg" alt=""></p></li></ol><h2 id="关于标签"><a href="#关于标签" class="headerlink" title="关于标签"></a>关于标签</h2><h3 id="高分电影中的比较多的标签"><a href="#高分电影中的比较多的标签" class="headerlink" title="高分电影中的比较多的标签"></a>高分电影中的比较多的标签</h3><p>电影的标签可以帮助我们更简单快捷的了解电影，每个电影都有丰富多彩的标签。在数据源中包含465564个标签，标签的相关性体现了它与电影的相关程度。我们想通过分析高分电影中哪些标签比较多，来研究高分电影与标签的联系。</p><h4 id="分析思路-2"><a href="#分析思路-2" class="headerlink" title="分析思路"></a>分析思路</h4><ol><li><p>数据采集</p><p>此次分析要用到数据源中的三个表genome-tas、genome-scores和tags。两个首先将预先处理过的数据采集到hdfs的/data/input/genome-tags 、/data/input/genome-scores和 /data/input/tags中，接着创建相应的数据表genome_tags、genome_scores和tags，并将目标文件的数据导入数据库表。</p></li><li><p>数据处理</p><ol><li><p>由于体现了标签和电影关联性的表中并没有标签id所对应的tag，所以首先将genome_scores表和genome_tags表通过tagid联接成一张表tags_relevance，genome_scores中每个tagid就有其相对应的tag。</p></li><li><p>因为每部电影相关联的标签很多，但并不是所有标签都是与内容关联性很紧密，所以数据处理的主要思路是根据关联程度筛选出每部电影最相关的三个标签。将tag_relevance表按电影名分组，并且在组内按标签关联度大小排序，最终每组取关联度最高的三个标签，得到每个电影关联性最大的三个标签，将此查询结果更新为新的tag_relevance表，其中每个电影只和关联度最大的三个标签对应。</p></li><li>将tag_relevance表和电影均分表avg_rating 通过电影id进行连接，并在联接结果中选出电影得分大于4分的所有电影，再按tagid和tag分组，求出每组tag的数量，按标签数从大到小的顺序排列，取前十。</li></ol></li></ol><h4 id="hive-代码"><a href="#hive-代码" class="headerlink" title="hive 代码"></a>hive 代码</h4><ol><li><p>建表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> genome_tags</span><br><span class="line">( tagId <span class="type">int</span>,</span><br><span class="line">  tag string</span><br><span class="line"> ) </span><br><span class="line"> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">  lines Terminated <span class="keyword">by</span> &quot;\n&quot;</span><br><span class="line">  STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">  location &quot;/data/input/genome-tags/&quot;;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>  tags</span><br><span class="line">( userId <span class="type">int</span>,</span><br><span class="line">  movieId <span class="type">int</span>,</span><br><span class="line">  tag string,</span><br><span class="line">  <span class="type">time</span> <span class="type">timestamp</span></span><br><span class="line"> ) </span><br><span class="line"></span><br><span class="line"> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;&#123;&#x27;</span> </span><br><span class="line">  lines Terminated <span class="keyword">by</span> &quot;\n&quot;</span><br><span class="line">  STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">  location &quot;/data/input/tags/&quot;;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>  genome_scores</span><br><span class="line">( movieid <span class="type">int</span>,</span><br><span class="line">  tagid <span class="type">int</span>,</span><br><span class="line">  relevance <span class="keyword">double</span></span><br><span class="line"> ) </span><br><span class="line"></span><br><span class="line"> <span class="type">ROW</span> FORMAT </span><br><span class="line">  DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">  lines Terminated <span class="keyword">by</span> &quot;\n&quot;</span><br><span class="line">  STORED <span class="keyword">AS</span> TEXTFILE</span><br><span class="line">  location &quot;/data/input/genome-scores/&quot;;</span><br></pre></td></tr></table></figure></li><li><p>关联genome_scores和genome_tags为一张表tags_relevance，为了让genome_scores中每个tagid对应相应tag</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tags_relevance</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span>,b.tag</span><br><span class="line"><span class="keyword">from</span> genome_scores a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> genome_tags b           </span><br><span class="line"><span class="keyword">on</span>(a.tagid <span class="operator">=</span> b.tagid);</span><br></pre></td></tr></table></figure></li></ol><ol><li><p>将tag_relevance表按电影名分组，并且在组内按标签关联度大小排序，最终每组取关联度最高的三个标签，得到每个电影关联性最大的三个标签</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tags_relevance</span><br><span class="line"><span class="keyword">select</span> movieid,tagid,relevance,tag <span class="keyword">from</span>(</span><br><span class="line"> <span class="keyword">select</span> <span class="operator">*</span> ,<span class="built_in">row_number</span>() <span class="keyword">over</span> </span><br><span class="line"> (<span class="keyword">partition</span> <span class="keyword">by</span> movieid <span class="keyword">order</span> <span class="keyword">by</span> relevance <span class="keyword">desc</span>)<span class="keyword">as</span> row_num <span class="keyword">from</span> tags_relevance)<span class="keyword">as</span> tags_relevance</span><br><span class="line"> <span class="keyword">where</span> row_num <span class="operator">&lt;=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure></li><li><p>将3处理后的tag_relevance表和电影均分表avg_rating 连接，筛选出电影分数大于4分的，再按tagid和tag分组，求出每组数量，按标签数量从大到小排名</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> c.tagid,c.tag,<span class="built_in">count</span>(c.tag) <span class="keyword">as</span> tag_num <span class="keyword">from</span></span><br><span class="line">(<span class="keyword">select</span> a.movieid,a.tagid,a.tag,b.avgrating <span class="keyword">from</span> </span><br><span class="line">  tags_relevance a <span class="keyword">join</span> avg_rating b</span><br><span class="line">  <span class="keyword">on</span>(a.movieid <span class="operator">=</span> b.movieid) </span><br><span class="line">  <span class="keyword">where</span> b.avgrating <span class="operator">&gt;</span> <span class="number">4</span>)c</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> c.tagid,c.tag <span class="keyword">order</span> <span class="keyword">by</span> tag_num <span class="keyword">desc</span> limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="图表-1"><a href="#图表-1" class="headerlink" title="图表"></a>图表</h4><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhw3izia8j30cy06wwee.jpg" alt=""></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhw4g1ynkj30oy0e8aa8.jpg" alt=""></p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>根据图表可知，高分电影中标签数量最多的是imdb top 250，说明imdb的电影排行榜还是很靠谱的，其排行榜上的电影也是movielens网站中用户给高分最多的；排名第二的为hitchcock，这是一个知名的导演的名字，他的代表作为<a href="https://baike.baidu.com/item/%E5%90%8E%E7%AA%97">后窗</a>、<a href="https://baike.baidu.com/item/%E6%83%8A%E9%AD%82%E8%AE%B0">惊魂记</a>、<a href="https://baike.baidu.com/item/%E8%A5%BF%E5%8C%97%E5%81%8F%E5%8C%97">西北偏北</a>、<a href="https://baike.baidu.com/item/%E8%9D%B4%E8%9D%B6%E6%A2%A6">蝴蝶梦</a>、<a href="https://baike.baidu.com/item/%E5%88%97%E8%BD%A6%E4%B8%8A%E7%9A%84%E9%99%8C%E7%94%9F%E4%BA%BA">列车上的陌生人</a>等，曾获得过奥斯卡奖终身成就奖，高分电影中以这位导演为标签的数目比较多，说明这位大众对这位导演的电影都有很高的评价；排名第三的为criterion标准，说明大众给标准化的电影打分也是可观的；排名第四的为oscar（best picture）奥斯卡最佳影片，说明奥斯卡评出来的电影是很权威的，也是符合大众口味的；第五名为Screwball comedy疯狂喜剧，之后又接着两个与奥斯卡相关的，看来奥斯卡评出的电影确实是很不错的电影，不论是对专家还是对看电影的用户来说。</p><h3 id="电影得分与标签数量之间的关系"><a href="#电影得分与标签数量之间的关系" class="headerlink" title="电影得分与标签数量之间的关系"></a>电影得分与标签数量之间的关系</h3><p>根据ratings.csv文件提取userId列、movieId列、rating列、stamp列和根据tags.csv文件提取userId列、movieId列、tag列、stamp列进行数据处理，从而进行分析每部电影标签总数与其得分之间的关系；</p><h4 id="数据处理-1"><a href="#数据处理-1" class="headerlink" title="数据处理"></a>数据处理</h4><ol><li><p>将预先处理过的文本文件上传到hdfs里面</p><p>新建文件夹</p><p><img src="http://oom3nz471.bkt.clouddn.com/15.jpg" alt="img"> </p><p>将虚拟机上的文件传到hdfs上面（以tags.txt文件为例）</p><p><img src="http://oom3nz471.bkt.clouddn.com/16.jpg" alt="img"> </p></li><li><p>创建相应的数据库表，上传相应数据</p><p>在虚拟机终端输入ifconfig命令，查看IP</p><p><img src="http://oom3nz471.bkt.clouddn.com/17.jpg" alt="img"> </p><p>在浏览器输入以下IP地址（加上：8888）</p><p><img src="http://oom3nz471.bkt.clouddn.com/18.jpg" alt="img"> </p></li></ol><h4 id="进入HUE界面，开始建表，主要部分代码如下"><a href="#进入HUE界面，开始建表，主要部分代码如下" class="headerlink" title="进入HUE界面，开始建表，主要部分代码如下"></a>进入HUE界面，开始建表，主要部分代码如下</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> tag(</span><br><span class="line">userId <span class="type">int</span>,</span><br><span class="line">movieId <span class="type">int</span>,</span><br><span class="line">tag string,</span><br><span class="line">stamp <span class="type">int</span>)<span class="operator">/</span><span class="operator">/</span>输入数据类型</span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span><span class="string">&#x27;,&#x27;</span></span><br><span class="line">LINES TERMINATED <span class="keyword">BY</span>&quot;\n&quot;</span><br><span class="line">stored <span class="keyword">as</span> textfile</span><br><span class="line">location &quot;/tag&quot;;<span class="operator">/</span><span class="operator">/</span>导入数据到数据库</span><br></pre></td></tr></table></figure><p>使用查询语句查询相应的信息: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(tag),<span class="built_in">avg</span>(rating)<span class="keyword">from</span> tag <span class="keyword">join</span> rating <span class="keyword">where</span> rating.movieId <span class="operator">=</span> tag.movieId <span class="keyword">group</span> <span class="keyword">by</span> rating.movieId;   <span class="operator">/</span><span class="operator">/</span>求每个电影标签总数和平均得分</span><br></pre></td></tr></table></figure><h4 id="利用HUE工具对新建数据库中的数据进行结果的绘制"><a href="#利用HUE工具对新建数据库中的数据进行结果的绘制" class="headerlink" title="利用HUE工具对新建数据库中的数据进行结果的绘制"></a>利用HUE工具对新建数据库中的数据进行结果的绘制</h4><ol><li><p>电影编号-标签数量-电影平均分: </p><p><img src="http://oom3nz471.bkt.clouddn.com/19.jpg" alt="img"> </p></li><li><p>电影得分标签数量分布图</p><p><img src="http://oom3nz471.bkt.clouddn.com/20.jpg" alt="img"></p></li><li><p>图表4.2.2-3 电影得分与标签数量关系柱状图</p><p><img src="http://oom3nz471.bkt.clouddn.com/21.jpg" alt="img"> </p></li></ol><h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>由上面的图表可以看出标签数量较多的集中在大致在电影得分在3.5-4.0范围之间，而标签数量大致随电影得分的增高而增长，在4.0到5.0之间这个趋势有点减弱，这个分数阶段的标签数量相对与3.5-4.0之间要少许多；就此分析，我们可以推论得分3.5-4.0的电影可能更被人熟知，被更多的人评论，从而贴上更多的标签，剧情各方面的综合性比较强；而可能得分0-4.0的电影不怎么符合大众的口味，或者剧情各方面经不起推敲，接近烂片的标准；至于得分4.0-5.0的电影属于被人遗忘的佳作的类型，亦或是佳作根本无需贴标签，只需要静静欣赏的类型…</p><h2 id="关于年代"><a href="#关于年代" class="headerlink" title="关于年代"></a>关于年代</h2><h3 id="电影数量与年代变化之间的关系"><a href="#电影数量与年代变化之间的关系" class="headerlink" title="电影数量与年代变化之间的关系"></a>电影数量与年代变化之间的关系</h3><p>电影数量随着年份将有怎样的变化？我们将对从1890年开始的27278部电影进行分析，统计出从1890年开始每20年电影数量的变化。</p><h4 id="分析思路-3"><a href="#分析思路-3" class="headerlink" title="分析思路"></a>分析思路</h4><p>由于数据源中没有单独的电影年份字段，所以要在电影名中提取出电影年份，为了便于mapreduce中切割，首先对数据源的movies表进行预处理，单独提取出movies表的title（电影名），并将电影名字段中的年份前面用专门的符号替换</p><p>数据源处理完后，开始编写mapreduce程序，map处理文件的每一行，其逻辑为判断分割出来的年份属于哪个年份区间，并将年份区间作为key，value值为1，在reduce中将分组后的（key，value）中的value值进行累加，算出每个年份区间中电影的数目。</p><h4 id="MapReduce代码"><a href="#MapReduce代码" class="headerlink" title="MapReduce代码"></a>MapReduce代码</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> film_numchange_years;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.IntWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">NumChangeTwoDecades</span> </span>&#123;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key,Text value,Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">String[] strs = value.toString().split(<span class="string">&quot;\&#123;&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span>(strs.length == <span class="number">2</span>)&#123;</span><br><span class="line"><span class="keyword">int</span> year = Integer.parseInt(strs[<span class="number">1</span>].trim());</span><br><span class="line">            Text keyY = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> ( year&gt;=<span class="number">1891</span> &amp;&amp; year&lt;=<span class="number">1911</span>)</span><br><span class="line">&#123;</span><br><span class="line">keyY.set(<span class="string">&quot;1891-1911&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (year&gt;=<span class="number">1912</span> &amp;&amp; year&lt;=<span class="number">1932</span>)</span><br><span class="line">&#123;</span><br><span class="line">keyY.set(<span class="string">&quot;1912-1932&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (year&gt;=<span class="number">1933</span> &amp;&amp; year&lt;=<span class="number">1953</span>)</span><br><span class="line">&#123;</span><br><span class="line">keyY.set(<span class="string">&quot;1933-1953&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (year&gt;=<span class="number">1954</span> &amp;&amp; year&lt;=<span class="number">1974</span>)</span><br><span class="line">&#123;</span><br><span class="line">keyY.set(<span class="string">&quot;1954-1974&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (year&gt;=<span class="number">1975</span> &amp;&amp; year&lt;=<span class="number">1995</span>)</span><br><span class="line">&#123;</span><br><span class="line">keyY.set(<span class="string">&quot;1975-1995&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> </span><br><span class="line">&#123;</span><br><span class="line">keyY.set(<span class="string">&quot;1996-2017&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line">context.write(keyY,<span class="keyword">new</span> IntWritable(<span class="number">1</span>));</span><br><span class="line"> &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">FReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">IntWritable</span>,<span class="title">Text</span>,<span class="title">IntWritable</span>&gt;</span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key,Iterable&lt;IntWritable&gt; value,Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">    <span class="keyword">int</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(IntWritable num:value)&#123;</span><br><span class="line">sum=sum+num.get();</span><br><span class="line">&#125;</span><br><span class="line">context.write(key, <span class="keyword">new</span> IntWritable(sum));</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(args.length&lt;<span class="number">2</span>)&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Usage:&lt;inputPath&gt;&lt;outputPath&gt;&quot;</span>);</span><br><span class="line">System.exit(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(args[<span class="number">0</span>]);</span><br><span class="line">Path OutputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"><span class="keyword">if</span>(fs.exists(OutputPath))&#123;</span><br><span class="line">fs.delete(OutputPath,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJobName(<span class="string">&quot;NumChangeTwoDecades&quot;</span>);</span><br><span class="line">job.setJarByClass(NumChangeTwoDecades.class);</span><br><span class="line">FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">FileOutputFormat.setOutputPath(job, OutputPath);</span><br><span class="line">job.setMapperClass(FMapper.class);</span><br><span class="line">job.setReducerClass(FReducer.class);</span><br><span class="line"><span class="comment">//job.setCombinerClass(WReducer.class);</span></span><br><span class="line">job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(IntWritable.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(IntWritable.class);</span><br><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="图表与结论"><a href="#图表与结论" class="headerlink" title="图表与结论"></a>图表与结论</h4><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhvz8s4rtj30a1061t8j.jpg" alt=""></p><p>从时间来看，如图所示，从1891年开始到2017年，上映的电影数量整体趋势上涨。在18世纪末，电影行业应该才刚刚起步，电影数量非常少，而到了十九世纪，电影逐渐增加，特别是到了19世纪末，电影数量开始飞速增长，到了20世纪，十几年内电影数量已经达到了14000万以上。</p><h3 id="各流派电影数量随年代的变化趋势"><a href="#各流派电影数量随年代的变化趋势" class="headerlink" title="各流派电影数量随年代的变化趋势"></a>各流派电影数量随年代的变化趋势</h3><h4 id="分析思路-4"><a href="#分析思路-4" class="headerlink" title="分析思路"></a>分析思路</h4><p>作为电影导演和制片人需要了解当今电影市场的需求，为了能够准确判断市场方向，需要了解哪种类型的电影市场需求大，更受观众喜爱，并且需要预测市场今后的走向。所以我们分析了电影产业起始以来，不同类型的电影随时间的变化趋势。</p><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><ol><li><p>首先需要对数据源做预处理操作，去除干扰字符，替换切分字符，易于切分操作。</p></li><li><p>然后对movie表进行操作，提取其中title列和genres列。由于每一个电影有多个genres所以将genres切割并且每一个都作为key值。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">String[] strs = value.toString().split(<span class="string">&quot;\&#123;&quot;</span>);<span class="comment">//切分行数据</span></span><br><span class="line">String Type=strs[<span class="number">2</span>];</span><br><span class="line">String[] Types=Type.split(<span class="string">&quot;\|&quot;</span>);<span class="comment">//切分不同类型</span></span><br><span class="line">keyWord = <span class="keyword">new</span> Text(Types[i]);<span class="comment">//作为key值</span></span><br></pre></td></tr></table></figure></li><li><p>再对title中的年份进行提取：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">String keyValue1=strs[<span class="number">1</span>];</span><br><span class="line">String[] title=keyValue1.toString().split(<span class="string">&quot;\`&quot;</span>);</span><br></pre></td></tr></table></figure></li><li><p>由于有的title列被多切分出了几个数，导致程序无法正常运行，此时需要加入判断语句，去除问题数据行。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (title.length==<span class="number">2</span>)&#123;</span><br><span class="line">String keyValue2=title[<span class="number">1</span>].substring(<span class="number">0</span>, <span class="number">4</span>);<span class="comment">//提取出四个数字，即年份</span></span><br><span class="line">     ……</span><br><span class="line">keyValue=<span class="keyword">new</span> Text(keyValue2);</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;Types.length;i++)</span><br><span class="line">&#123;</span><br><span class="line">……</span><br><span class="line">context.write(keyWord,keyValue);<span class="comment">//类型作为key，年份作为value输出</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>reduce中计算每个类型不同年份的数量</li></ol><p><strong>完整代码如下：</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> movie;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OneMivieTrend</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TrendMap</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">Text keyWord;</span><br><span class="line">Text keyValue;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key,Text value,Context context)</span><span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">String[] strs = value.toString().split(<span class="string">&quot;\\&#123;&quot;</span>);   </span><br><span class="line">String Type=strs[<span class="number">2</span>];</span><br><span class="line">String[] Types=Type.split(<span class="string">&quot;\\|&quot;</span>);</span><br><span class="line">String keyValue1=strs[<span class="number">1</span>];</span><br><span class="line">String[] title=keyValue1.toString().split(<span class="string">&quot;\\`&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (title.length==<span class="number">2</span>)&#123;</span><br><span class="line">String keyValue2=title[<span class="number">1</span>].substring(<span class="number">0</span>, <span class="number">4</span>);</span><br><span class="line">keyValue=<span class="keyword">new</span> Text(keyValue2);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;Types.length;i++)</span><br><span class="line">&#123;</span><br><span class="line">keyWord = <span class="keyword">new</span> Text(Types[i]);</span><br><span class="line">context.write(keyWord,keyValue);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TrendReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key,Iterable&lt;Text&gt; values,Context context)</span> </span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">      String strfirst=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strsec=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strth=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strfour=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strfif=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strsix=<span class="string">&quot;&quot;</span>;</span><br><span class="line"><span class="keyword">int</span> first=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> second=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> third=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> fourth=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> fifth=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> sixth=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(Text val:values)&#123;</span><br><span class="line">    String vall=val.toString();</span><br><span class="line">    <span class="keyword">int</span> year=Integer.parseInt(vall);</span><br><span class="line">    <span class="keyword">if</span>(year&gt;<span class="number">1896</span>&amp;&amp;year&lt;<span class="number">1916</span>)</span><br><span class="line">    &#123;</span><br><span class="line">    first=first+<span class="number">1</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1916</span>&amp;&amp;year&lt;<span class="number">1936</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      second=second+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1936</span>&amp;&amp;year&lt;<span class="number">1956</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  third=third+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1956</span>&amp;&amp;year&lt;<span class="number">1976</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  fourth=fourth+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1979</span>&amp;&amp;year&lt;<span class="number">1996</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  fifth=fifth+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">  sixth=sixth+<span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  strfirst=String.valueOf(first);</span><br><span class="line">  strsec=String.valueOf(second);</span><br><span class="line">  strth=String.valueOf(third);</span><br><span class="line">      strfour=String.valueOf(fourth);</span><br><span class="line">  strfif=String.valueOf(fifth);</span><br><span class="line">  strsix=String.valueOf(sixth);</span><br><span class="line">    &#125;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">//sb.deleteCharAt(sb.length() - 1);</span></span><br><span class="line">    <span class="comment">//Text value = new Text(sb.toString());</span></span><br><span class="line">    context.write(key,<span class="keyword">new</span> Text(<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1896-1916&quot;</span> +<span class="string">&quot;:&quot;</span>+strfirst+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1916-1936&quot;</span>+<span class="string">&quot;:&quot;</span>+strsec+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1936-1956&quot;</span>+<span class="string">&quot;:&quot;</span>+strth+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1956-1976&quot;</span>+<span class="string">&quot;:&quot;</span>+strfour+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1976-1996&quot;</span>+<span class="string">&quot;:&quot;</span>+strfif+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1996-&quot;</span>+<span class="string">&quot;:&quot;</span>+strsix+<span class="string">&quot;\n&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">   </span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(args.length&lt;<span class="number">2</span>)&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Usage:&lt;inputPath&gt;&lt;outputPath&gt;&quot;</span>);</span><br><span class="line">System.exit(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(args[<span class="number">0</span>]);</span><br><span class="line">Path OutputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"><span class="keyword">if</span>(fs.exists(OutputPath))&#123;</span><br><span class="line">fs.delete(OutputPath,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJobName(<span class="string">&quot;movie&quot;</span>);</span><br><span class="line">job.setJarByClass(OneMivieTrend.class);</span><br><span class="line"></span><br><span class="line">FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">FileOutputFormat.setOutputPath(job, OutputPath);</span><br><span class="line">job.setMapperClass(TrendMap.class);</span><br><span class="line">job.setReducerClass(TrendReduce.class);</span><br><span class="line"><span class="comment">//job.setCombinerClass(WReducer.class);</span></span><br><span class="line">job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="图表-2"><a href="#图表-2" class="headerlink" title="图表"></a>图表</h4><p><img src="http://oom3nz471.bkt.clouddn.com/23.jpg" alt="img">  </p><p><img src="http://oom3nz471.bkt.clouddn.com/24.jpg" alt="img"> </p><h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><p>由图1可看出在电影史上drama类型的电影数量一直处于上升趋势，很多电影喜欢加入drama的因素在其中，也说明市场一直需要drama类型的电影。但是由图2知在2011年之后drama类型的电影出现了陡然下降的趋势，其原因可能是drama类型的电影的关注开始降低，也可能是由于整体电影市场中电影数量的减少导致。</p><p>虽然在整个电影史上电影的总数量呈极速增长的趋势，但是在2011年之后几乎所有类型的电影的数量都开始减少。经分析，其原因可能是：1、电影的类型开始不多元化（如果一个电影有多个类型标签，则通过此统计算出的此类型的电影数量会变多）2、电影拍摄数量确实开始减少，可能是因为电影拍摄成本升高的原因3、电影市场需求量减少。总而言之，电影圈的竞争越来越激烈。</p><p>从图2我们还可以看出只有IMAX类型的电影处于增长趋势，这说明今后此技术的电影需求量在上升。</p><h3 id="不同年代的电影数量最多的流派"><a href="#不同年代的电影数量最多的流派" class="headerlink" title="不同年代的电影数量最多的流派"></a>不同年代的电影数量最多的流派</h3><h4 id="分析思路-5"><a href="#分析思路-5" class="headerlink" title="分析思路"></a>分析思路</h4><p>不同年代电影观众的口味可能会不同，所以我们研究了不同年代哪些电影数量最多的电影类型。通过研究这个，可以判断每个年代的社会风气、群众关注焦点以及观众品味，有利于对电影史的研究以及对人类社会的研究。</p><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><p>使用mapreduce</p><p>其中map类容与上述一致</p><p> Reduce：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(year&gt;<span class="number">1960</span>&amp;&amp;year&lt;<span class="number">1970</span>)</span><br><span class="line">&#123;</span><br><span class="line">first=first+<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1970</span>&amp;&amp;year&lt;<span class="number">1980</span>)</span><br><span class="line">&#123;</span><br><span class="line">second=second+<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1980</span>&amp;&amp;year&lt;<span class="number">1990</span>)</span><br><span class="line">&#123;</span><br><span class="line">third=third+<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1990</span>&amp;&amp;year&lt;<span class="number">2000</span>)</span><br><span class="line">&#123;</span><br><span class="line">fourth=fourth+<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">2000</span>&amp;&amp;year&lt;<span class="number">2010</span>)</span><br><span class="line">&#123;</span><br><span class="line">fifth=fifth+<span class="number">1</span>;</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">2010</span>)&#123;</span><br><span class="line">sixth=sixth+<span class="number">1</span>;</span><br><span class="line">&#125;<span class="comment">// 对不同年代电影数量进行统计</span></span><br></pre></td></tr></table></figure><p><strong>完整代码如下</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> movie;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MovieMountCompare</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TrendMap</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">Text keyWord;</span><br><span class="line">Text keyValue;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key,Text value,Context context)</span><span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">String[] strs = value.toString().split(<span class="string">&quot;\\&#123;&quot;</span>);</span><br><span class="line">String Type=strs[<span class="number">2</span>];</span><br><span class="line">String[] Types=Type.split(<span class="string">&quot;\\|&quot;</span>);</span><br><span class="line">String keyValue1=strs[<span class="number">1</span>];</span><br><span class="line">String[] title=keyValue1.toString().split(<span class="string">&quot;\\`&quot;</span>);</span><br><span class="line"><span class="keyword">if</span> (title.length==<span class="number">2</span>)&#123;</span><br><span class="line">String keyValue2=title[<span class="number">1</span>].substring(<span class="number">0</span>, <span class="number">4</span>);</span><br><span class="line">keyValue=<span class="keyword">new</span> Text(keyValue2);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;Types.length;i++)</span><br><span class="line">&#123;</span><br><span class="line">keyWord = <span class="keyword">new</span> Text(Types[i]);</span><br><span class="line">context.write(keyWord,keyValue);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">TrendReduce</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt;</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key,Iterable&lt;Text&gt; values,Context context)</span> </span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException,InterruptedException</span>&#123;</span><br><span class="line">      String strfirst=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strsec=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strth=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strfour=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strfif=<span class="string">&quot;&quot;</span>;</span><br><span class="line">String strsix=<span class="string">&quot;&quot;</span>;</span><br><span class="line"><span class="keyword">int</span> first=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> second=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> third=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> fourth=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> fifth=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> sixth=<span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(Text val:values)&#123;</span><br><span class="line">  String vall=val.toString();</span><br><span class="line">  <span class="keyword">int</span> year=Integer.parseInt(vall);</span><br><span class="line">  <span class="keyword">if</span>(year&gt;<span class="number">1960</span>&amp;&amp;year&lt;<span class="number">1970</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  first=first+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1970</span>&amp;&amp;year&lt;<span class="number">1980</span>)</span><br><span class="line">  &#123;</span><br><span class="line">      second=second+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1980</span>&amp;&amp;year&lt;<span class="number">1990</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  third=third+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">1990</span>&amp;&amp;year&lt;<span class="number">2000</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  fourth=fourth+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">2000</span>&amp;&amp;year&lt;<span class="number">2010</span>)</span><br><span class="line">  &#123;</span><br><span class="line">  fifth=fifth+<span class="number">1</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> <span class="keyword">if</span>(year&gt;=<span class="number">2010</span>)&#123;</span><br><span class="line">  sixth=sixth+<span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">strfirst=String.valueOf(first);</span><br><span class="line">strsec=String.valueOf(second);</span><br><span class="line">strth=String.valueOf(third);</span><br><span class="line">    strfour=String.valueOf(fourth);</span><br><span class="line">strfif=String.valueOf(fifth);</span><br><span class="line">strsix=String.valueOf(sixth);</span><br><span class="line">  &#125;</span><br><span class="line">    <span class="comment">//sb.deleteCharAt(sb.length() - 1);</span></span><br><span class="line">    <span class="comment">//Text value = new Text(sb.toString());</span></span><br><span class="line">    context.write(key,<span class="keyword">new</span> Text(<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1960-1970&quot;</span> +<span class="string">&quot;:&quot;</span>+strfirst+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1970-1980&quot;</span>+strsec+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1980-1990&quot;</span>+strth+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;1990-2000&quot;</span>+strfour+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;2000-2010&quot;</span>+strfif+<span class="string">&quot;\n&quot;</span>+<span class="string">&quot;2010-2017&quot;</span>+strsix+<span class="string">&quot;\n&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span>  <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"><span class="keyword">if</span>(args.length&lt;<span class="number">2</span>)&#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Usage:&lt;inputPath&gt;&lt;outputPath&gt;&quot;</span>);</span><br><span class="line">System.exit(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(args[<span class="number">0</span>]);</span><br><span class="line">Path OutputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"><span class="keyword">if</span>(fs.exists(OutputPath))&#123;</span><br><span class="line">fs.delete(OutputPath,<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJobName(<span class="string">&quot;MovieMountCompare&quot;</span>);</span><br><span class="line">job.setJarByClass(MovieMountCompare.class);</span><br><span class="line">FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">FileOutputFormat.setOutputPath(job, OutputPath);</span><br><span class="line">job.setMapperClass(TrendMap.class);</span><br><span class="line">job.setReducerClass(TrendReduce.class);</span><br><span class="line"><span class="comment">//job.setCombinerClass(WReducer.class);</span></span><br><span class="line">job.setOutputFormatClass(SequenceFileOutputFormat.class);</span><br><span class="line">job.setMapOutputKeyClass(Text.class);</span><br><span class="line">job.setMapOutputValueClass(Text.class);</span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">job.setOutputValueClass(Text.class);</span><br><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="图表-3"><a href="#图表-3" class="headerlink" title="图表"></a>图表</h4><p><img src="http://oom3nz471.bkt.clouddn.com/25.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/26.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/27.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/28.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/29.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/30.jpg" alt="img"> </p><h4 id="结论-3"><a href="#结论-3" class="headerlink" title="结论"></a>结论</h4><p>60、70、80年代Drama、conmedy和action的电影类型数量最多，90年代之后thriller类型替换了action进入了前三。</p><p>戏剧与喜剧一直高居榜首可见这两个类型是大多数电影的主要元素，市场欢迎程度最高，这也说明了大多数人看电影主要是为了娱乐和放松，也揭示了电影最主要的功能是娱乐大众。</p><p>在90年代以后，恐怖片的数量超过动作片，可以说明恐怖片的市场在增长，可能是因为人们开始更多寻求心理上的刺激。</p><h3 id="上世纪与本世纪比较受欢迎的电影类别"><a href="#上世纪与本世纪比较受欢迎的电影类别" class="headerlink" title="上世纪与本世纪比较受欢迎的电影类别"></a>上世纪与本世纪比较受欢迎的电影类别</h3><h4 id="分析思路-6"><a href="#分析思路-6" class="headerlink" title="分析思路"></a>分析思路</h4><p>根据表“movies.csv”，可以分析出所有电影类别下各有哪些电影，以及这些电影各自诞生的年份，将年份与 2000 比较，即可得到上世纪的电影与本世纪的电影，亦即得到以下列表：</p><blockquote><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhvpq8qqtj306h0i5dgb.jpg" alt="上世纪电影的部分列表"></p></blockquote><p>其中，第一个元素为电影 ID，第二各元素为该电影所属的类别。</p><p>然后通过表“ratings.csv”，可以得到所有电影的平均分，即下表：</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhvqh8pm1j302505ugli.jpg" alt=""></p><p>其中，第一个元素为电影 ID，第二个元素为该电影的平均分。</p><p>将第一步得到的 RDD 与第二步得到的 RDD 进行 Join 操作即可分别得到上世纪和本世纪的一个详细列表，如下所示：</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fzhvqv17z5j306b07gmx9.jpg" alt=""></p><p>这里我们已经不再需要电影 ID 这个元素了，因为我们的目的是提取出所有电影类别及其对应的平均分即可，具体操作为用“|”分割出所有的电影类别，再将同一个电影类别的总分除以该类别出现在不同电影中的次数即可得到该类别的平均分，这里的麻烦之处在于，如何将一个 RDD 中每一行的第一个元素用“|“分割后产生的每个元素与该行的第二个元素即分数相组合成一个新的元组，这里我写了一个专门用了处理这个过程的函数，这个函数会先将第一个元素用”|“分割，再将每个元素与分数组合为一个元组，再将这个元组加入一个事先定义好的列表中，当这个 RDD 的所有行都处理完毕后，用这个列表生成一个新的 RDD，这个新的 RDD 就包含了每个电影类别以及分数，不过这时会有重复的电影类别，之后再用 ReduceByKey 处理一下即可得到各个电影类别的平均分，最终结果如下所示：</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhvr34x14j309107ijre.jpg" alt=""></p><p>左边为上世纪的结果，右边为本世纪的结果</p><h4 id="图表分析与结论"><a href="#图表分析与结论" class="headerlink" title="图表分析与结论"></a>图表分析与结论</h4><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhvs724vhj30di065t8l.jpg" alt=""></p><p>通过对比分析不难得出，恐怖片无论在上世纪还是本世纪都是得分最低的，这可能跟恐怖片成本不高，导致滥竽充数的作品很多有关，而上个世纪中得分最高的电影类别为”没有类别“，这看上去有些不合情理，但仔细一想，上个世纪的跨度为 100 年，早期的电影也许无法准确定性为何种流派的电影，而这些作品又确实吸引了一大批观众，将这些作品带上了首席宝座，而这个世纪平均分最高的类别为 Film-Noir，网上给出的翻译为<strong>黑色电影</strong>，我对这方面没有什么研究，感兴趣的同学可以自己分析下这其中的原因。</p><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> lines1 = sc.textFile(<span class="string">&quot;D:/BigData/input/ratings.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> lines2 = sc.textFile(<span class="string">&quot;D:/BigData/input/movies.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> movieIds_ratings = lines1.map(line =&gt; (line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>), line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">2</span>).toDouble))</span><br><span class="line">    <span class="keyword">val</span> ratings = movieIds_ratings.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> movieIds = lines1.map(line =&gt; (line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>), <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> movie_num = movieIds.reduceByKey((v1, v2) =&gt; v1 + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result1 = ratings.join(movie_num)   <span class="comment">// 存储每部电影的平均分(需要计算一下)</span></span><br><span class="line"></span><br><span class="line">    lines2.map(line =&gt; (line.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">0</span>), line.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">2</span>), line.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">1</span>))).foreach(x =&gt; (parseYear(x._1, x._2, x._3)))</span><br><span class="line">    sc.parallelize(ls_lastCentury).join(result1).map(x =&gt; (x._2._1, (x._2._2._1 / x._2._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>))).foreach(x =&gt; parse(x._1, x._2, <span class="number">0</span>))</span><br><span class="line">    sc.parallelize(ls_thisCentury).join(result1).map(x =&gt; (x._2._1, (x._2._2._1 / x._2._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>))).foreach(x =&gt; parse(x._1, x._2, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd_lastCentury = sc.parallelize(resultLs_lastCentury).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 +<span class="number">1</span>)).map(x =&gt; (x._1, (x._2._1 / x._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>)))</span><br><span class="line">    <span class="keyword">val</span> rdd_thisCentury = sc.parallelize(resultLs_thisCentury).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 +<span class="number">1</span>)).map(x =&gt; (x._1, (x._2._1 / x._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>)))</span><br><span class="line"></span><br><span class="line">    rdd_thisCentury.collect.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> resultLs_lastCentury = <span class="type">List</span>((<span class="string">&quot;Test&quot;</span>, (<span class="number">0.0</span>, <span class="number">1</span>)))</span><br><span class="line">  <span class="keyword">var</span> resultLs_thisCentury = <span class="type">List</span>((<span class="string">&quot;Test&quot;</span>, (<span class="number">0.0</span>, <span class="number">1</span>)))</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse</span></span>(str:<span class="type">String</span>, rating:<span class="type">String</span>, flag:<span class="type">Int</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (a &lt;- str.split(<span class="string">&quot;\\|&quot;</span>)) &#123;</span><br><span class="line">      <span class="keyword">if</span> (flag == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> t = <span class="type">Tuple2</span>(a, (rating.toDouble, <span class="number">1</span>))</span><br><span class="line">        resultLs_lastCentury = t :: resultLs_lastCentury</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> t = <span class="type">Tuple2</span>(a, (rating.toDouble, <span class="number">1</span>))</span><br><span class="line">        resultLs_thisCentury = t :: resultLs_thisCentury</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> ls_lastCentury = <span class="type">List</span>((<span class="string">&quot;Test&quot;</span>, <span class="string">&quot;Test&quot;</span>))</span><br><span class="line">  <span class="keyword">var</span> ls_thisCentury = <span class="type">List</span>((<span class="string">&quot;Test&quot;</span>, <span class="string">&quot;Test&quot;</span>))</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parseYear</span></span>(str1:<span class="type">String</span>, str2:<span class="type">String</span>, str3:<span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (str3.split(<span class="string">&quot;`&quot;</span>).length == <span class="number">2</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">Integer</span>.parseInt(str3.split(<span class="string">&quot;`&quot;</span>)(<span class="number">1</span>).substring(<span class="number">0</span>, <span class="number">4</span>)) &lt; <span class="number">2000</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> t = <span class="type">Tuple2</span>(str1, str2)</span><br><span class="line">        ls_lastCentury = t :: ls_lastCentury</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> t = <span class="type">Tuple2</span>(str1, str2)</span><br><span class="line">        ls_thisCentury = t :: ls_thisCentury</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="各分数段的电影数量与年代的关系"><a href="#各分数段的电影数量与年代的关系" class="headerlink" title="各分数段的电影数量与年代的关系"></a>各分数段的电影数量与年代的关系</h3><p>数据源中的电影评分最低为 0，最高为 5，因此我将其从 0~0.5 开始分为了 10 各分数段，分析了各分数段的电影数量与年代的关系，另外，由于数据源中最早的电影为 18 世纪 90 年代，为了方便，我将 20 年定义为一个年代，因此至今共有 7 个年代。</p><h4 id="分析思路-7"><a href="#分析思路-7" class="headerlink" title="分析思路"></a>分析思路</h4><p>工具：MapReduce。</p><p>通过表“ratings.csv”，可以分析出所有电影的平均分，并以第一个元素为电影 ID，第二个元素为平均分组成一个列表，写入文件 movieQuality1.txt 中，如下所示：</p><blockquote><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhw6q05o3j304c08djrc.jpg" alt=""></p></blockquote><p>根据事先定义好的分数段，可以将目前算出的所有平均分都放入一个分数段中，再统计出每个分数段中各有哪些电影，如图所示：</p><blockquote><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhw6zaxyrj30cr0610u0.jpg" alt=""></p></blockquote><p>由于电影的数量很多，因此每个分数段后面都有很多的电影 ID。</p><p>目前只统计出了各分数段下的电影，但还不知道这些电影都属于什么年代，因此这里需要用到另一张表“movies.csv”，再新建一个 MapReduce，读入这张表以及上一步生成的文件，扫描每个分数段中的电影 ID，定位到表“movies.csv“相对应的电影 ID，再获取相应的电影名，从名字中提取出年份信息，再将其定位到相应年代，即可分析出各分数段中每个年代的电影数量，最终可以得到下面所示的列表：</p><blockquote><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhw7dv3vwj305l03l747.jpg" alt=""></p></blockquote><p>其中，第一列为分数段，方便起见，我将分数段用 0.1 到 1 代替，后面的每一列都依次代表了一个年代的电影数量。</p><h4 id="图表分析与结论-1"><a href="#图表分析与结论-1" class="headerlink" title="图表分析与结论"></a>图表分析与结论</h4><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fzhw7tuz94j30wc0he0w2.jpg" alt=""></p><p>不难看出，1990~2010 年间，世界上涌现出了很多优秀的电影作品，无论从电影数量还是质量来说，1990~2010 都是电影史上的一个黄金时期。另外，由于 2010 年至今只有 8 年，因此电影数量看起来比较低。</p><h4 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h4><ol><li><p>MovieQuality1</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> movie_program;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">movieQuality1</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text ktext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line">Text vtext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">String[] strs = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">String movieId = strs[<span class="number">1</span>];</span><br><span class="line">String rating = strs[<span class="number">2</span>];</span><br><span class="line">ktext.set(movieId);</span><br><span class="line">vtext.set(rating);</span><br><span class="line">context.write(ktext, vtext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text vtext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key,Iterable&lt;Text&gt; values,Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">double</span> sum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> num = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span>(Text val:values) &#123;</span><br><span class="line">sum += Double.parseDouble(val.toString());</span><br><span class="line">num++;</span><br><span class="line">&#125;</span><br><span class="line">vtext.set(String.valueOf(sum / num));</span><br><span class="line">context.write(key, vtext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * input: /data/movie_program/rating/ratings.txt</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Usage:&lt;inputPath&gt;&lt;outputPath&gt;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(args[<span class="number">0</span>]);</span><br><span class="line">Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line"><span class="keyword">if</span> (fs.exists(outputPath)) &#123;</span><br><span class="line">fs.delete(outputPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJobName(<span class="string">&quot;movieQuality1&quot;</span>);</span><br><span class="line">job.setJarByClass(movieQuality1.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(RMapper.class);</span><br><span class="line">job.setReducerClass(RReducer.class);</span><br><span class="line">        </span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>MovieQuality2</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> movie_program;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.text.DecimalFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">movieQuality2</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text ktext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line">Text vtext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line"><span class="keyword">double</span> rank = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">String[] strs = value.toString().split(<span class="string">&quot;&quot;</span>);</span><br><span class="line">String movieId = strs[<span class="number">0</span>];</span><br><span class="line">String rating = strs[<span class="number">1</span>];</span><br><span class="line">rank = Double.parseDouble(rating) / <span class="number">5</span>;</span><br><span class="line"><span class="keyword">if</span>(Double.parseDouble(rating) / <span class="number">5</span> == <span class="number">0</span>) &#123;</span><br><span class="line">rank -= <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">DecimalFormat df = <span class="keyword">new</span> DecimalFormat(<span class="string">&quot;#.0&quot;</span>);</span><br><span class="line">rank = Double.parseDouble(df.format(rank));</span><br><span class="line">ktext.set(String.valueOf(rank));</span><br><span class="line">vtext.set(movieId);</span><br><span class="line">context.write(ktext, vtext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>,<span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Text ktext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line">Text vtext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key,Iterable&lt;Text&gt; values,Context context)</span> <span class="keyword">throws</span> IOException,InterruptedException </span>&#123;</span><br><span class="line">String movieIds = <span class="string">&quot;&quot;</span>;</span><br><span class="line"><span class="keyword">for</span>(Text val:values) &#123;</span><br><span class="line">movieIds += val.toString() + <span class="string">&quot; &quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line">vtext.set(movieIds);</span><br><span class="line">context.write(key, vtext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * input: /data/movie_program/output/movieQuality1/part-r-00000</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">if</span> (args.length &lt; <span class="number">2</span>) &#123;</span><br><span class="line">System.out.println(<span class="string">&quot;Usage:&lt;inputPath&gt;&lt;outputPath&gt;&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">Path inputPath = <span class="keyword">new</span> Path(args[<span class="number">0</span>]);</span><br><span class="line">Path outputPath = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line"><span class="keyword">if</span> (fs.exists(outputPath)) &#123;</span><br><span class="line">fs.delete(outputPath, <span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Job job = Job.getInstance(conf);</span><br><span class="line">job.setJobName(<span class="string">&quot;movieQuanlity2&quot;</span>);</span><br><span class="line">job.setJarByClass(movieQuality2.class);</span><br><span class="line"></span><br><span class="line">job.setMapperClass(RMapper.class);</span><br><span class="line">job.setReducerClass(RReducer.class);</span><br><span class="line">        </span><br><span class="line">job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(job, inputPath);</span><br><span class="line">        FileOutputFormat.setOutputPath(job, outputPath);</span><br><span class="line"></span><br><span class="line">job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>MovieQuality3</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> movie_program;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> movie_program.Rating3.Rating2Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.commons.lang.StringUtils;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">movieQuality3</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="class"><span class="keyword">class</span> <span class="title">RMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">Map&lt;String,String&gt; movieInfoMap = <span class="keyword">new</span> HashMap&lt;String, String&gt;();</span><br><span class="line">        Text ktext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        Text vtext = <span class="keyword">new</span> Text(<span class="string">&quot;&quot;</span>);</span><br><span class="line">        </span><br><span class="line">        <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">setup</span><span class="params">(Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">            BufferedReader br = <span class="keyword">new</span> BufferedReader(<span class="keyword">new</span> InputStreamReader(<span class="keyword">new</span> FileInputStream(<span class="string">&quot;/home/cloudera/Desktop/movies.txt&quot;</span>)));</span><br><span class="line">            String line;</span><br><span class="line">            String movieId = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            String movieYear = <span class="string">&quot;&quot;</span>;</span><br><span class="line">            <span class="keyword">while</span>(StringUtils.isNotEmpty(line = br.readLine())) &#123;</span><br><span class="line">                String[] strs = line.split(<span class="string">&quot;\\&#123;&quot;</span>);<span class="comment">// example: 3 1</span></span><br><span class="line">                movieId = strs[<span class="number">0</span>];</span><br><span class="line">                <span class="keyword">if</span>(strs[<span class="number">1</span>].contains(<span class="string">&quot;`&quot;</span>)) &#123;</span><br><span class="line">    movieYear = strs[<span class="number">1</span>].substring(strs[<span class="number">1</span>].indexOf(<span class="string">&quot;`&quot;</span>) + <span class="number">1</span>, strs[<span class="number">1</span>].indexOf(<span class="string">&quot;`&quot;</span>) + <span class="number">5</span>);<span class="comment">// example: 1995</span></span><br><span class="line">    movieInfoMap.put(movieId, movieYear);<span class="comment">// example: &#123;(&quot;1&quot;,&quot;1995&quot;)&#125;(movieId, year)</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"><span class="keyword">int</span>[] num_eachTwoDecades = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">7</span>];</span><br><span class="line"><span class="keyword">int</span> decade = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">String[] strs = value.toString().split(<span class="string">&quot;&quot;</span>);<span class="comment">// rank movieIds </span></span><br><span class="line">String rank = strs[<span class="number">0</span>];</span><br><span class="line">String movieId_str = strs[<span class="number">1</span>];</span><br><span class="line">String[] movieIds = movieId_str.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">String movieYear = <span class="string">&quot;&quot;</span>;</span><br><span class="line"><span class="keyword">for</span>(String movieId:movieIds) &#123;</span><br><span class="line">movieYear = movieInfoMap.get(movieId);</span><br><span class="line"><span class="keyword">if</span>(movieYear != <span class="keyword">null</span>) &#123;</span><br><span class="line">decade = (Integer.parseInt(movieYear) - <span class="number">1890</span>) / <span class="number">20</span>;</span><br><span class="line">num_eachTwoDecades[decade] ++;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">ktext.set(rank);</span><br><span class="line">vtext.set(num_eachTwoDecades[<span class="number">0</span>] + <span class="string">&quot; &quot;</span> + num_eachTwoDecades[<span class="number">1</span>] + <span class="string">&quot; &quot;</span> + num_eachTwoDecades[<span class="number">2</span>] + <span class="string">&quot; &quot;</span> + num_eachTwoDecades[<span class="number">3</span>] + <span class="string">&quot; &quot;</span></span><br><span class="line">+ num_eachTwoDecades[<span class="number">4</span>] + <span class="string">&quot; &quot;</span> + num_eachTwoDecades[<span class="number">5</span>] + <span class="string">&quot; &quot;</span> + num_eachTwoDecades[<span class="number">6</span>]);</span><br><span class="line">context.write(ktext, vtext);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * input: /data/movie_program/output/movieQuality2/part-r-00000</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        Job job = Job.getInstance(conf);</span><br><span class="line">        job.setJarByClass(movieQuality3.class);</span><br><span class="line">        job.setMapperClass(RMapper.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(Text.class);</span><br><span class="line">        FileInputFormat.setInputPaths(job, <span class="keyword">new</span> Path(args[<span class="number">0</span>]));</span><br><span class="line">        FileSystem fs = FileSystem.get(conf);</span><br><span class="line">        Path path = <span class="keyword">new</span> Path(args[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">if</span>(fs.isDirectory(path))&#123;</span><br><span class="line">            fs.delete(path, <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        FileOutputFormat.setOutputPath(job, <span class="keyword">new</span> Path(args[<span class="number">1</span>]));</span><br><span class="line">        job.setNumReduceTasks(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">boolean</span> res =job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        System.exit(res?<span class="number">1</span>:<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="其他角度"><a href="#其他角度" class="headerlink" title="其他角度"></a>其他角度</h2></li></ol><h3 id="电影评分与受关注度（体现在评分用户数）的关系"><a href="#电影评分与受关注度（体现在评分用户数）的关系" class="headerlink" title="电影评分与受关注度（体现在评分用户数）的关系"></a>电影评分与受关注度（体现在评分用户数）的关系</h3><p>一部电影可能会被多个用户评分，那么一部电影的平均得分和给它评分的用户数是否有关系呢，换句话说，是不是电影越出名，它就越有可能得高分呢？</p><h4 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h4><p>这个分析方向其实比较简单，因为之用到了一张表的两列，因此思路也很简单。</p><p>首先提取出表“ratings.csv”的第二列和第三列，即 movieId 列和 rating 列，存储在一个 RDD 中，然后通过 <code>ReduceByKey</code> 并稍加运算即可得到每部电影的平均分和被评论的次数，具体代码也就两行，最后即可得到以下结果：</p><blockquote><p><img src="http://oom3nz471.bkt.clouddn.com/43.jpg" alt="1530863023819"></p></blockquote><p>其中，第一列为一部电影被评论的次数，第二列为该电影的平均分。</p><h4 id="图表分析与结论-2"><a href="#图表分析与结论-2" class="headerlink" title="图表分析与结论"></a>图表分析与结论</h4><p><img src="http://oom3nz471.bkt.clouddn.com/44.jpg" alt="1530863072337"></p><p>如图所示，横轴代表分数，纵轴代表被评分的次数，正如图中黑色的线所示，随着评分次数的增加，分数是有一个增加的趋势的，但是波动比较大，因为一部电影除了因为拍的很好导致大家都想去评分外，也可能是因为拍的很烂，但还是前者的可能性大一些，从而产生了图中的趋势，当然这只是一部分的原因。但总的来说，一部电影越出名，它的分数越高的可能性是比较大的。</p><h4 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;WordCount&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(<span class="string">&quot;D:/BigData/input/ratings.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> movieIds = lines.map(line =&gt; (line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">1</span>), line.split(<span class="string">&quot;,&quot;</span>)(<span class="number">2</span>))).map(x =&gt; (x._1, (x._2.toDouble, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">val</span> output = movieIds.reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + <span class="number">1</span>)).map(x =&gt; (x._2._2, (x._2._1 / x._2._2).formatted(<span class="string">&quot;%.2f&quot;</span>)))</span><br><span class="line">    output.collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="用户一般看电影的时段"><a href="#用户一般看电影的时段" class="headerlink" title="用户一般看电影的时段"></a>用户一般看电影的时段</h3><h4 id="分析思路-8"><a href="#分析思路-8" class="headerlink" title="分析思路"></a>分析思路</h4><p>通过分析用户在网上评论电影的时间可以推断出用户看电影的时间，从而可以得出一些商业价值的信息。比如网站在哪些时间段投放较多广告收益最高，网站在哪些时间段更新电影信息获得流量最高等等。</p><h4 id="代码-6"><a href="#代码-6" class="headerlink" title="代码"></a>代码</h4><ol><li><p>首先转换时间戳为方便人读的时间：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tranTimeToString</span></span>(tm:<span class="type">String</span>) :<span class="type">String</span>=&#123;</span><br><span class="line">  <span class="keyword">val</span> fm = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH&quot;</span>)</span><br><span class="line">  <span class="keyword">val</span> tim = fm.format(<span class="keyword">new</span> <span class="type">Date</span>(tm.toLong))</span><br><span class="line">  tim</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>然后分割数据并且对数据进行累加，得出结论：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> result = rdd.map(x =&gt; (tranTimeToString(x.split(<span class="string">&quot;**\&#123;&quot;</span>)(<span class="number">3</span>)+<span class="string">&quot;000&quot;</span>**),<span class="number">1</span>)).reduceByKey((x,y)=&gt;x+y).sortByKey()</span><br></pre></td></tr></table></figure><p><strong>完整代码</strong></p></li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.text.<span class="type">SimpleDateFormat</span></span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">Date</span></span><br><span class="line"><span class="keyword">import</span> java.io.<span class="type">File</span></span><br><span class="line"><span class="keyword">import</span> java.io.<span class="type">PrintWriter</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TimeTurn</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;TimeTurn&quot;</span>).setMaster(<span class="string">&quot;local&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;C:\\Users\\iprcc\\Desktop\\movie\\tags5.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> result = rdd.map(x =&gt; (tranTimeToString(x.split(<span class="string">&quot;\\&#123;&quot;</span>)(<span class="number">3</span>)+<span class="string">&quot;000&quot;</span>),<span class="number">1</span>)).reduceByKey((x,y)=&gt;x+y).sortByKey()</span><br><span class="line">    println(result.collect().mkString(<span class="string">&quot;\n&quot;</span>))</span><br><span class="line">    <span class="keyword">val</span> writer = <span class="keyword">new</span> <span class="type">PrintWriter</span>(<span class="keyword">new</span> <span class="type">File</span>(<span class="string">&quot;C:\\Users\\iprcc\\Desktop\\movie\\result2.txt&quot;</span>))</span><br><span class="line">    writer.println(result.collect().mkString(<span class="string">&quot;\n&quot;</span>))</span><br><span class="line">    writer.close()</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">tranTimeToString</span></span>(tm:<span class="type">String</span>) :<span class="type">String</span>=&#123;</span><br><span class="line">    <span class="keyword">val</span> fm = <span class="keyword">new</span> <span class="type">SimpleDateFormat</span>(<span class="string">&quot;HH&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> tim = fm.format(<span class="keyword">new</span> <span class="type">Date</span>(tm.toLong))</span><br><span class="line">    tim</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="图表-4"><a href="#图表-4" class="headerlink" title="图表"></a>图表</h4><p><img src="http://oom3nz471.bkt.clouddn.com/45.jpg" alt="img"> </p><h4 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h4><p>人们喜欢在北京时间3到5点的时候上网评论电影，转换成当地时间大概是下午4点前后流量最多。说明人们多喜欢在下午看电影。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><h2 id="各流派的电影数量"><a href="#各流派的电影数量" class="headerlink" title="各流派的电影数量"></a>各流派的电影数量</h2><p><img src="http://oom3nz471.bkt.clouddn.com/3.jpg" alt="img"> </p><p><img src="http://oom3nz471.bkt.clouddn.com/4.jpg" alt="img"></p><p>从各流派电影数图来看，电影数最多的是Drama类，电影数最少的是IMAX类。我们猜测，IMAX技术出现比较晚，受年代限制所以这类电影数量比较少，除IMAX类之外最少的是Film-Noir（黑色电影）类。</p><h2 id="各流派得分随时间的变化-1"><a href="#各流派得分随时间的变化-1" class="headerlink" title="各流派得分随时间的变化"></a>各流派得分随时间的变化</h2><p><img src="http://oom3nz471.bkt.clouddn.com/8.jpg" alt="1530875595962"></p><p>上图包含了很多信息，可以从很多角度进行分析。比如，恐怖片的质量看上去越来越差强人意了，犯罪主题的电影似乎也越来越不合人的胃口，但又比恐怖片好一些。纪录片似乎质量一直不错，枪战片的质量在上世纪七八十年代似乎很不错，另外，似乎所有流派的电影质量都有所下滑，这也许是人们已经厌倦了现代的好莱坞式商业电影大行于世导致的。</p><h2 id="高分电影中的比较多的标签-1"><a href="#高分电影中的比较多的标签-1" class="headerlink" title="高分电影中的比较多的标签"></a>高分电影中的比较多的标签</h2><p><img src="http://oom3nz471.bkt.clouddn.com/14.jpg" alt="img"></p><p>根据图表可知，高分电影中标签数量最多的是imdb top 250，说明imdb的电影排行榜还是很靠谱的，其排行榜上的电影也是movielens网站中用户给高分最多的；排名第二的为hitchcock，这是一个知名的导演的名字，他的代表作为<a href="https://baike.baidu.com/item/%E5%90%8E%E7%AA%97">后窗</a>、<a href="https://baike.baidu.com/item/%E6%83%8A%E9%AD%82%E8%AE%B0">惊魂记</a>、<a href="https://baike.baidu.com/item/%E8%A5%BF%E5%8C%97%E5%81%8F%E5%8C%97">西北偏北</a>、<a href="https://baike.baidu.com/item/%E8%9D%B4%E8%9D%B6%E6%A2%A6">蝴蝶梦</a>、<a href="https://baike.baidu.com/item/%E5%88%97%E8%BD%A6%E4%B8%8A%E7%9A%84%E9%99%8C%E7%94%9F%E4%BA%BA">列车上的陌生人</a>等，曾获得过奥斯卡奖终身成就奖，高分电影中以这位导演为标签的数目比较多，说明这位大众对这位导演的电影都有很高的评价；排名第三的为criterion标准，说明大众给标准化的电影打分也是可观的；排名第四的为oscar（best picture）奥斯卡最佳影片，说明奥斯卡评出来的电影是很权威的，也是符合大众口味的；第五名为Screwball comedy疯狂喜剧，之后又接着两个与奥斯卡相关的，看来奥斯卡评出的电影确实是很不错的电影，不论是对专家还是对看电影的用户来说。</p><h2 id="电影得分与标签数量之间的关系-1"><a href="#电影得分与标签数量之间的关系-1" class="headerlink" title="电影得分与标签数量之间的关系"></a>电影得分与标签数量之间的关系</h2><p><img src="http://oom3nz471.bkt.clouddn.com/20.jpg" alt="img"></p><p><img src="http://oom3nz471.bkt.clouddn.com/21.jpg" alt="img"></p><p>由上面的图表可以看出标签数量较多的集中在大致在电影得分在3.5-4.0范围之间，而标签数量大致随电影得分的增高而增长，在4.0到5.0之间这个趋势有点减弱，这个分数阶段的标签数量相对与3.5-4.0之间要少许多；就此分析，我们可以推论得分3.5-4.0的电影可能更被人熟知，被更多的人评论，从而贴上更多的标签，剧情各方面的综合性比较强；而可能得分0-4.0的电影不怎么符合大众的口味，或者剧情各方面经不起推敲，接近烂片的标准；至于得分4.0-5.0的电影属于被人遗忘的佳作的类型，亦或是佳作根本无需贴标签，只需要静静欣赏的类型。</p><h2 id="电影数量与年代变化之间的关系-1"><a href="#电影数量与年代变化之间的关系-1" class="headerlink" title="电影数量与年代变化之间的关系"></a>电影数量与年代变化之间的关系</h2><p><img src="http://oom3nz471.bkt.clouddn.com/22.jpg" alt="img"></p><p>从时间来看，如图所示，从1891年开始到2017年，上映的电影数量整体趋势上涨。在18世纪末，电影行业应该才刚刚起步，电影数量非常少，而到了十九世纪，电影逐渐增加，特别是到了19世纪末，电影数量开始飞速增长，到了20世纪，十几年内电影数量已经达到了14000万以上。</p><h2 id="各流派的电影数量随年代的变化趋势"><a href="#各流派的电影数量随年代的变化趋势" class="headerlink" title="各流派的电影数量随年代的变化趋势"></a>各流派的电影数量随年代的变化趋势</h2><p><img src="http://oom3nz471.bkt.clouddn.com/23.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/24.jpg" alt="img"></p><p>由图1可看出在电影史上drama类型的电影数量一直处于上升趋势，很多电影喜欢加入drama的因素在其中，也说明市场一直需要drama类型的电影。但是由图2知在2011年之后drama类型的电影出现了陡然下降的趋势，其原因可能是drama类型的电影的关注开始降低，也可能是由于整体电影市场中电影数量的减少导致。</p><p>虽然在整个电影史上电影的总数量呈极速增长的趋势，但是在2011年之后几乎所有类型的电影的数量都开始减少。经分析，其原因可能是：1、电影的类型开始不多元化（如果一个电影有多个类型标签，则通过此统计算出的此类型的电影数量会变多）2、电影拍摄数量确实开始减少，可能是因为电影拍摄成本升高的原因3、电影市场需求量减少。总而言之，电影圈的竞争越来越激烈。</p><p>从图2我们还可以看出只有IMAX类型的电影处于增长趋势，这说明今后此技术的电影需求量在上升。</p><h2 id="不同年代的拥有电影数量最多的电影流派"><a href="#不同年代的拥有电影数量最多的电影流派" class="headerlink" title="不同年代的拥有电影数量最多的电影流派"></a>不同年代的拥有电影数量最多的电影流派</h2><p><img src="http://oom3nz471.bkt.clouddn.com/25.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/26.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/27.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/28.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/29.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/30.jpg" alt="img"></p><p>60、70、80年代Drama、conmedy和action的电影类型数量最多，90年代之后thriller类型替换了action进入了前三。</p><p>戏剧与喜剧一直高居榜首可见这两个类型是大多数电影的主要元素，市场欢迎程度最高，这也说明了大多数人看电影主要是为了娱乐和放松，也揭示了电影最主要的功能是娱乐大众。</p><p>在90年代以后，恐怖片的数量超过动作片，可以说明恐怖片的市场在增长，可能是因为人们开始更多寻求心理上的刺激。</p><h2 id="上世纪与本世纪比较受欢迎的电影类别-1"><a href="#上世纪与本世纪比较受欢迎的电影类别-1" class="headerlink" title="上世纪与本世纪比较受欢迎的电影类别"></a>上世纪与本世纪比较受欢迎的电影类别</h2><p><img src="http://oom3nz471.bkt.clouddn.com/37.jpg" alt="img"><br><img src="http://oom3nz471.bkt.clouddn.com/38.jpg" alt="img"></p><p>通过对比分析不难得出，恐怖片无论在上世纪还是本世纪都是得分最低的，这可能跟恐怖片成本不高，导致滥竽充数的作品很多有关，而上个世纪中得分最高的电影类别为”没有类别“，这看上去有些不合情理，但仔细一想，上个世纪的跨度为 100 年，早期的电影也许无法准确定性为何种流派的电影，而这些作品又确实吸引了一大批观众，将这些作品带上了首席宝座，而这个世纪平均分最高的类别为 Film-Noir，网上给出的翻译为<strong>黑色电影</strong>，我对这方面没有什么研究，感兴趣的同学可以自己分析下这其中的原因。</p><h2 id="各分数段的电影数量与年代的关系-1"><a href="#各分数段的电影数量与年代的关系-1" class="headerlink" title="各分数段的电影数量与年代的关系"></a>各分数段的电影数量与年代的关系</h2><p><img src="http://oom3nz471.bkt.clouddn.com/42.jpg" alt="img"></p><p>不难看出，1990~2010 年间，世界上涌现出了很多优秀的电影作品，无论从电影数量还是质量来说，1990~2010 都是电影史上的一个黄金时期。另外，由于 2010 年至今只有 8 年，因此电影数量看起来比较低。</p><h2 id="电影评分与受关注度（体现在评分用户数）的关系-1"><a href="#电影评分与受关注度（体现在评分用户数）的关系-1" class="headerlink" title="电影评分与受关注度（体现在评分用户数）的关系"></a>电影评分与受关注度（体现在评分用户数）的关系</h2><p><img src="http://oom3nz471.bkt.clouddn.com/44.jpg" alt="img"></p><p>如图所示，横轴代表分数，纵轴代表被评分的次数，正如图中黑色的线所示，随着评分次数的增加，分数是有一个增加的趋势的，但是波动比较大，因为一部电影除了因为拍的很好导致大家都想去评分外，也可能是因为拍的很烂，但还是前者的可能性大一些，从而产生了图中的趋势，当然这只是一部分的原因。但总的来说，一部电影越出名，它的分数越高的可能性是比较大的。</p><h2 id="用户一般看电影的时段-1"><a href="#用户一般看电影的时段-1" class="headerlink" title="用户一般看电影的时段"></a>用户一般看电影的时段</h2><p><img src="http://oom3nz471.bkt.clouddn.com/45.jpg" alt="img"></p><p>从各流派电影数图来看，电影数最多的是Drama类，电影数最少的是IMAX类。我们猜测，IMAX技术出现比较晚，受年代限制所以这类电影数量比较少，除IMAX类之外最少的是Film-Noir（黑色电影）类。</p><h2 id="各流派内电影得分榜top10-1"><a href="#各流派内电影得分榜top10-1" class="headerlink" title="各流派内电影得分榜top10"></a>各流派内电影得分榜top10</h2><p>结论即图表</p><ol><li><p><strong>冒险片 Adventure 排行榜</strong></p><p> <img src="http://oom3nz471.bkt.clouddn.com/9.jpg" alt="img"></p></li><li><p><strong>喜剧片 Comedy 排行榜</strong></p><p> <img src="http://oom3nz471.bkt.clouddn.com/10.jpg" alt="img"></p></li><li><p><strong>爱情片 Romance 排行榜</strong></p><p> <img src="http://oom3nz471.bkt.clouddn.com/11.jpg" alt="img"></p></li><li><p><strong>恐怖片 Thriller 排行榜</strong></p><p> <img src="http://oom3nz471.bkt.clouddn.com/12.jpg" alt="img"></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 大数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>年终总结与新年展望</title>
      <link href="2018/03/05/2018/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E4%B8%8E%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/"/>
      <url>2018/03/05/2018/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E4%B8%8E%E6%96%B0%E5%B9%B4%E5%B1%95%E6%9C%9B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>17 年对于我来说，像是个分水岭。</p></blockquote><a id="more"></a><p>回想高考结束后填志愿时，不像那些早已决定要进入计算机或互联网行业的早有准备的有识之士，我只根据自己的分数填了几所学校，根据网上的评论选了几个专业，最后阴差阳错的走上了计算机这条路。这时是 15 年。</p><p>按时上课、通过考试，参加几个学校的活动，我以为大学不过如此：完成四年的学业，找一份说得过去的工作或继续考取研究生，大学就这么弹指而过。这时是 16 年。</p><p>也许是某次上课发呆时，我突然为过去一年半的生活感到沮丧，我似乎看不到以后的道路通向何方，人的一生何其短暂，就这样挥霍四年是否太过奢侈。我决定做些更有意义的事情。这时已经是 17 年的年初了，大二下学期，大学快要过去一半了。</p><p>通过老师和同学的推荐，凭借过去两年多少懂点的知识以及热情，我加入了学院的一个项目组。现在回想初入项目组时，我对于新接触到的一切事物都感到好奇，虽然面对突如其来的新环境，我有些不知所措，不知自己何时才能融入这里，但内心更多的还是充满了热情与信心。我暗下决心，要打破过去的枷锁，以全新的姿态迎接一种全新的生活。</p><p>现在的我，面对一个全新的知识或语言，可以很快的通过各种途径了解并渐渐掌握它，然而初入项目组时，面对从前并未使用过的 Java，我有些畏惧。我花了一个星期的时间阅读一本 Java 入门书，正当我打算为 Java 再花一个星期时，老师和学长建议我可以看 Android 的入门书籍了，虽然有些疑惑，但我遵从了建议。一个星期之后，我又被通知可以帮助学长完成项目中的一个小模块了。之后的几天，我几乎将全部时间用在了工作上，从书上、网上查找了大量资料以及解决一个个 Bug 后，我用了三天将 Demo 做好了，并展示给了老师和学长看。我记得当时得到老师和学长的肯定后，一种成就感和自信感充斥着我的内心。虽然这只是个很简单的任务，但意味着我渐渐地脱离了过去的生活模式，找到了新的生活轨道。</p><p>日子一天天过去，日历上的数字从 3 变成了 5，夏天慢慢到来。过去的几个月，我的内心在遇到 Bug 和解决 Bug 的过程中不断经历着失落、焦躁和兴奋与自信。同学长一起，我们已经渐渐完成了一个 Android App 的开发，我也从懵懂无知逐渐拥有了独自完成一个模块的能力。但对我来说，更重要的不是有关 Android 的知识增加了，而是我的眼界渐渐更加广阔了。</p><p>由于经常在网上查找大量资料，我渐渐接触了一些互联网行业的大 V，关注了他们的微信公众号，通过阅读他们的文章，我接触到了一个全新的世界，我第一次感觉自己在和这个世界接轨，我第一次觉得自己所在的行业是多么欣欣向荣，第一次为当时选择这个专业而庆幸，也第一次决定了自己一定要在这个行业有所成就。</p><p>然而视野的开阔必然会导致分心，由于每天面对如此大的信息量，我一时有些不知所措，昨天发现 Python 正越来越火热，今天发现 Google 公布将 Kotlin 作为 Android 的官方语言，明天又发现 Pytorch 似乎比 TensorFlow 更方便。我在湍急的信息流中踉跄前行，目标也摆动不定，昨天还在学习 Kotlin，今天就投入了机器学习的怀抱。这带给我的影响有利有弊，利在于我短时间内对如此多的知识都有了一定了解，弊在于我没有任何实质性的掌握。</p><p>结合目前行业的趋势，经过深思熟虑之后，我认为虽然自己在项目组中负责的是 Android，但无论从发展趋势还是对于整个人类的意义来说，我更愿意投入人工智能的怀抱，因此在不影响项目进度的情况下，我买了一些相关书籍，并报名了吴恩达在 Coursera 上的机器学习课程，开始自学机器学习。</p><p>经过两个多月的学习，我完成了整个课程并拿到了结课证书，自己对于机器学习以及人工智能的理解也不再像从前那样肤浅了。然而知道的越多，就会发现自己不知道的越多，我逐渐意识到，对于一个我这样的本科生来说，希望在本科期间仅凭一己之力便掌握像机器学习以及人工智能这样一个庞大的学科甚至找到一个工作是不现实的，于是我第一次找到了读研究生的目的。</p><p>随着各方面知识以及眼界的不断开阔，虽然信心不断增加，但是我偶尔也有些浮躁。17 年的暑假，手头上的项目已告一段落，于是我有了大把可以自由支配的时间，现在回想起来，当时我并没有充分利用好这段时间。俗话说“胜不骄败不馁”，一时被几个月以来取得的成果蒙蔽双眼的我，却放慢了前进的步伐，由于没有充分利用好时间，一个月的时间飞快地掠过，我却并没有收获多少新的知识，相比前几个月，我的进步变的缓慢无比，甚至可以用“停滞”来形容。</p><p>好在经常接触一些行业资讯，我渐渐又意识到，自己目前的水平与心中的期望还有十万八千里之距，现在绝不是原地踏步的时候。于是我减少了刷微博、微信这些琐事的时间，调整心态，收拾好行囊，重新上路。</p><p>在项目组度过的时间越来越多，我也越来越把这里当成自己在学校的第二个家。刚来这里的时候，学长学姐对我给予了无私的帮助，使我很快就融入了这里，但天下没有不散的宴席，学长学姐在六月底登上前往远方实习的大巴后，我望着空落落的实验室，心里也有些空落。一批老人的离开，一批新人的到来，我意识到自己已不再是当初那个懵懂无知的新人，我有责任像当初的学长学姐们一样，继续严格要求自己，在这里挥洒自己的汗水，继续前行。</p><p>我经常听到一种声音，程序员不能只把眼光局限在自己技术领域的一亩三分地，否则迟早会被淘汰。人类进程在过去的五百年间发生了极大地飞跃，三次工业革命使人类社会与过去的几千年发生了质的变化，而当前我们正处在第四次革命的关口，而这次的革命将比前三次对人类的生活产生更大的影响。互联网行业已经和人们的生活息息相关，作为这个行业的一员，我们不应该把眼光局限于未来可以找到一份什么工作、可以赚多少钱，诚然，这些是个人发展最基本的要素，但既然有机会做些更大的事情，为什么不去寻找这样的机会呢？虽然这么说显得有些虚伪，但现在的程序员就是推动世界前进的最中坚的力量之一，作为这个集体的一份子，有责任有义务拥有更远大的理想。</p><p>但凡事都要一步步来，我深知当前所处的环境无法使我实现理想，我也无法改变环境，只有想办法进入更好的环境。考研对我来说不只是一场考试而已，我希望借助这个机会可以进入一个更好的环境，遇见更优秀的人，迎接更好的机遇。</p><p>17 年之前，未来对我来说是迷茫的，然而就在这一年，我渐渐从迷雾中窥探到了一条发着光的路，指引着我脚踏实地的前行。18 年年底，我就要参加研究生入学考试了，某种意义上，这场考试甚至比高考对我的意义更大，因此今年的重心自然就在于准备考试上。我不愿再像三年前那样，分没分清主次的我没有抓住人生的第一个转折点，既然第二个转折点正在到来，我不能再次错过。</p><p>写了这么多，最后做下总结：</p><ol><li>17 年，我不仅收获了各方面的知识，也提升了自己的眼界，对未来的道路有了比较清晰的规划。我对于这一年其实很满意，但如果我能避免偶尔的浮躁，相信可以有更丰富的收获；</li><li>18 年，重心在于备考，这场考试关系到人生走向，不得有失；</li><li>大学已过去 3/4，我希望当我离开这里时，收获的不仅是知识，而是可以成为一个更全面的人，至于怎样的人才是全面的人，可能我还不是特别清楚，但有一点我可以肯定，知识的多少绝不是衡量一个人的唯一要素，大学的教育目标应该是培养一个人自由自主的精神。</li></ol><p>所有的惊艳都来自长久的努力，所有的幸运都来自不懈的坚持，既然选择了这条路，就要走到底，没有人会在意这一路有多少坎坷、多少荆棘，人们只会关注成功一刻释放的花火。</p><p>2018 加油💪</p>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线程问题与 volatile</title>
      <link href="2018/01/26/2017/%E7%BA%BF%E7%A8%8B%E9%97%AE%E9%A2%98%E4%B8%8E%20volatile/"/>
      <url>2018/01/26/2017/%E7%BA%BF%E7%A8%8B%E9%97%AE%E9%A2%98%E4%B8%8E%20volatile/</url>
      
        <content type="html"><![CDATA[<blockquote><p>上周花了一整周的时间去排查项目中出现的一个 Bug。主要是和线程有关，现在把过程记录一下。  </p></blockquote><a id="more"></a><h1 id="两个线程"><a href="#两个线程" class="headerlink" title="两个线程"></a>两个线程</h1><p>项目中需要从一块开发板上获取数据，然后对获取到的数据进行处理，因此就需要两个线程，一个用来读取数据，一个用来处理数据。由于开发板传过来的数据非常快非常多，为了提升效率，我们采用了两个数组，一个用来存放读数据线程读到的数据，一个用来存放标志位，标志位全都是 True 或 False，初始化时全是 False，这两个数组的大小相同，两个线程循环读取每个下标，即读完最后一个位置后就回到第一个位置。当读数据线程读到数据并存放进第一个数组时，第二个数组的相应位置就变为 True，在这个过程中数据处理线程一直在判断第二个数组的标志位，如果读到的是 False，就等待，直到读到了 True，才会去第一个数组的相应位置读取数据并进行处理，处理完后又会把第二个数组的相应位置置为 False，而如果读数据线程发现某个位置是 True，也会等待，直到数据处理线程将该位置变为 False 后才会继续执行。这个过程持续执行，直到测量结束。</p><h1 id="隐患"><a href="#隐患" class="headerlink" title="隐患"></a>隐患</h1><p>为了避免两个线程读到同一个位置，因此加入了第二个数组，但是这样又引出了另一个问题：由于数据的读入和处理非常快，因此第二个数组中的变化也非常快，一个位置的 True 和 False 变化非常迅速。我们的操作系统有一个机制叫做<strong>编译器优化</strong>，即当一组指令对一个变量的赋值操作非常频繁时，为了提升效率，操作系统会直接取最后一个指令的值赋给这个变量，中间的值的改变就忽略了，这样的机制在线程中的影响是非常大的，因为这直接导致了第二个数组的变化不够及时，导致有的应该是 True 的地方是 False，有的应该是 False 的地方是 True，这就导致读数据线程可能读到 True 而等待，而此时处理数据线程读的下标比读数据线程小，因此这个 True 永远不会变为 False，所以读数据线程会一直等待，数据处理线程也会一直等待，这就导致读不到数据的情况发生。</p><h1 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h1><p>关键在于禁止编译器优化，java 中的关键字 volatile 的一个功能就是禁止编译器优化，相当于告诉了操作系统，这个变量会频繁变化，但不要优化。这里频发变化的是第二个数组，因此在声明第二个数组时加上 volatile 关键字即可。经测试，之后的确没有出现读不到数据的情况了。</p><h1 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h1><p>线程很复杂，导致这一 Bug 的原因可以有很多，这可能只是其中一个，因此在进行多线程编程时一定要小心，项目中的线程算简单了，当遇到更复杂的线程时，会需要更多安全机制，这值得我们不断学习。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图形图像综合处理平台</title>
      <link href="2018/01/04/2017/%E8%AF%BE%E8%AE%BE%E8%AF%B4%E6%98%8E%E4%B9%A6/"/>
      <url>2018/01/04/2017/%E8%AF%BE%E8%AE%BE%E8%AF%B4%E6%98%8E%E4%B9%A6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>该软件分为两部分：图形处理部分和图像处理部分，下面分别进行讲解。<br><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn4fgkby28j30xc0q8dmz.jpg" alt=""></p></blockquote><a id="more"></a><h1 id="图形部分"><a href="#图形部分" class="headerlink" title="图形部分"></a>图形部分</h1><h2 id="2D-图形"><a href="#2D-图形" class="headerlink" title="2D 图形"></a>2D 图形</h2><p>当用户点击左上角的 2D 按钮后，即可对 2D 图形进行处理。</p><h3 id="选择要绘制的图形"><a href="#选择要绘制的图形" class="headerlink" title="选择要绘制的图形"></a>选择要绘制的图形</h3><p>用户可通过蓝色标题“2D Operation”下的第一个下拉框选择想要绘制的图形，如图所示</p><ul><li><p>三角形<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gfcfe2yj31kw0timzf.jpg" alt=""></p></li><li><p>矩形<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4gfc182kj31kw0ti76i.jpg" alt=""></p></li></ul><h3 id="图形变换"><a href="#图形变换" class="headerlink" title="图形变换"></a>图形变换</h3><p>用户可通过蓝色标题“2D Operation”下的四个点选框使图形进行变换，其四个点选框分别为</p><ol><li>x 方向的平移<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gfblje6j31kw0titax.jpg" alt=""> </li><li>y 方向的平移<br><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4gfb6gm9j31kw0titax.jpg" alt=""></li><li>旋转<br><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn4gfastarj31kw0tijtl.jpg" alt=""></li><li>缩放<br><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4gfaew0cj31kw0tidi2.jpg" alt=""></li></ol><h3 id="设置裁剪窗口"><a href="#设置裁剪窗口" class="headerlink" title="设置裁剪窗口"></a>设置裁剪窗口</h3><p>用户可通过蓝色标题“Please choose a clipping algorthm”下的三个按钮选择是否需要裁剪或者选择 Cohen-Sutherland 裁剪算法或者 Liang-Barsky 裁剪算法进行裁剪。由于无论是 CS 算法还是 LB 算法，只要裁剪窗口的位置和大小一定，那么裁剪后的图形也是一样的，因此这里只展示一张图片。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4gfa0243j31kw0ti76g.jpg" alt=""></p><p>如图所示，即为被边长为 15 的裁剪窗口 裁剪后的三角形。</p><p>用户还可以通过上述三个按钮下面的点选框来自定义裁剪窗口的大小，如图所示</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4gf9nmbjj31kw0tidi0.jpg" alt=""></p><p>上图为被边长为 12.2 的裁剪窗口裁剪时的三角形。</p><h3 id="设置颜色"><a href="#设置颜色" class="headerlink" title="设置颜色"></a>设置颜色</h3><p>用户可通过左下角的三个滑尺来自定义图形的颜色，三个滑尺由上到下分别代表 R、G、B 三个通道，颜色效果如图所示</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gf970pbj31kw0timzh.jpg" alt=""></p><h2 id="3D-图形"><a href="#3D-图形" class="headerlink" title="3D 图形"></a>3D 图形</h2><p>当用户点击左上角的 3D 按钮后，即可对 3D 图形进行处理。</p><h3 id="选择要绘制的图形-1"><a href="#选择要绘制的图形-1" class="headerlink" title="选择要绘制的图形"></a>选择要绘制的图形</h3><p>用户可通过蓝色标题“3D Operation”下的第一个下拉框选择想要绘制的图形，如图所示</p><ul><li>棱锥<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4gf8t0w2j31kw0ti0uz.jpg" alt=""></li><li>茶壶<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gf8fafgj31kw0tidjx.jpg" alt=""></li></ul><h3 id="图形变换-1"><a href="#图形变换-1" class="headerlink" title="图形变换"></a>图形变换</h3><p>用户可通过蓝色标题“3D Operation”下的三个点选框使图形进行变换，其三个点选框分别为</p><ol><li>图形距离<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gf7xmw5j31kw0tiwnv.jpg" alt=""><br><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn4gf74qmsj31kw0titb9.jpg" alt=""></li><li>绕 x 轴旋转<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gf6ljlbj31kw0tijvk.jpg" alt=""></li><li>绕 y 轴旋转<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gf64gocj31kw0tiq72.jpg" alt=""></li></ol><p>用户还可通过上述三个点选框下面的下拉框选择渲染方式，除了默认的线渲染外，还有两种，分别为</p><ol><li>点渲染<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4gjmqpmcj31kw0tigpk.jpg" alt=""></li><li>面渲染<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4gjmafbij31kw0tiack.jpg" alt=""></li></ol><h3 id="设置裁剪平面"><a href="#设置裁剪平面" class="headerlink" title="设置裁剪平面"></a>设置裁剪平面</h3><p>用户可通过蓝色标题“Please choose a clipping algorthm”下的三个按钮选择是否需要裁剪或者采用两个裁剪平面中的任意一种，分别如下图所示</p><ol><li>裁剪平面为 (1, 0, 0)<br><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4gjlwg31j31kw0tigov.jpg" alt=""></li><li>裁剪平面为 (1, 1, 0)<br><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4gjlewoxj31kw0ti41u.jpg" alt=""></li></ol><blockquote><p>为了充分体现裁剪效果，因此视角向 x 轴正方向做了略微移动。</p></blockquote><h3 id="设置颜色-1"><a href="#设置颜色-1" class="headerlink" title="设置颜色"></a>设置颜色</h3><p>和 2D 图形的颜色设置方法相同，只展示一张图片</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4gjkznubj31kw0tigq6.jpg" alt=""></p><h1 id="图像部分"><a href="#图像部分" class="headerlink" title="图像部分"></a>图像部分</h1><p>图像部分的操作流程是</p><ol><li>打开一张 RGB 图像</li><li>转化为灰度图并显示直方图</li><li>直方图均衡化并显示直方图</li><li>傅里叶变换并进行巴特沃斯低通滤波</li><li>傅里叶反变换</li><li>保存所有图像</li></ol><p>下面分别讲解</p><h2 id="打开图像"><a href="#打开图像" class="headerlink" title="打开图像"></a>打开图像</h2><p>点击右上角的“1. Open File…”按钮即可打开用户选择的图像</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn4fgkby28j30xc0q8dmz.jpg" alt=""></p><h2 id="转化为灰度图"><a href="#转化为灰度图" class="headerlink" title="转化为灰度图"></a>转化为灰度图</h2><p>点击右侧的“rgb2gray”按钮即可将原图转化为灰度图并显示直方图</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4fhs2390j30xc0q87dv.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4fhxafo1j30sg0fg0sl.jpg" alt=""></p><h2 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h2><p>点击右侧的“Equalize Hist”按钮即可将灰度图进行直方图均衡化处理并显示处理后的图像及相应的直方图</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fn4fj1wfgdj30xc0q8qcp.jpg" alt=""></p><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fn4fj5jd37j30sg0fgq2u.jpg" alt=""></p><h2 id="傅里叶变换并进行巴特沃斯低通滤波"><a href="#傅里叶变换并进行巴特沃斯低通滤波" class="headerlink" title="傅里叶变换并进行巴特沃斯低通滤波"></a>傅里叶变换并进行巴特沃斯低通滤波</h2><p>点击右侧“DFT”按钮即可对灰度图进行傅里叶变换，使其从空域图像转换为频域图像，然后进行巴特沃斯低通滤波</p><ol><li><p>频域图像<br><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4fku512cj30xc0q8wvy.jpg" alt=""></p></li><li><p>滤波后的频域图像<br><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn4fl7fljij30xc0q87lx.jpg" alt=""></p></li></ol><h2 id="傅里叶反变换"><a href="#傅里叶反变换" class="headerlink" title="傅里叶反变换"></a>傅里叶反变换</h2><p>点击右侧“IDFT”即可对滤波后的频域图像进行傅里叶反变换操作，使其从频域图像转换为空域图像，一边观察滤波效果</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fn4fn7r59pj30xc0q8dm3.jpg" alt=""></p><h2 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h2><p>点击右上角的“Save File…”按钮即可保存所有图像</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fn4ft47otsj31kw0zkth4.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Summary of Lesson The Discrete Fourier Transform</title>
      <link href="2017/12/27/2017/Summary%20of%20Lesson3.2/"/>
      <url>2017/12/27/2017/Summary%20of%20Lesson3.2/</url>
      
        <content type="html"><![CDATA[<p>In this lesson, we have studied in greater details the discrete Fourier transform (DFT), the Fourier analysis tool for finite-length signals.</p><p>The DFT is just a simple change of basis. The analysis formula (moving from the time to the frequency domain) is given by the inner product of the signal with the Fourier basis vector,</p><script type="math/tex; mode=display">\begin{align} X[k] &= \langle \mathbf{w}^{(k)}, \mathbf{x} \rangle \\&= \sum_{n=0}^{N-1} x[n] e^{-j \frac{2\pi}{N} nk}\end{align}.</script><p>The synthesis formula (moving from the frequency to the time domain) is a linear combination of the basis functions, scaled by the coefficients we found in the analysis formula,</p><script type="math/tex; mode=display">\begin{align} x[n]&= \frac{1}{N} \sum_{k=0}^{N-1} X[k]\mathbf{w}^{(k)} \\&= \frac{1}{N} \sum_{k=0}^{N-1} X[k] e^{j \frac{2\pi}{N} nk}\end{align}</script><p>In the most general case, the DFT coefficients \(X[k]\) are complex numbers and thus have a real and imaginary part. In order to interpret these coefficients, it is convenient to represent their magnitude \(|X[k]|\) and phase \(\angle X[k]\), wrapped between \(-\pi\) and \(\pi\). In a DFT plot, frequencies between \(0\) and \(N/2\) (resp. \(N/2\) and \(N-1\) correspond to digital frequencies smaller than \(\pi\) (resp. larger than \(\pi\)), in other words counterclockwise rotations (resp. clockwise rotations). Frequencies close to \(0\) and \(N-1\) corresponds to the low frequencies and those close to \(N/2\) to the high frequencies.</p><p>Finally, the DFT of a real signal is symmetric in magnitude, mathematically</p><p>\(|X[k]| = |X[N-k]|, k = 1, …, \lfloor{N/2}\rfloor.\)</p>]]></content>
      
      
      <categories>
          
          <category> 数字图像处理 </category>
          
          <category> 傅里叶变换 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字图像处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>（译）傅里叶理论的直观理解</title>
      <link href="2017/12/27/2017/%EF%BC%88%E8%AF%91%EF%BC%89%E5%82%85%E9%87%8C%E5%8F%B6%E7%90%86%E8%AE%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/"/>
      <url>2017/12/27/2017/%EF%BC%88%E8%AF%91%EF%BC%89%E5%82%85%E9%87%8C%E5%8F%B6%E7%90%86%E8%AE%BA%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>该文翻译自<a href="http://cns-alumni.bu.edu/~slehar/fourier/fourier.html">http://cns-alumni.bu.edu/~slehar/fourier/fourier.html</a></p></blockquote><p><strong>作者：<a href="http://cns-alumni.bu.edu/~slehar/">Steven Lehar</a></strong></p><p>slehar@cns.bu.edu</p><p>傅里叶理论在数学上非常复杂，但在其背后有些相对容易理解的整体概念。你可以在网上找到很多关于傅里叶变换的数学公式，我在这里将只展示一些应用于空间图像的傅里叶变换的直观理解。</p><hr><a id="more"></a><h2 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h2><p>傅里叶理论认为，任何信号都可以表示为一系列正弦波的叠加，以图像为例，即是图像亮度的正弦变化。比如下图中的正弦波即可通过傅里叶项来表示：1. 空间频率 2. 幅度（正或负） 3. 相位。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1.gif" alt=""></p><p>这三个值包含了上图的所有信息。空间频率就是亮度变化（上图中方向为 x 轴）的频率。比如下面这幅正弦图像有更高的空间频率。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3.gif" alt=""></p><p>幅度体现的是对比度，换句话说就是图像中最亮与最暗部分的差别。负幅度表示对比度反转，亮的变暗，暗的变亮。相位体现的是波形偏离原点的程度，在上面的例子中即图像左移或右移的程度。</p><p>傅里叶变换叠加的不是单一的正弦波，而是一系列正弦波，这些正弦波的频率范围从零一直升到奈奎斯特频率。只有一个空间频率 f 的信号可绘制为下图，其中只有 f 点一处峰值，该峰值取决于该信号的幅度，或者说对比度。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/fourier1.gif" alt=""></p><p>其中频率为零处的 DC 项表示这张图片的平均亮度。DC 项为 0 说明图片的平均亮度为 0，这意味着正弦曲线在正值和负值间交替。但实际上并不存在所谓的负亮度，因此所有图像的 DC 项都是正的。</p><p>事实上由于数学上的原因，傅里叶变换还绘制了上图关于原点的镜像，空间频率在两个方向上都在增加。由于数学上的原因，这两个图总是互为镜像，且在 f 和 -f 处有相同的峰值，如下图所示。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/fourier2.gif" alt=""></p><p>我目前展示的仅是对于一维信号的傅里叶变换。对于二维信号，比如一副图像，二维傅里叶变换会对图像的每一行进行一维傅里叶变换，且对每一列也进行一维傅里叶变换，最终得到一副和原始图像大小相同的图像。</p><p>下图展示了一副正弦图像以及对其施加二维傅里叶变换后的傅里叶图像。傅里叶图像中的每个像素点表示空间频率值，该值的大小由该像素点的亮度决定。图像中心有一个很亮的像素，这是 DC 项，在其两侧还有两个亮像素。傅里叶图像中的像素越亮，说明原始图像的对比度越大。由于在这幅简单的图像中只有一个傅里叶分量，傅里叶图像其余位置的像素值为 0，被绘制为黑色。</p><div class="table-container"><table><thead><tr><th>Brightness Image</th><th>Fourier transform</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3real.gif" alt=""></td></tr></tbody></table></div><p>下面是另一个例子，与上例的区别是原始图像的空间频率更低，傅里叶图像中仍然有三个亮像素点，只不过它们距离更近，意味着更低的空间频率。</p><div class="table-container"><table><thead><tr><th>Brightness Image</th><th>Fourier transform</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1real.gif" alt=""></td></tr></tbody></table></div><p>需要注意的是，傅里叶图像与原始图像包含的信息完全相同，只不过其使幅度作为空间频率的函数，而不是使亮度作为空间位移的函数。对傅里叶图像进行傅里叶反变换将得到和原图一模一样的图像。</p><p>正弦信号的方向与傅里叶图像中峰值相对于中心 DC 点的方向有关。在下图中，倾斜的正弦信号将得到倾斜的傅里叶图像。</p><div class="table-container"><table><thead><tr><th>Brightness Image</th><th>Fourier transform</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3a.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3areal.gif" alt=""></td></tr></tbody></table></div><p>不同的傅里叶系数可以通过叠加组合来产生组合模式。比如下图中的正弦图像是由倾斜的正弦图像和垂直的正弦图像相加计算得到的。</p><div class="table-container"><table><thead><tr><th>Brightness Image</th><th>Fourier transform</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3b.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3breal.gif" alt=""></td></tr></tbody></table></div><p>原始图像与傅里叶图像完全可相互转化，因为它们包含完全相同的信息。上左图可以通过倾斜的正弦图像和垂直的正弦图像逐像素叠加得到，也可以先将二者的傅里叶图像逐像素叠加，再通过傅里叶反变换得到。两种方法得到的结果完全相同。</p><h2 id="高次谐波与“振铃”现象"><a href="#高次谐波与“振铃”现象" class="headerlink" title="高次谐波与“振铃”现象"></a>高次谐波与“振铃”现象</h2><p>傅里叶变换的基向量都是平滑的正弦函数，非常适合用来表达平滑的圆形。但实际上傅里叶变换可以用来表示任何形状，甚至是有直线边界的形状，这很难用傅里叶代码来表示，因为它们需要很多高阶项或高谐波。下面我们用例子来阐述这些“方波”如何用平滑的正弦曲线来表示。</p><p>下面列出了空间频率分别为 1、3、5、7 的五幅正弦图像。其中第一幅图像可以作为基波，另外三幅是基于该基波的高谐波，因为它们的频率是基波频率的整数倍。实际上这些都是基波上的奇次谐波，它们图像的中心都有一条明亮的垂直带。这些图像的傅里叶图像也如下所示。</p><div class="table-container"><table><thead><tr><th>1</th><th>3</th><th>5</th><th>7</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin5.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin7.gif" alt=""></td></tr><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1real.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin3real.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin5real.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin7real.gif" alt=""></td></tr></tbody></table></div><p>下面这张表列出了逐次将高次谐波叠加到基波上的结果。注意中央垂直光带逐渐变得越来越尖锐和突出，而背景部分亮度逐渐下降到均匀暗场。同时注意高次谐波如何在从基波向外扩散的傅里叶图像中产生峰值的，这在频域中定义了周期性的模式。</p><div class="table-container"><table><thead><tr><th>1</th><th>1+3</th><th>1+3+5</th><th>1+3+5+7</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sinsum13.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sinsum135.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sinsum1357.gif" alt=""></td></tr><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sin1real.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sinsum13real.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sinsum135real.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/sinsum1357real.gif" alt=""></td></tr></tbody></table></div><p>下面的图像展示了如果我们一直持续这个过程直到奈奎斯特频率会发生什么——这就会使图像中心产生一条细直的光纹，比如沿 x 方向的亮方波。该图像的傅里叶变换呈现出“无限”的一系列谐波或高阶项，不过由于原图像的分辨率是有限的，这里的“无限”要打上引号。总之，这就是如何用傅里叶变换将一系列平滑的正弦波叠加为方波。</p><div class="table-container"><table><thead><tr><th>Brightness Image</th><th>Fourier transform</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/stripe.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/stripereal.gif" alt=""></td></tr></tbody></table></div><h2 id="光学傅里叶变换"><a href="#光学傅里叶变换" class="headerlink" title="光学傅里叶变换"></a>光学傅里叶变换</h2><p>用一个棱镜也可以实现傅里叶变换，下面这个例子将让你更好理解傅里叶变换的原理。把一张图片放置到棱镜的焦距处，并用相干光对其照射，比如矢量激光束。在棱镜的另一个焦距处放置一个磨砂玻璃屏。然后这个棱镜就会自动对这张图片实施傅里叶变换，并将结果投影到磨砂玻璃屏上。比如如果这张图像是正弦光栅，如下图所示，那么傅里叶图像的中心将会有一个亮点——DC 项，其两侧各一个亮点，中心亮点与两侧亮点的距离会随着正弦光栅的空间频率的变化而变化。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/fourier3.gif" alt=""></p><p>我们现在来看看傅里叶变换背后的整体原理。输入图像的每一点都会向棱镜投射一个圆锥光束，由于输入图像位于棱镜的焦距处，这些光束会被棱镜折射成平行光束，投射到磨砂玻璃屏上。换句话说，输入图像上的每个点都均匀分布在傅里叶图像上，其中相长干涉和相消干涉将自行产生合适的傅里叶表示。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/fourier4.gif" alt=""></p><p>相反，来自输入图像的平行光束将通过棱镜而聚焦到傅里叶图像的中心，这就是 DC 项为输入图像平均亮度的原因。</p><p><img src="http://cns-alumni.bu.edu/~slehar/fourier/fourier5.gif" alt=""></p><p>光学傅里叶变换也可自动实施傅里叶反变换，使傅里叶图像变回原图像。在数学上，正向变换和反向变换只相差一个减号。</p><h2 id="傅里叶滤波"><a href="#傅里叶滤波" class="headerlink" title="傅里叶滤波"></a>傅里叶滤波</h2><p>现在我将展示如何利用傅里叶变换来实施滤波操作，这可以调整一副图像的空间频率。我们以下面这幅图像为例，先对其实施傅里叶变换，再实施傅里叶反变换。傅里叶反变换后得到的图像与原图像在每个像素点上的值都是一模一样的。</p><div class="table-container"><table><thead><tr><th>Brightness Image</th><th>Fourier Transform</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/lehar.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/leharreal.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/lehar.gif" alt=""></td></tr></tbody></table></div><p>现在我将阐述如何对变换后的图像进行操作以调整其空间频率，然后通过反变换得到进行滤波后的图像。我们以低通滤波为例，所谓低通，即只运行低频部分通过，而截断高频部分。由于低频部分位于 DC 点附近，我们只需要以 DC 点为中心定义一个半径范围，将所有处于范围外的像素值置为零。换句话说，低通滤波操作相当于傅里叶变换的中心部分，而将其余部分的值置为零。对低通滤波处理后的图像进行傅里叶反变换后的图像如下所示。</p><div class="table-container"><table><thead><tr><th>Low-Pass Filtered</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/real-lp.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/filt-lp.gif" alt=""></td></tr></tbody></table></div><p>我们可以看到，低通滤波后的图像变得模糊了，这是因为低通滤波保留了图像中暗与亮的低频平滑部分，丢弃了清晰的轮廓与边缘。在数学上，低通滤波相当于模糊函数。</p><p>下面我们进行一个相反地操作，即高通滤波，我们将对傅里叶图像使用相同频率阈值。所有低于该阈值的频率将被丢弃，只有高于该阈值的频率才会得到保留。再实施傅里叶反变换后，我们看到了高通滤波的效果，这使得图像中的清晰轮廓和边缘得到了保留，而大部分暗与亮的部分信息被丢弃了。</p><div class="table-container"><table><thead><tr><th>High-Pass Filtered</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/real-hp.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/filt-hp.gif" alt=""></td></tr></tbody></table></div><p>如果将低通滤波后的图像的每个像素值都加到高通滤波后的图像的相应位置，我们将得到滤波前的图像。因此这两张图像是互补关系，彼此携带着对方缺失的信息。</p><p>接下来我们演示一种带通滤波，它仅保留落在某个频带内的空间频率，该部分频率大于低截止频率，低于高截止频率。</p><div class="table-container"><table><thead><tr><th>Band-Pass Filtered</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/real-bp1.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/filt-bp1.gif" alt=""></td></tr></tbody></table></div><p>下个模拟与上个相同，只不过其空间频率范围更小。</p><div class="table-container"><table><thead><tr><th>Band-Pass Filtered</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/real-bp2.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/filt-bp2.gif" alt=""></td></tr></tbody></table></div><p>下个模拟为更高空间频率的带通滤波。</p><div class="table-container"><table><thead><tr><th>Band-Pass Filtered</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/real-bp3.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/filt-bp3.gif" alt=""></td></tr></tbody></table></div><p>最后这个模拟与上个相同，只不过频带范围更窄。</p><div class="table-container"><table><thead><tr><th>Band-Pass Filtered</th><th>Inverse Transformed</th></tr></thead><tbody><tr><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/real-bp4.gif" alt=""></td><td><img src="http://cns-alumni.bu.edu/~slehar/fourier/filt-bp4.gif" alt=""></td></tr></tbody></table></div><p>上述模拟结果表明，傅里叶表示可以通过整体分布式方式对图像的信息进行编码，再通过对变换后的频域图像进行空间操作，来处理图像的全局信息内容。</p>]]></content>
      
      
      <categories>
          
          <category> 数字图像处理 </category>
          
          <category> 傅里叶变换 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字图像处理 </tag>
            
            <tag> 译文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python3 实现火车票查询工具</title>
      <link href="2017/11/10/2017/Python3%20%E5%AE%9E%E7%8E%B0%E7%81%AB%E8%BD%A6%E7%A5%A8%E6%9F%A5%E8%AF%A2%E5%B7%A5%E5%85%B7/"/>
      <url>2017/11/10/2017/Python3%20%E5%AE%9E%E7%8E%B0%E7%81%AB%E8%BD%A6%E7%A5%A8%E6%9F%A5%E8%AF%A2%E5%B7%A5%E5%85%B7/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前几天看了一个爬取12306来获得火车票信息的教程，发现12306官网的存储车票信息的 Json 数据格式已经变了，导致这篇教程的代码已经没法继续使用了，因此我针对新的格式重新进行了解析，最后达到了目的。在此记录一下整个过程。</p></blockquote><p>先看一下最终效果吧</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-cfe43fa7204b40b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="最终效果"></p><a id="more"></a><p>只需要输入查询细节，就可以输出你想查询的车票信息，而且界面一目了然。</p><h3 id="接口设计"><a href="#接口设计" class="headerlink" title="接口设计"></a>接口设计</h3><p>用户在使用这个工具的时候，需要输入1.车次类型2.始发站3.终点站以及4.日期。火车有很多类型，可以大致分为如下几种：</p><ul><li>-g 高铁  </li><li>-d 动车  </li><li>-t 特快  </li><li>-k 快车  </li><li>-z 直达 </li></ul><p>我们需要的接口就是刚刚提到的 4 种，因此接口看起来应该是这个样子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python tickets.py [-gdtkz] <span class="keyword">from</span> to date</span><br></pre></td></tr></table></figure><p>其中，<code>tickets.py</code> 是这个程序的名字，<code>-gdtkz</code> 是车次类型，<code>from</code> 是始发站，<code>to</code> 是终点站，<code>date</code> 是日期，用户在使用时需要填入这几个信息。</p><h3 id="需要的库"><a href="#需要的库" class="headerlink" title="需要的库"></a>需要的库</h3><ul><li><code>requests</code> 使用 Python 访问 HTTP 资源</li><li><code>docopt</code> Python3 命令行解析工具</li><li><code>prettytable</code> 格式化信息打印工具，见过过 MySQL 打印数据的界面吧</li><li><code>colorama</code> 命令行着色工具</li></ul><p>最方便的下载方式还是<code>pip</code>，如果觉得<code>pip</code>的下载速度太慢可以参考这篇文章解决：<a href="http://www.liuhdme.com/2017/11/07/mac%20%E4%B8%8B%E6%9B%B4%E6%8D%A2%20pip%20%E6%BA%90/">更换 pip 源</a></p><h3 id="解析参数"><a href="#解析参数" class="headerlink" title="解析参数"></a>解析参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;命令行火车票查看器</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Usage:</span></span><br><span class="line"><span class="string">    tickets [-gdtkz] &lt;from&gt; &lt;to&gt; &lt;date&gt;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Options:</span></span><br><span class="line"><span class="string">    -h,--help   显示帮助菜单</span></span><br><span class="line"><span class="string">    -g          高铁</span></span><br><span class="line"><span class="string">    -d          动车</span></span><br><span class="line"><span class="string">    -t          特快</span></span><br><span class="line"><span class="string">    -k          快速</span></span><br><span class="line"><span class="string">    -z          直达</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Example:</span></span><br><span class="line"><span class="string">    tickets 武汉 上海 2017-11-20</span></span><br><span class="line"><span class="string">    tickets -dg 北京 南京 2017-11-20</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> docopt <span class="keyword">import</span> docopt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cli</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;command-line interface&quot;&quot;&quot;</span></span><br><span class="line">    arguments = docopt(__doc__)</span><br><span class="line">    print(arguments)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    cli()</span><br></pre></td></tr></table></figure><p>上面的程序中，<code>docopt</code>会根据我们在程序开头定义的格式自动解析出参数并返回一个字典，也就是<code>arguments</code>，然后打印出这个字典的内容。</p><p>运行一下这个程序，比如查询一下11月20号从武汉到十堰的动车和快车，可以得到解析的结果如下所示，这和我们的接口是对应的</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-97037c5b7b319507.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="演示"></p><h3 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h3><p>整个过程的关键是从 12306 获取数据和解析数据。</p><p>打开 12306 官网，点击“余票查询”，进入如下网页</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-7d95e4bc2a657e1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="余票查询"></p><p>随便查询一下车票，比如我查一下 11 月 20 号从武汉到十堰的票，如图</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-656c6911c9d41f84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="随便查询"></p><p>然后进入开发者模式下的 Network 页面，如图所示（我的浏览器是 Chrome，不同浏览器的进入方法可能不一样，不清楚的可以百度）</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-1466a5812369bcef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="开发者模式-Network"></p><p>再点击一次查询按钮，会发现 Network 页面有所变化，点击如图所示的项目，然后进入右边显示的 Request URL</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-679bffb4a06cc643.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="URL"></p><p>你看到应该是如下图所示的一团杂乱无章的数据</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-f180ac9a77d91911.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="杂乱无章的数据"></p><p>其实这是 Json 格式的数据，里面其实保存了我们查询的车次的所有车票的信息，我们的任务就是想办法把它们提取出来并显示出来。</p><p>我们先看看刚才的 URL：</p><p><a href="https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2017-11-20&amp;leftTicketDTO.from_station=WHN&amp;leftTicketDTO.to_station=SNN&amp;purpose_codes=ADULT">https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=2017-11-20&amp;leftTicketDTO.from_station=WHN&amp;leftTicketDTO.to_station=SNN&amp;purpose_codes=ADULT</a></p><p>不难发现几个关键信息：</p><ul><li><code>train_date=2017-11-20</code> 这是我刚才查询的日期</li><li><code>from_station=WHN</code> 这是始发站</li><li><code>to_station=SNN</code> 这是终点站</li></ul><p>其中始发站和终点站的名字是用大写字母组成的代号代替的，然而用户输入的是汉字，我们需要找到汉字和代号的对应关系。查看一下网页的源代码，搜索 station_version 关键字，找到如下位置</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-2a1419baa312a97d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="station_version"></p><p>复制 src 中的链接，并在前面加上 12306 的一级域名，即 <a href="https://kyfw.12306.cn/otn/resources/js/framework/station_name.js?station_version=1.9030">https://kyfw.12306.cn/otn/resources/js/framework/station_name.js?station_version=1.9030</a></p><p>打开这个链接，你会发现一个惊喜</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-fa21fade771a4478.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="station_version"></p><p>这里面存储了全国的城市代号，接下来我们写一个脚本，把城市和代号以字典的形式存入一个 Python 文件</p><p>新建 <code>parse_station.py</code> 文件，并写入以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint </span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://kyfw.12306.cn/otn/resources/js/framework/station_name.js?station_version=1.8971&#x27;</span></span><br><span class="line">response = requests.get(url, verify=<span class="literal">False</span>)</span><br><span class="line">stations = re.findall(<span class="string">u&#x27;([\u4e00-\u9fa5]+)\|([A-Z]+)&#x27;</span>, response.text)</span><br><span class="line">pprint(<span class="built_in">dict</span>(stations), indent=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>这里用到了正则表达式，通过正则表达式把所有汉字和后面紧跟着的字母解析出来。</p><p>运行这个脚本，它将以字典的形式返回所有车站和代号, 并将结果保存到到 <code>stations.py</code> 文件中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python3 parse_station.py &gt; stations.py</span><br></pre></td></tr></table></figure><p>打开<code>stations.py</code>文件，看起来是这样的（因为这个字典没有名字，所以 Pycharm 发出了 warning，所以界面看起来黄黄的…）</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-e78c1e6a7e35f218.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="stations.py"></p><p>给这个字典命名为 stations，最终<code>stations.py</code>看起来是这样的</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-25e166a44cfa3617.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="stations.py"></p><p>现在，用户输入车站的中文名，我们就可以直接从这个字典中获取它的字母代码了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">from</span> stations <span class="keyword">import</span> stations</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cli</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;command-line interface&quot;&quot;&quot;</span></span><br><span class="line">    arguments = docopt(__doc__)</span><br><span class="line">    from_station = stations.get(arguments[<span class="string">&#x27;&lt;from&gt;&#x27;</span>])</span><br><span class="line">    to_station = stations.get(arguments[<span class="string">&#x27;&lt;to&gt;&#x27;</span>])</span><br><span class="line">    date = arguments[<span class="string">&#x27;&lt;date&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 构建 URL</span></span><br><span class="line">    url = <span class="string">&#x27;https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=&#123;&#125;&amp;leftTicketDTO.from_station=&#x27;</span> \</span><br><span class="line">          <span class="string">&#x27;&#123;&#125;&amp;leftTicketDTO.to_station=&#123;&#125;&amp;purpose_codes=ADULT&#x27;</span>.<span class="built_in">format</span>(date, from_station, to_station)</span><br></pre></td></tr></table></figure><p>回想一下我们的最终目的是从 Json 数据中解析出车票的信息，我们先向存储 Json 数据的 URL 发送请求：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cli</span>():</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment"># 添加verify=False参数不验证证书</span></span><br><span class="line">    r = requests.get(url, verify=<span class="literal">False</span>)</span><br><span class="line">    print(r.json())</span><br></pre></td></tr></table></figure><p>这里打印出了 Json 数据，的确是杂乱无章的，下一步就进行解析。</p><h3 id="解析数据"><a href="#解析数据" class="headerlink" title="解析数据"></a>解析数据</h3><p>仔细观察和对比 Json 数据和 12306 网站上显示的车票信息，可以发现所有的车票信息都存储在 <code>r.json()[&quot;data&quot;][&quot;result&quot;]</code> 下，并且存储的形式是 Python 中的列表，一个车次对应列表中的一个元素，这个元素是一个特别长的字符串，但是里面却有我们需要的所有信息，包括始发站，终点站，开车时间，到达时间，总时间，以及各个座位的车票是否有剩余，下面用红框框住的是其中一个车次的数据</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-8a1e7be46fded0f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="json"></p><p>这里面除了两段很长的貌似没有意义的字符串，剩余的信息都用 <code>|</code> 隔开了，剩下的工作就是遍历这个列表里的所有元素，并针对每个元素进行解析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainsCollection</span>:</span></span><br><span class="line"></span><br><span class="line">    header = <span class="string">&#x27;车次 车站 时间 历时 商务特等座 一等座 二等座 高级软卧 软卧 硬卧 硬座 无座&#x27;</span>.split()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, available_trains, station_map, options</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;查询到的火车班次集合</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param available_trains: 一个列表, 包含着所有车次的信息</span></span><br><span class="line"><span class="string">        :param station_map: 一个字典，包含不同代号对应的站点</span></span><br><span class="line"><span class="string">        :param options: 查询的选项, 如高铁, 动车, etc...</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.available_trains = available_trains</span><br><span class="line">        self.station_map = station_map</span><br><span class="line">        self.options = options</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">geturation</span>(<span class="params">self, duration</span>):</span></span><br><span class="line">        duration = duration.replace(<span class="string">&#x27;:&#x27;</span>, <span class="string">&#x27;小时&#x27;</span>) + <span class="string">&#x27;分&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> duration.startswith(<span class="string">&#x27;00&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> duration[<span class="number">4</span>:]</span><br><span class="line">        <span class="keyword">if</span> duration.startswith(<span class="string">&#x27;0&#x27;</span>):</span><br><span class="line">            <span class="keyword">return</span> duration[<span class="number">1</span>:]</span><br><span class="line">        <span class="keyword">return</span> duration</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">trains</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> raw_train <span class="keyword">in</span> self.available_trains:</span><br><span class="line">            <span class="comment"># 利用正则表达式得到列车的类型</span></span><br><span class="line">            train_type = re.findall(<span class="string">&#x27;[\u4e00-\u9fa5]+\|\w+\|(\w)&#x27;</span>, raw_train)[<span class="number">0</span>].lower()</span><br><span class="line">            <span class="keyword">if</span> train_type <span class="keyword">in</span> self.options <span class="keyword">and</span> <span class="string">&#x27;售&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> raw_train <span class="keyword">and</span> <span class="string">&#x27;停运&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> raw_train:</span><br><span class="line">                station = re.findall(<span class="string">&#x27;(\w+)\|(\w+)\|\d+:&#x27;</span>, raw_train)[<span class="number">0</span>]    <span class="comment"># 元组，保存始发站和终点站的代号</span></span><br><span class="line">                s_station = station[<span class="number">0</span>]   <span class="comment"># 始发站的代号</span></span><br><span class="line">                e_station = station[<span class="number">1</span>]   <span class="comment"># 终点站的代号</span></span><br><span class="line">                train = [</span><br><span class="line">                    <span class="comment"># 车次</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;[\u4e00-\u9fa5]+\|\w+\|(\w+)&#x27;</span>, raw_train)[<span class="number">0</span>],</span><br><span class="line">                    <span class="comment"># 始发站和终点站</span></span><br><span class="line">                    <span class="string">&#x27;\n&#x27;</span>.join([Fore.MAGENTA+self.station_map[s_station]+Fore.RESET,</span><br><span class="line">                               Fore.BLUE+self.station_map[e_station]+Fore.RESET]),</span><br><span class="line">                    <span class="comment"># 发车时间和到站时间</span></span><br><span class="line">                    <span class="string">&#x27;\n&#x27;</span>.join([Fore.MAGENTA+re.findall(<span class="string">&#x27;\|(\d+:\d+)&#x27;</span>, raw_train)[<span class="number">0</span>]+Fore.RESET,</span><br><span class="line">                               Fore.BLUE+re.findall(<span class="string">&#x27;\|(\d+:\d+)&#x27;</span>, raw_train)[<span class="number">1</span>]+Fore.RESET]),</span><br><span class="line">                    self.geturation(re.findall(<span class="string">&#x27;\|(\d+:\d+)&#x27;</span>, raw_train)[-<span class="number">1</span>]),  <span class="comment"># 行驶总时间</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;18&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],  <span class="comment"># 商务特等座</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;17&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],  <span class="comment"># 一等座</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;16&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],  <span class="comment"># 二等座</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;7&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],   <span class="comment"># 高级软卧</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;9&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],   <span class="comment"># 软卧</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;14&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],  <span class="comment"># 硬卧</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;15&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>],  <span class="comment"># 硬座</span></span><br><span class="line">                    re.findall(<span class="string">&#x27;(\d)&#123;8&#125;\|(\w*\|)&#123;12&#125;(\w*)&#x27;</span>, raw_train)[<span class="number">0</span>][-<span class="number">1</span>]   <span class="comment"># 无座</span></span><br><span class="line">                ]</span><br><span class="line">                <span class="keyword">yield</span> train</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pretty_print</span>(<span class="params">self</span>):</span></span><br><span class="line">        pt = PrettyTable()</span><br><span class="line">        pt._set_field_names(self.header)</span><br><span class="line">        <span class="keyword">for</span> train <span class="keyword">in</span> self.trains:</span><br><span class="line">            pt.add_row(train)</span><br><span class="line">        print(pt)</span><br></pre></td></tr></table></figure><p>我们封装一个类专门用来解析数据，这个类对传来的列表进行遍历，并用正则表达式解析每一个元素，然后把这些信息存储在列表<code>train</code>中，最后再通过<code>prettytable</code>库将所有信息有序的打印出来。</p><blockquote><p>在原教程中，车票的信息是存储在 12306 网站中的字典里的，因此解析十分方便，然而后来 12306 将车票信息的存储格式改为了列表，使得信息的提取变难了，但是只要将正则表达式正确运用，依然可以解析出我们想要的信息，只不过比字典要麻烦一些而已。</p></blockquote><h3 id="显示结果"><a href="#显示结果" class="headerlink" title="显示结果"></a>显示结果</h3><p>最后，我们将上述过程进行汇总并将结果输出到屏幕上：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cli</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;command-line interface&quot;&quot;&quot;</span></span><br><span class="line">    arguments = docopt(__doc__)</span><br><span class="line">    from_station = stations.get(arguments[<span class="string">&#x27;&lt;from&gt;&#x27;</span>])</span><br><span class="line">    to_station = stations.get(arguments[<span class="string">&#x27;&lt;to&gt;&#x27;</span>])</span><br><span class="line">    date = arguments[<span class="string">&#x27;&lt;date&gt;&#x27;</span>]</span><br><span class="line">    <span class="comment"># 构建 URL</span></span><br><span class="line">    url = <span class="string">&#x27;https://kyfw.12306.cn/otn/leftTicket/query?leftTicketDTO.train_date=&#123;&#125;&amp;leftTicketDTO.from_station=&#x27;</span> \</span><br><span class="line">          <span class="string">&#x27;&#123;&#125;&amp;leftTicketDTO.to_station=&#123;&#125;&amp;purpose_codes=ADULT&#x27;</span>.<span class="built_in">format</span>(date, from_station, to_station)</span><br><span class="line">    options = <span class="string">&#x27;&#x27;</span>.join([</span><br><span class="line">        key <span class="keyword">for</span> key, value <span class="keyword">in</span> arguments.items() <span class="keyword">if</span> value <span class="keyword">is</span> <span class="literal">True</span></span><br><span class="line">    ])</span><br><span class="line">    r = requests.get(url, verify=<span class="literal">False</span>)</span><br><span class="line">    available_trains = r.json()[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;result&#x27;</span>]</span><br><span class="line">    station_map = r.json()[<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;map&#x27;</span>]</span><br><span class="line">    TrainsCollection(available_trains, station_map, options).pretty_print()</span><br></pre></td></tr></table></figure><p>其中，我们通过<code>colorama</code>库为站点和时间信息添加了颜色，使结果看起来更加舒服。</p><h3 id="全部代码"><a href="#全部代码" class="headerlink" title="全部代码"></a>全部代码</h3><p>由于<code>stations.py</code>中的字典很长，所以就不在这里将所有代码贴出来了，感兴趣的可以到 Github 上下载查看：<a href="https://github.com/LiuHDme/PythonLearning/tree/master/2-tickets">Python3 实现火车票查询工具</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> 爬虫 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数字图像处理考试重点</title>
      <link href="2017/11/08/2017/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9/"/>
      <url>2017/11/08/2017/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E9%87%8D%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p><strong>有损压缩：</strong>牺牲图像复原的准确度以换取压缩能力的增加，如果产生的失真可以容忍，则压缩能力的增加是有效的</p><p><strong>无损压缩：</strong>在压缩后不丢失信息，即对压缩的图像解码后可以不不失真地恢复原图像</p><p><strong>保真度准则：</strong>需要评价信息损失的测度以描述解码图像相 对于原始图像的偏离程度，这些测度称为保真度准则</p><p><strong>开操作：</strong>使图像的轮廓变得光滑，断开狭窄的间断和消除细的突出物（先用B对A腐蚀，然后用B对结果膨胀）A°B=(A-B)+B</p><p><strong>闭操作：</strong>同样使图像的轮廓变得光滑，但与开操作相反，它能消除狭窄的间断和长细的鸿沟，消除小的孔洞，并填补轮廓线中的裂痕（先用B对A膨胀，然后用B对结果腐蚀）A·B=(A+B)-B</p><p><strong>击中击不中变换：</strong>形状检测的一个基本工具</p><p><strong>拉普拉斯算子优缺点：</strong></p><p><strong>缺点：</strong><br>拉普拉斯算子对噪声具有敏感性；拉普拉斯算子的幅值产生双边缘；拉普拉斯算子不能检测边缘的方向</p><p><strong>优点：</strong><br>可以利用零交叉的性质进行边缘定位；可以确定一个像素是在边缘暗的一边还是亮的一边</p><p><strong>阈值处理算法：</strong></p><ol><li>选择一个T的初始估计值</li><li>用T分割图像，生成两组像素：G1由所有灰度值大 于T的    像素组成，而G2由所有灰度值小于或等于T的 像素组成</li><li>对区域G1和G2中的所有像素计算平均灰度值µ1和µ2</li><li>计算新的阈值 T = 1/2 (µ1 + µ2)</li><li>重复步骤2到4，直到逐次迭代所得的T值之差小于事先定义的参数ΔT</li></ol><p><strong>区域增长算法：</strong></p><ol><li>根据图像的不同应用选择一个或一组种子，它或者是最亮或最暗的点，或者是位于点簇中心的点</li><li>选择一个描述符（条件）</li><li>从该种子开始向外扩张，首先把种子像素 加入结果集合，然后不断将与集合中各个 像素连通、且满足描述符的像素加入集合</li><li>上一过程进行到不再有满足条件的新结点 加入集合为止</li></ol>]]></content>
      
      
      <categories>
          
          <category> 考试 </category>
          
          <category> 数字图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 考试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>更换 pip 源解决下载速度过慢问题</title>
      <link href="2017/11/07/2017/%E6%9B%B4%E6%8D%A2%20pip%20%E6%BA%90%E8%A7%A3%E5%86%B3%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E8%BF%87%E6%85%A2%E9%97%AE%E9%A2%98/"/>
      <url>2017/11/07/2017/%E6%9B%B4%E6%8D%A2%20pip%20%E6%BA%90%E8%A7%A3%E5%86%B3%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E8%BF%87%E6%85%A2%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>在使用 Python 的过程中，免不了要用 pip 安装各种模块，但是由于官方 Pypi 经常被墙，导致下载速度过慢甚至出错，最好的办法是将自己使用 pip 源更换一下。以下是几个常用的国内源：</p><blockquote><p>阿里云<br><a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a>  </p><p>中国科技大学<br><a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a>  </p><p>豆瓣<br><a href="http://pypi.douban.com/simple/">http://pypi.douban.com/simple/</a>  </p><p>清华大学<br><a href="https://pypi.tuna.tsinghua.edu.cn/simple/">https://pypi.tuna.tsinghua.edu.cn/simple/</a>  </p><p>中国科学技术大学<br><a href="http://pypi.mirrors.ustc.edu.cn/simple/">http://pypi.mirrors.ustc.edu.cn/simple/</a></p></blockquote><p>以下是更换方法：</p><h3 id="临时使用"><a href="#临时使用" class="headerlink" title="临时使用"></a>临时使用</h3><p>在安装语句最后加上 -i 以及相应 URL 即可，比如：</p><p><code>pip install numpy -i https://pypi.tuna.tsinghua.edu.cn/simple/</code></p><h3 id="一劳永逸"><a href="#一劳永逸" class="headerlink" title="一劳永逸"></a>一劳永逸</h3><p>进入目录 ~/.pip，编辑 pip.conf 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[global]  </span><br><span class="line">index-url &#x3D; http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;pypi&#x2F;simple&#x2F;</span><br></pre></td></tr></table></figure><p>最后面跟的是你选择的源，我这里是以阿里云的为例。</p><p>如果你是 Mac 用户，那么你可能没有 .pip 文件夹，那就新建一个，然后再新建 pip.conf 文件，即在终端中分别输入以下三个语句，之后再将上面的内容写进 pip.conf 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir .pip  </span><br><span class="line">cd .pip  </span><br><span class="line">touch pip.conf</span><br></pre></td></tr></table></figure><p>现在你可以体验一下下载速度了。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Tips </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>canvas 在项目中的运用</title>
      <link href="2017/10/20/2017/canvas%20%E4%BD%BF%E7%94%A8/"/>
      <url>2017/10/20/2017/canvas%20%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="基本介绍"><a href="#基本介绍" class="headerlink" title="基本介绍"></a>基本介绍</h2><p>关于 canvas 的基本使用，可以参考以下两个网站：</p><p><a href="http://blog.csdn.net/iispring/article/details/49770651">Android Canvas绘图详解(图文) - 泡在网上的日子</a></p><p><a href="http://www.jcodecraeer.com/a/anzhuokaifa/androidkaifa/2012/1212/703.html">Android中Canvas绘图基础详解(附源码下载) - CSDN博客</a></p><p>这里主要讲解如何将 canvas 实际运用到我们的项目中。</p><a id="more"></a><h2 id="手势控制"><a href="#手势控制" class="headerlink" title="手势控制"></a>手势控制</h2><p>canvas 没有提供有关手势缩放的功能，但我们可以利用 onTouchListener 来监测手势，并根据手势的不同对扫描图作不同处理，比如移动和缩放。首先，让绘制图形的这个类继承一个接口 —— View.OnTouchListener，然后再实现该接口中的 onTouch 方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="comment">// 实现接口 View.OnTouchListener 的 onTouch 方法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">onTouch</span><span class="params">(View v, MotionEvent event)</span> </span>&#123;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>只要有手指触碰到绘制的图形，就会触发 onTouch 方法，因此我们只要可以监测到触碰到图形的手指正在进行什么动作，就可以对图形做相应的处理。比如，如果 onTouch 监测到有一根手指从屏幕的左边滑到了右边，那么说明图形应该向右移，如果 onTouch 监测到有两根手指触碰到了屏幕，并且它们的距离在不断减小，那很显然，图形应该被缩小。可是，手指的动作这么灵活，该怎么监测呢？下面我们就来解决这个问题。</p><p>无论是什么动作，手指肯定需要先触碰到屏幕，最后再离开屏幕，这样才能完成一整个动作。Android 提供了一个方法来专门监测这两个动作以及更多的动作：</p><blockquote><p><code>event.getAction()</code><br><small><i>（event 是 onTouch 方法的第二个参数）</i></small></p></blockquote><p><code>getAction()</code> 会返回一个 <code>int</code> 型的值，不同的动作对应着不同的值，比如手指按下对应 0，手指抬起对应 1 等等。当然，这么多动作和值，我们不可能全记得，好在 Android 将不同的值都取了一个名字并保存在 MotionEvent 类中，比如</p><blockquote><p><code>MotionEvent.ACTION_DOWN = 0</code><br><code>MotionEvent.ACTION_UP = 1</code><br><code>MotionEvent.MOVE = 2</code><br>…</p></blockquote><p>既然这么方便，我们就可以通过 switch-case 结构来精准监测不同的动作了，看一下下面的代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">onTouch</span><span class="params">(View v, MotionEvent event)</span> </span>&#123;</span><br><span class="line"><span class="keyword">switch</span> (event.getAction()) &#123;</span><br><span class="line"><span class="comment">// 手指按下</span></span><br><span class="line"><span class="keyword">case</span> MotionEvent.ACTION_DOWN:</span><br><span class="line"><span class="comment">// ...针对该动作，对图形作出处理</span></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="comment">// 最后一根手指抬起</span></span><br><span class="line"><span class="keyword">case</span> MotionEvent.ACTION_UP:</span><br><span class="line"><span class="comment">// ...针对该动作，对图形作出处理</span></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="comment">// 手指移动</span></span><br><span class="line"><span class="keyword">case</span> MotionEvent.ACTION_MOVE:</span><br><span class="line"><span class="comment">// ...针对该动作，对图形作出处理</span></span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"><span class="comment">// ...更多的动作</span></span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>onTouch</code>方法 通过 <code>event.getAction()</code> 获取到的值，自动判断执行哪一个 case 中的代码，即通过监测不同的动作来对图形作出相应处理。我们的处理主要就是移动和缩放，那么下面分别介绍这两方面该如何处理。</p><h3 id="移动"><a href="#移动" class="headerlink" title="移动"></a>移动</h3><p>Android 提供了两个方法 <code>event.getX()</code> 和 <code>event.getY()</code>，这两个方法可以获取到当前手指在屏幕上的坐标值，那么只要将当前的坐标值减去之前的坐标值就可以得到手指在 x 和 y 方向分别移动了多少，再让图形移动这么多就可以了。下面是具体步骤：</p><ol><li><p>我们先在绘制图形类中新增两个 float 型成员变量 <code>xDown</code> 和 <code>yDown</code>，用来分别记录手指当前的 x 坐标和 y 坐标。</p></li><li><p>在 <code>onTouch</code> 方法中的 switch-case 结构中的 <code>MotionEvent.ACTION_DOWN</code> case 中，记录下手指刚按下时的坐标：  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xDown = event.getX();</span><br><span class="line">yDown = event.getY();</span><br></pre></td></tr></table></figure><p>（只有手指刚按下去的一刻才会触发<code>MotionEvent.ACTION_DOWN</code>中的代码）</p></li><li><p>在 <code>onTouch</code> 方法中的 switch-case 结构中的 <code>MotionEvent.ACTION_MOVE</code> case 中，动态更新每次手指移动的坐标距离：  </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">xTranslate += (event.getX() - xDown) / xScale;</span><br><span class="line">xDown = event.getX();</span><br><span class="line">yTranslate += (event.getY() - yDown) / yScale;</span><br><span class="line">yDown = event.getY();</span><br></pre></td></tr></table></figure><p>稍微解释一下，手指每移动一小距离都会执行以上代码，其中 <code>xTranslate</code> 和 <code>yTranslate</code> 是用来控制图形移动的，初始值是 0，只要它们的值变化了，图形就会移动；<code>xScale</code> 和 <code>yScale</code> 是用来控制图形缩放的，初始值是 1，只要它们的值变化了，图形就会缩放。拿 <code>xTranslate</code> 来说，手指每移动一小距离，都把当前手指的 x 坐标值减去移动之前的 x 坐标值，然后除以当前缩放的比例，再把这个值赋给 <code>xTranslate</code>，这时图形就会移动相应的距离，并且移动的距离和你手指移动的距离完全相等。需要注意的是，在手指移动的过程中，需要不断的把当前手指的 x 坐标值赋给 <code>xDown</code>，即 <code>xDown = event,getX()</code>，因为 <code>event.getX()</code> 的值始终比 <code>xDown</code> 先变化，这样就能保证它们之间始终有一个微小的差值，这个差值就是图形每次移动的那一点微小的距离，因为距离实在太小，所以整个过程看起来就是连续移动了。简而言之，图形的一整段移动是由无数段微小的移动组成的。</p></li><li><p>加上当前手指数目的判断。因为当手指移动时，可能是一根手指也可能是两根手指，如果是两根手指，要实现的功能就是缩放而不是移动了，因此需要加上手指数目的判断，这个很好完成，因为 Android 提供了一个方法来获取手指数目的方法：<code>event.getPointerCounter()</code>，这个方法可以直接返回当前触摸到屏幕的手指数目，然后通过 <code>if</code> 语句加入到 <code>MotionEvent.ACTION_MOVE</code> case 中就可以了，如果返回 1，就执行有关图形移动的代码，如果返回 2，就执行有关图形缩放的代码。</p></li></ol><h3 id="缩放"><a href="#缩放" class="headerlink" title="缩放"></a>缩放</h3><p>缩放的原理也很好理解。首先，要实现缩放，一定有两根手指触碰到屏幕，那么，我们可以获取当前两根手指的距离和之前两根手指的距离，然后算出比例，这个比例就是图形应该缩放的比例。比如之前手指间的距离是 1，现在是 2，那么图形应该被放大 \(\frac{2}{1}\) 即 2 倍。</p><p>下面来看具体步骤：</p><ol><li>我们先要获取两根手指触碰到屏幕时它们之间的距离。之前提到过，手指的每一个动作都对应着一个 <code>int</code> 型的值，两根手指触碰到屏幕这个动作对应的值是 261。然后我们可以通过 <code>event.getX(0)</code> 和 <code>event.getX(1)</code> 分别获取两根手指的坐标，然后相减即可得到两根手指在 x 轴方向的距离，同样的方法也能得到 y 轴方向的距离，然后这两个距离平方相加即可得到两根手指之间的距离，代码如下：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="number">261</span>:</span><br><span class="line"><span class="keyword">double</span> xLenDown = Math.abs(event.getX(<span class="number">0</span>) - event.getX(<span class="number">1</span>));</span><br><span class="line"><span class="keyword">double</span> yLenDown = Math.abs(event.getY(<span class="number">0</span>) - event.getY(<span class="number">1</span>));</span><br><span class="line">lenDown = Math.sqrt(xLenDown * xLenDown + yLenDown * yLenDown);</span><br><span class="line"><span class="keyword">break</span>;</span><br></pre></td></tr></table></figure></li><li>每次移动手指，都记录下当前手指间的距离，然后除以上次移动时手指间的距离，再减去 1，就得到了这次移动后图形应该缩放的比例，如果大于 0，图形就会放大，否则就会缩小，并且为了不让图形缩小到消失，加入一条 <code>if</code> 语句，设置最小缩放比例为 0.4。代码如下：<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (event.getPointerCount() == <span class="number">2</span>) &#123;</span><br><span class="line"><span class="comment">// 实现扫描图缩放</span></span><br><span class="line"><span class="keyword">double</span> xLenMove = Math.abs(event.getX(<span class="number">0</span>) - event.getX(<span class="number">1</span>));</span><br><span class="line"><span class="keyword">double</span> yLenMove = Math.abs(event.getY(<span class="number">0</span>) - event.getY(<span class="number">1</span>));</span><br><span class="line"><span class="keyword">double</span> lenMove = Math.sqrt(xLenMove * xLenMove + yLenMove * yLenMove);</span><br><span class="line"><span class="comment">// 动态更新</span></span><br><span class="line"><span class="comment">// 设置最小缩放比例为 0.4</span></span><br><span class="line"><span class="keyword">if</span> (xScale + (lenMove / lenDown - <span class="number">1</span>) &gt; <span class="number">0.4</span>) &#123;</span><br><span class="line">xScale += (lenMove / lenDown - <span class="number">1</span>);</span><br><span class="line">yScale += (lenMove / lenDown - <span class="number">1</span>);</span><br><span class="line">lenDown = lenMove;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="首页折线图和扫描图同步移动和缩放"><a href="#首页折线图和扫描图同步移动和缩放" class="headerlink" title="首页折线图和扫描图同步移动和缩放"></a>首页折线图和扫描图同步移动和缩放</h3></li></ol><p>这个功能的目的是，当折线图或者扫描图任何一者移动或者缩放时，另一者也要移动或缩放同样的距离或程度。其中，另一者只在横轴方向上保持同步移动，并且二者缩放时均以当前图形的中心点为缩放中心。</p><p>这个功能分为两个部分，一个是改变折线图的同时改变扫描图，一个是改变扫描图的同时改变折线图，先说简单的。</p><h4 id="改变折线图的同时改变扫描图"><a href="#改变折线图的同时改变扫描图" class="headerlink" title="改变折线图的同时改变扫描图"></a>改变折线图的同时改变扫描图</h4><p>如果上面的移动和缩放弄清楚了，那么这个功能其实不难实现。关键在于同步改变 <code>xTranslate</code> 和 <code>xScale</code>。</p><p>在 <code>FragmentDataMeasure</code> 类中，折线图的实例是 <code>mGraphicaView</code>，那么监控折线图的手势，当出现移动和缩放的手势时，同步更改扫描图中的 <code>xTranslate</code> 和 <code>xScale</code> ，另外在注意一些细节即可。这里就不在赘述了。</p><h4 id="改变扫描图的同时改变折线图"><a href="#改变扫描图的同时改变折线图" class="headerlink" title="改变扫描图的同时改变折线图"></a>改变扫描图的同时改变折线图</h4><p>这个功能的困难在于，虽然绘制折线图的库 <code>GraphicaView</code> 是以 canvas 为基础封装成的，但对于绘制图形的方法，两者有很大的区别，比如 canvas 在绘制图形时是直接根据给出的像素坐标值确定位置的，这个坐标值是基于屏幕自身的；而 <code>GraphicaView</code> 是根据对应于坐标轴上的坐标值确定位置的，这个坐标值是基于用户自己确定的坐标轴的长度的。要解决这个问题，需要找到折线图和扫描图的一个共同特征作为桥梁，将两种坐标值联系起来。</p><p>不过在研究 <code>GraphicaView</code> 库后发现，<code>GraphicaView</code> 类中提供了两个方法，可以分别获取和设置当前屏幕上显示出来的 x 轴的最小和最大坐标，即图中所示的两个位置的坐标</p><p><img src="http://oom3nz471.bkt.clouddn.com/107/3492A7CFE07BE410E7921D91111219B2.jpg" alt=""></p><p>有了这个方法，这个功能的实现就应该有思路了。我们先考虑移动时的同步。</p><h5 id="移动时同步"><a href="#移动时同步" class="headerlink" title="移动时同步"></a>移动时同步</h5><p>我们先考虑一下折线图和扫描图的共同特征是什么，由于两幅图在 x 轴方向上都显示的是扫描的距离，因此这个距离应该是相等的，这个距离就是共同特征。</p><p>在 <code>ScanningService</code> 类中，有一个 <code>xDistance</code> 属性，专门用来记录这个距离，而且，<code>xDistance</code> 的值与折线图中的 x 轴长度是相等的，如图所示：</p><p><img src="http://oom3nz471.bkt.clouddn.com/107/BE9E0224E0D8DDFB7C2B16AA7D410FCA.jpg" alt=""></p><p>图中折线图的红色箭头之间的距离大致为 0.35，扫描图的绿色箭头之间的距离也大致为 0.35，而 0.35 其实就是 <code>xDistance</code> 的值。</p><p>当移动扫描图时，由于我们现在可以获取到手指移动的距离 <code>xDistance</code>（注意这个距离是基于屏幕坐标系的，而不是折线图的坐标系），那么只要知道扫描图的 x 轴方向的总距离 <code>width</code>（基于屏幕坐标系），然后让 <code>xDistance</code> 除以 <code>width</code>，就得到了移动距离占总距离的比例，最后让这个比例乘以 <code>xDistance</code>，就得到了基于折线图坐标系的距离。Android 正好提供了一个方法 <code>canvas.getWidth()</code> 用来获取 x 轴方向的距离，因此三个值都有了，那么折线图移动的距离就可以算出来了，代码如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 同步折线图</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">syncGraphicalView</span><span class="params">(<span class="keyword">double</span> xTrans)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 更新折线图</span></span><br><span class="line">FragmentDataMeasure.getInstance().mService.getMultipleSeriesRenderer()</span><br><span class="line">                .setXAxisMin(-xTrans);</span><br><span class="line">FragmentDataMeasure.getInstance().mService.getMultipleSeriesRenderer()</span><br><span class="line">                .setXAxisMax(scanView.getXDistance() - xTrans);</span><br><span class="line"><span class="comment">// 重绘折线图</span></span><br><span class="line">FragmentDataMeasure.getInstance().mGraphicalView.repaint();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 <code>setXAxisMin()</code> 和 <code>setXAxisMax()</code> 是设置折线图 x 轴最小和最大坐标的方法，由于图形向右移，屏幕同样位置的坐标值就会减小，因此参数前带有负号。</p><p>接下来考虑缩放时的同步。</p><h5 id="缩放时同步"><a href="#缩放时同步" class="headerlink" title="缩放时同步"></a>缩放时同步</h5><p>缩放比移动复杂一点。</p><p>以下两幅图分别是扫描图缩小前和缩小后的图像</p><p><img src="http://oom3nz471.bkt.clouddn.com/107/B90A4B51C8DDE2563BA1B39F2E48FEEF.png" alt=""></p><p><img src="http://oom3nz471.bkt.clouddn.com/107/759BEF2938CDD5F8E088A1BBF635FD7B.png" alt=""></p><p>很明显缩小后，横轴所显示的长度比缩小前更长了，由于缩放中心是图形的中心点，因此左右两边多出的距离应该是相同的，除以二就可以得到两边各自多出的距离，这个距离就是折线图的 x 轴左右两边应该移动的量。</p><p>用代码来描述就是如下形式：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(scanView.getXDistance() / scanView.getXScale() - scanView.getXDistance()) / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>其中，<code>getXScale()</code> 用来获取当前缩放的比例，之后用缩放后的 <code>xDistance</code> 减去缩放前的，然后除以二就得到了折线图 x 轴左侧和右侧各应该移动的距离（左侧坐标减小右侧坐标变大即为放大折线图，反之则为缩小折线图）。</p><p>最后我们发现，其实移动和缩放折线图的方法都是通过设置折线图 x 轴左右两侧的坐标实现的，因此可以将移动和缩放的代码加在一起。如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 同步折线图</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">syncGraphicalView</span><span class="params">(<span class="keyword">double</span> xTrans)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 更新折线图</span></span><br><span class="line">FragmentDataMeasure.getInstance().mService.getMultipleSeriesRenderer()</span><br><span class="line">                .setXAxisMin(-xTrans -</span><br><span class="line">                        (scanView.getXDistance() / scanView.getXScale() - scanView.getXDistance()) / <span class="number">2</span>);</span><br><span class="line">FragmentDataMeasure.getInstance().mService.getMultipleSeriesRenderer()</span><br><span class="line">                .setXAxisMax(scanView.getXDistance() - xTrans +</span><br><span class="line">                        (scanView.getXDistance() / scanView.getXScale() - scanView.getXDistance()) / <span class="number">2</span>);</span><br><span class="line"><span class="comment">// 重绘折线图</span></span><br><span class="line">FragmentDataMeasure.getInstance().mGraphicalView.repaint();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="在线程中更新扫描图"><a href="#在线程中更新扫描图" class="headerlink" title="在线程中更新扫描图"></a>在线程中更新扫描图</h2><p>由于测量数据时，折线图和扫描图的绘制都需要在线程中进行，因此需要在 <code>MainActivity.java</code> 中加入如下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 改变 UI，绘制扫描图</span></span><br><span class="line">Runnable drawScanChart = <span class="keyword">new</span> Runnable() &#123;</span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">m_vFragmentDataMeasure.scanView = <span class="keyword">new</span> ScanningService(</span><br><span class="line">m_vFragmentDataMeasure.getContext(),</span><br><span class="line">ThreadParameter.getInstance().xList,</span><br><span class="line">ThreadParameter.getInstance().yList,</span><br><span class="line">ThreadParameter.getInstance().flawValue);</span><br><span class="line">m_vFragmentDataMeasure.addToScanChart(m_vFragmentDataMeasure.scanView);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>以及将 <code>handler.post(drawScanChart);</code> 语句加入到 <code>MainActivity</code> 的内部类 <code>MyThread</code> 中。</p><p><a href="http://www.liuhdme.com/2017/10/20/canvas%20%E4%BD%BF%E7%94%A8/">Canvas 在 LCJCSys 中的运用</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning 第三周（上）</title>
      <link href="2017/09/26/2017/ML%20%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
      <url>2017/09/26/2017/ML%20%E7%AC%AC%E4%B8%89%E5%91%A8%EF%BC%88%E4%B8%8A%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<blockquote><p>现在我们的学习方向该从回归问题转向分类问题了，因此这一周的主要内容是逻辑回归（Logistic Regression），千万别觉得逻辑回归这个名字弄混淆了，逻辑回归之所以被命名为逻辑回归只是因为一些历史遗留问题，你只需要知道这是一种处理分类问题而不是回归问题的方法就行了。</p></blockquote><a id="more"></a><h1 id="二元分类"><a href="#二元分类" class="headerlink" title=" 二元分类 "></a><p style="color:#3A5FCD"> 二元分类 </p></h1><p>不同于回归问题，回归问题的输出可能是一个存储着很多连续值的向量，二元分类问题的输出只能是 0 或 1，即 y∈{0,1}</p><p>一般 0 代表负面的类，而 1 代表正面的类，不过当然你也可以自己选择 0 和 1 分别代表哪一类。</p><p>而且我们只处理两个类别，因此这个问题被称为二元分类或二值分类。</p><p>其中一个解决这个问题的方法是，先采用线性回归，然后把预测值中所有大于 0.5 的结果标记为 1，把所有小于 0.5 的结果标记为 0。但是因为大多数分类问题并不是线性的，概率的分布可能不是均匀的，因此这种方法并不是很好用。</p><p>我们还是先表示一下假设函数，我们的假设函数应当满足：</p><script type="math/tex; mode=display">0 \leq h_\theta (x) \leq 1</script><p>因为概率的分布是由 0 到 1 的。</p><p>然后，我们要引进一个叫做“Sigmoid”的函数，它也被称为逻辑函数。</p><script type="math/tex; mode=display">g(z) = \dfrac{1}{1 + e^{-z}}</script><p><img src="http://upload-images.jianshu.io/upload_images/4933688-b12fdb69bcc83114.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>可以发现，它的值是在 0 和 1 之间的，因此它很适合用来描述分类问题。你可以通过这个网站来更直观的感受一下 Sigmoid 函数：<a href="https://www.desmos.com/calculator/bgontvxotm">https://www.desmos.com/calculator/bgontvxotm</a></p><p>那么如何用 Sigmoid 函数来表示预测函数呢，很简单，只需要把表达式中的 z 换成 \(\theta^Tx\) 就可以了，如果你还记得上节课的内容，就能理解 \(θ^Tx\) 其实是你的输入 x 乘上相应的参数，由于矩阵相乘需要满足第一个矩阵的列数等于第二个矩阵的行数，因此需要把参数矩阵 \(\theta\) 转置，这才有了 \(\theta^Tx\)。</p><p>因此我们预测函数应该是这个样子：</p><script type="math/tex; mode=display">\begin{align\*}h_\theta (x) &= g ( \theta^T x )\newline\newline& = \dfrac{1}{1 + e^{-\theta^Tx}}\newline\end{align\*}</script><p>\(h<em>\theta(x)\) 的结果告诉了我们输出为 1 的可能性，比如 \(h</em>\theta(x) = 0.7\) 就告诉我们这个类别为 1 的可能性为 70%。</p><script type="math/tex; mode=display">\begin{align\*}& h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline& P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align\*}</script><p>结果为 0 的可能性和结果为 1 的可能性加在一起等于 1。</p><h1 id="决策边界"><a href="#决策边界" class="headerlink" title=" 决策边界 "></a><p style="color:#3A5FCD"> 决策边界 </p></h1><p>为了得到 0 和 1 两个类别，我们可以把预测函数得到的结果进行如下转换：</p><script type="math/tex; mode=display">\begin{align\*}& h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline& h_\theta(x) < 0.5 \rightarrow y = 0 \newline\end{align\*}</script><p>回想一下 Sigmoid 函数的图像，当 z 等于 0 时，g(z)=0.5；当 z 大于 0 时，g(z)&gt;0.5；当 z 小于 0 时，g(z)&lt;0.5。</p><p>这很好解释，因为：</p><script type="math/tex; mode=display">\begin{align\*}z=0,  e^{0}=1 \Rightarrow  g(z)=1/2\newline z \to \infty, e^{-\infty} \to 0 \Rightarrow g(z)=1 \newline z \to -\infty, e^{\infty}\to \infty \Rightarrow g(z)=0 \end{align\*}</script><p>所以当把 z 换成 \(\theta^Tx\) 后，就有</p><script type="math/tex; mode=display">\begin{align\*}& h_\theta(x) = g(\theta^T x) \geq 0.5 \newline& when \; \theta^T x \geq 0\end{align\*}</script><p>于是，通过上面的推导，我们最终可以得到如下的结论：</p><script type="math/tex; mode=display">\begin{align\*}& \theta^T x \geq 0 \Rightarrow y = 1 \newline& \theta^T x < 0 \Rightarrow y = 0 \newline\end{align\*}</script><p>决策边界就是把 y=0 和 y=1 这两个区域分开的线，这是由我们的预测函数产生的。</p><p>比如：</p><script type="math/tex; mode=display">\begin{align\*}& \theta = \begin{bmatrix}5 \newline -1 \newline 0\end{bmatrix} \newline & y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \newline & 5 - x_1 \geq 0 \newline & - x_1 \geq -5 \newline& x_1 \leq 5 \newline \end{align\*}</script><p>在上面这个例子中，决策边界就是 x=5 这条垂直于 x 轴的线，所有在这条线左边的区域的 y 值都为 1，右边的区域的 y 值都为 0。</p><p>最后注意一点，Sigmoid 函数的输入值不一定非要是直线，它也可以是一个圆（比如\(z = \theta_0 + \theta_1 x_1^2 +\theta_2 x_2^2\)）或者任何形状。</p><h1 id="损失函数"><a href="#损失函数" class="headerlink" title=" 损失函数 "></a><p style="color:#3A5FCD"> 损失函数 </p></h1><p>我们不能使用在线性回归中使用的损失函数，因为逻辑回归会产生很多过于波动的输出，这就会产生许多局部最优解。换句话说，这将不是一个凸函数。</p><p>逻辑回归的损失函数应该像下面这样：</p><script type="math/tex; mode=display">\begin{align\*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align\*}</script><p><img src="http://upload-images.jianshu.io/upload_images/4933688-9ac7ed192f29692e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-e50acb8f875c0aee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>我解释一下当 y=1 的情况（即实际结果为 1）。</p><p>由于预测函数 \(h<em>\theta(x)\) 的取值范围是 0 到 1，因此 \(log(h</em>\theta(x))\) 的取值范围是负无穷到 0，那么 \(-log(h<em>\theta(x))\) 的取值范围就是正无穷到 0。如果 \(h</em>\theta(x) = 1\)，说明和真实情况相符，那么损失函数理应为 0，反之则应该为无穷大。</p><p>因此，预测函数的结果和真实情况偏离的越多，损失函数的值就越大，如果预测函数的结果和真实结果一致，那么损失函数就为 0。</p><script type="math/tex; mode=display">\begin{align\*}& \mathrm{Cost}(h_\theta(x),y) = 0 \text{  if  } h_\theta(x) = y \newline & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{  if  } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{  if  } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align\*}</script><p>把损失函数写成这种形式可以保证 \(J(\theta)\) 是一个凸函数。</p><h1 id="化简损失函数和梯度下降"><a href="#化简损失函数和梯度下降" class="headerlink" title=" 化简损失函数和梯度下降 "></a><p style="color:#3A5FCD"> 化简损失函数和梯度下降 </p></h1><p>我们可以把损失函数的两种情况合并成一种情况：</p><script type="math/tex; mode=display">\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))</script><p>当 y=0 时，等号右边的第一部分就为 0 了，只剩下了第二部分；当 y=1 时，等号右边的第二部分就为 0 了，只剩下了第一部分。</p><p>我们可以把损失函数写完整：</p><script type="math/tex; mode=display">J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]</script><p>向量形式如下：</p><script type="math/tex; mode=display">\begin{align\*}& h = g(X\theta)\newline& J(\theta)  = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right)\end{align\*}</script><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>回想一下在线性回归中梯度下降的一般形式：</p><script type="math/tex; mode=display">\begin{align\*}& Repeat \; \lbrace \newline & \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \newline & \rbrace\end{align\*}</script><p>通过积分我们可以把它进一步写成如下形式：</p><script type="math/tex; mode=display">\begin{align\*}& Repeat \; \lbrace \newline& \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline & \rbrace\end{align\*}</script><p>向量形式为：</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})</script><p>在逻辑回归中，由于损失函数和线性回归的损失函数不同，因此积分的过程肯定不同，但结果都是 \(\frac{1}{m} \sum<em>{i=1}^m (h</em>\theta(x^{(i)}) - y^{(i)}) x<em>j^{(i)}\) 这种形式，不同在于 \(h</em>\theta(x)\) 的内容不一样，下一节是具体推导过程（不感兴趣可以跳过）</p><h2 id="J-theta-的偏导"><a href="#J-theta-的偏导" class="headerlink" title="\(J(\theta)\) 的偏导"></a>\(J(\theta)\) 的偏导</h2><p>首先需要计算 Sigmoid 函数的偏导：</p><script type="math/tex; mode=display">\begin{align\*}\sigma(x)'&=\left(\frac{1}{1+e^{-x}}\right)'=\frac{-(1+e^{-x})'}{(1+e^{-x})^2}=\frac{-1'-(e^{-x})'}{(1+e^{-x})^2}=\frac{0-(-x)'(e^{-x})}{(1+e^{-x})^2}=\frac{-(-1)(e^{-x})}{(1+e^{-x})^2}=\frac{e^{-x}}{(1+e^{-x})^2} \newline &=\left(\frac{1}{1+e^{-x}}\right)\left(\frac{e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{+1-1 + e^{-x}}{1+e^{-x}}\right)=\sigma(x)\left(\frac{1 + e^{-x}}{1+e^{-x}} - \frac{1}{1+e^{-x}}\right)=\sigma(x)(1 - \sigma(x))\end{align\*}</script><p>现在计算 \(J(\theta)\) 的偏导：</p><script type="math/tex; mode=display">\begin{align\*}\frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} \frac{-1}{m}\sum_{i=1}^m \left [ y^{(i)} log (h_\theta(x^{(i)})) + (1-y^{(i)}) log (1 - h_\theta(x^{(i)})) \right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} \frac{\partial}{\partial \theta_j} log (h_\theta(x^{(i)}))   + (1-y^{(i)}) \frac{\partial}{\partial \theta_j} log (1 - h_\theta(x^{(i)}))\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} h_\theta(x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - h_\theta(x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \frac{\partial}{\partial \theta_j} \sigma(\theta^T x^{(i)})}{h_\theta(x^{(i)})}   + \frac{(1-y^{(i)})\frac{\partial}{\partial \theta_j} (1 - \sigma(\theta^T x^{(i)}))}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   + \frac{- (1-y^{(i)}) \sigma(\theta^T x^{(i)}) (1 - \sigma(\theta^T x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     \frac{y^{(i)} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{h_\theta(x^{(i)})}   - \frac{(1-y^{(i)}) h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \frac{\partial}{\partial \theta_j} \theta^T x^{(i)}}{1 - h_\theta(x^{(i)})}\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) x^{(i)}_j - (1-y^{(i)}) h_\theta(x^{(i)}) x^{(i)}_j\right ] \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} (1 - h_\theta(x^{(i)})) - (1-y^{(i)}) h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [     y^{(i)} - y^{(i)} h_\theta(x^{(i)}) - h_\theta(x^{(i)}) + y^{(i)} h_\theta(x^{(i)}) \right ] x^{(i)}_j \newline&= - \frac{1}{m}\sum_{i=1}^m \left [ y^{(i)} - h_\theta(x^{(i)}) \right ] x^{(i)}_j  \newline&= \frac{1}{m}\sum_{i=1}^m \left [ h_\theta(x^{(i)}) - y^{(i)} \right ] x^{(i)}_j\end{align\*}</script><p>向量形式为：</p><script type="math/tex; mode=display">\nabla J(\theta) = \frac{1}{m} \cdot  X^T \cdot \left(g\left(X\cdot\theta\right) - \vec{y}\right)</script><h1 id="多元分类"><a href="#多元分类" class="headerlink" title=" 多元分类 "></a><p style="color:#3A5FCD"> 多元分类 </p></h1><p>相比于二元分类，多元分类中有更多类别等着我们去分类。不同于二元分类中的 y={0, 1}，多元分类中的 y={0, 1, 2 …n}。</p><p>其实我们只需要把这个问题看作是 n+1 次的二元分类就可以了（+1 是因为类别的下标从 0 开始），每次二元分类中，我们都预测一下结果是当前类别的可能性。</p><script type="math/tex; mode=display">\begin{align\*}& y \in \lbrace0, 1 ... n\rbrace \newline& h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \newline& h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \newline& \cdots \newline& h_\theta^{(n)}(x) = P(y = n | x ; \theta) \newline& \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\newline\end{align\*}</script><p>那么最后的输出结果就是这 n+1 次预测中可能性最大的那个类别（即 \(h_\theta(x)\) 最大的那个类别）。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning 第二周</title>
      <link href="2017/09/21/2017/ML%20%E7%AC%AC%E4%BA%8C%E5%91%A8/"/>
      <url>2017/09/21/2017/ML%20%E7%AC%AC%E4%BA%8C%E5%91%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>上一周的主要内容是围绕单元线性回归介绍了机器学习中的几个基础概念，第二周将围绕多元线性回归讲解。</p></blockquote><a id="more"></a><h1 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title=" 多元线性回归 "></a><p style="color:#3A5FCD"> 多元线性回归 </p></h1><p>先列举出将会用到的一些符号</p><blockquote><p>\(<br>\begin{align*}<br>x_j^{(i)} &amp;= \text{ 第 } i \text{ 组训练数据的第 } j \text{ 个特征的值 } \newline<br> x^{(i)}&amp; = \text{ 第 } i \text{ 组训练数据的所有特征构成的列向量 } \newline<br>m &amp;= \text{训练集的数据量} \newline<br>n &amp;= \left| x^{(i)} \right| ; \text{ (每组训练数据特征数量) }<br>\end{align*}<br>\)</p></blockquote><h2 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h2><p>下面是多元线性回归中的假设函数</p><script type="math/tex; mode=display">h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n</script><p>如果任务是预测房价，那么在这个式子中，\(\theta_0\) 可以被看作是起步价，\(\theta_1\) 是每平米的价格，\(\theta_2\) 是每层的价格等等，而 \(x_1\) 就是房子的大小，\(x_2\) 是房子的层数等等。利用每个特征的参数，我们就能通过输入房子的特征来得到房价。</p><p>希望你学过并且还记得矩阵，利用矩阵乘法，假设函数可以被表达成向量形式</p><script type="math/tex; mode=display">h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em}  \theta_1 \hspace{2em}  ...  \hspace{2em}  \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x</script><p>方便起见，可以认为 \(x_0 = 1\)</p><p>假如我们有三组训练数据，每组数据都有一个特征，那么数据集和特征就可以用矩阵和列向量来表示</p><script type="math/tex; mode=display">\begin{align\*}X = \begin{bmatrix}x^{(1)}_0 & x^{(1)}_1  \newline x^{(2)}_0 & x^{(2)}_1  \newline x^{(3)}_0 & x^{(3)}_1 \end{bmatrix}&,\theta = \begin{bmatrix}\theta_0 \newline \theta_1 \newline\end{bmatrix}\end{align\*}</script><p>其中，\(\theta_0\) 相当于之前提到的起步价，\(x_0\) 都为 1。矩阵 \(X\) 的每一行就相当于一组训练数据，列向量 \(\theta\) 保存了所有特征的参数。这时假设函数只需短短的一个表达式就能表达</p><script type="math/tex; mode=display">h_\theta(X) = X \theta</script><p><small style="color:red"><strong>在接下来的内容以及以后的文章中，大写的 \(X\) 都代表一个矩阵，其中每一行都是一组训练数据。</strong></small></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>假设函数并没有什么区别，依然是如下形式</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} \displaystyle \sum_{i=1}^m \left (h_\theta (x^{(i)}) - y^{(i)} \right)^2</script><p>但它可以被表达为向量形式</p><script type="math/tex; mode=display">J(\theta) = \dfrac {1}{2m} (X\theta - \vec{y})^{T} (X\theta - \vec{y})</script><p>其中，\(\vec{y}\) 表示包含了所有真实值的列向量。</p><h2 id="多元线性回归的梯度下降"><a href="#多元线性回归的梯度下降" class="headerlink" title="多元线性回归的梯度下降"></a>多元线性回归的梯度下降</h2><p>梯度下降也没什么变化，唯一要注意的就是，比起在单元线性回归中只用更新 \(\theta_0\) 和 \(\theta_1\) 的值，多元线性回归中要更新所有的参数值，即从 \(\theta_0\) 到 \(\theta_n\) 都要更新。即不断执行以下内容直到达到最优解</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \;\;\;\;\;\; \text{for j := 0..n}</script><h3 id="矩阵表示"><a href="#矩阵表示" class="headerlink" title="矩阵表示"></a>矩阵表示</h3><p>梯度下降可以被这样表达</p><script type="math/tex; mode=display">\theta := \theta - \alpha \nabla J(\theta)</script><p>其中，\(\nabla J(\theta)\) 是损失函数对每个特征所对应的参数的偏导所构成的列向量，即所有参数的梯度值，表达如下</p><script type="math/tex; mode=display">\nabla J(\theta)  = \begin{bmatrix}\frac{\partial J(\theta)}{\partial \theta_0}   \newline \frac{\partial J(\theta)}{\partial \theta_1}   \newline \vdots   \newline \frac{\partial J(\theta)}{\partial \theta_n} \end{bmatrix}</script><p>由上一周中对梯度下降公式的推导结果可以知道，第 \(j\) 个参数的梯度值为</p><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_j} =  \frac{1}{m} \sum\limits_{i=1}^{m}  \left(h_\theta(x^{(i)}) - y^{(i)} \right) \cdot x_j^{(i)}</script><p>该式可以进一步变形为</p><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum\limits_{i=1}^{m}   x_j^{(i)} \cdot \left(h_\theta(x^{(i)}) - y^{(i)}  \right)</script><p>我们再把这个式子变为矩阵表示。其中</p><blockquote><ol><li>\(x^{(i)}<em>j\) 是第 \(i\) 组数据的第 \(j\) 个特征值，那么 \(\sum\limits</em>{i=1}^mx^{(i)}_j\) 就表示所有组数据的第 \(j\) 个特征值，那么它们就构成了一个列向量 \(\vec{x_j}\)；</li><li>\(h<em>\theta(x^{(i)}) - y^{(i)}\) 是第 \(i\) 组数据的预测结果与真实结果的差值，那么 \(\sum\limits</em>{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}\right)\) 就表示所有组数据的预测值与真实值的差值，即 \(X\theta - \vec{y}\)。</li></ol></blockquote><p>那么第 \(j\) 个参数的梯度值就可以进一步变为</p><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta_j} = \frac1m  \vec{x_j}^{T} (X\theta - \vec{y})</script><p>如果 \(j\) 从 \(0\) 到 \(n\)，那么这就构成了所有参数的梯度值 \(\nabla J(\theta)\)，即</p><script type="math/tex; mode=display">\nabla J(\theta) = \frac1mX^T\left(X\theta - \vec{y}\right)</script><p>最后，梯度下降的式子就可以被表示为</p><script type="math/tex; mode=display">\theta := \theta - \frac{\alpha}{m} X^{T} (X\theta - \vec{y})</script><h1 id="特征归一化"><a href="#特征归一化" class="headerlink" title=" 特征归一化 "></a><p style="color:#3A5FCD"> 特征归一化 </p></h1><p>如果我们把所有的输入都控制在大致类似的范围内，那么梯度下降的速度将会提高。这是因为，\(\theta\) 在较小范围内下降地较快，而在大范围内只会慢慢下降，因此当输入的变量分布地非常不均匀时，梯度下降的效率将会非常低下。</p><p>防止这一点的方法就是把输入变量的取值范围修改为大致相同的范围，比如像</p><script type="math/tex; mode=display">-1 ≤ x_{(i)} ≤ 1</script><p>或者</p><script type="math/tex; mode=display">-0.5 ≤ x_{(i)} ≤ 0.5</script><p>当然这不是必须的，我们的只是希望加速梯度下降的速度而已。</p><p>那么如何控制变量的范围呢，有两个方法：<big style="color:red">特征缩放</big>和<big style="color:red">均值归一化</big>。</p><p>特征缩放就是让每个特征的输入除以其取值范围（最大值减最小值），均值归一化就是让每个特征的输入减去其平均值，下面举个例子：</p><p>\(x_i\) 是某训练集中的房价，取值范围是 8000元/平米 到 19000元/平米，训练集中房价的平均值为 1000，那么经过特征缩放和均值归一化后 \(x_i\) 就为 \(\frac{x_i - 1000}{1900}\)。</p><h1 id="梯度下降中的技巧"><a href="#梯度下降中的技巧" class="headerlink" title=" 梯度下降中的技巧 "></a><p style="color:#3A5FCD"> 梯度下降中的技巧 </p></h1><p><big style="color:red">检查错误：</big>通过绘制一副以梯度下降中的迭代次数为横轴，损失函数\(J(\theta)\) 的值为纵轴的图像，可以直观的看到损失函数的值的变化情况，如果你发现 \(J(\theta)\) 在增加，那么你可能需要减小学习率 \(\alpha\)。</p><p><big style="color:red">自动收敛检验：</big>设置一个值 E，如果在一次迭代中 \(J(\theta)\) 减小到小于 E，则说明已经收敛，即达到最优解，不过在实际应用中很难决定这个值。</p><h1 id="特征与多项式回归"><a href="#特征与多项式回归" class="headerlink" title=" 特征与多项式回归 "></a><p style="color:#3A5FCD"> 特征与多项式回归 </p></h1><p>在一个数据集中，每一组数据的输入都有很多特征，然而，这些特征是人工选择的，因此不一定都是最恰当的，往往好的特征也能带来好的预测函数，那么改进特征和预测函数有很多方法。</p><p>我们可以把一些已有的特征组合产生新的特征，比如把特征 \(x_1\) 和 \(x_2\) 组合成 \(x_3 = x_1 · x_2\)。</p><h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>如果我们的预测函数不能很好的预测结果，那么应该尽量避免其是直线形式。</p><p>我们可以通过加入一些二次方或三次方项来改变预测函数的形式。</p><p>比如，如果预测函数为 \(h<em>\theta(x) = \theta_0 + \theta_1 x_1\)，那么我们可以针对 \(x_1\) 新增二次项和三次项，\(h</em>\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3\)。</p><p>有一点需要注意的是，如果你选择这种方法增加特征，那么特征归一化将很重要，因为这种方法将会使得特征值分布的范围各不相同。</p><h1 id="正规方程"><a href="#正规方程" class="headerlink" title=" 正规方程 "></a><p style="color:#3A5FCD"> 正规方程 </p></h1><p>正规方程是另一种寻找最优参数值的方法，相比于梯度下降需要不断迭代直到找到最优解，但正规方程不需要迭代。</p><p>直接通过下面的式子就能直接得到所有参数值</p><script type="math/tex; mode=display">\theta = (X^T X)^{-1}X^T y</script><p>而且正规方程不需要特征归一化。</p><p>关于正规方程的推导过程需要用到线性代数的知识，如果你感兴趣，可以通过 Coursera 给出的两个链接查看（英文），或者百度一下</p><p><a href="https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics">https://en.wikipedia.org/wiki/Linear<em>least_squares</em>(mathematics)</a>)</p><p><a href="http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression">http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression</a></p><p>虽然正规方程看上去很方便，但如果特征量非常大，那么使用正规方程将会花掉比梯度下降多很多的时间，因为其计算量很大。一般在实际应用中，如果特征数在 10000 以内，正规方程还可以胜任，但如果大于 10000，也许梯度下降会是更好的选择。</p><h1 id="总结"><a href="#总结" class="headerlink" title=" 总结 "></a><p style="color:#3A5FCD"> 总结 </p></h1><p>这一周先对单元线性回归进行了拓展，在多元线性回归中，概念没有太大的变化，只是进一步介绍了如何用向量、矩阵形式来表示相应的式子。</p><p>之后又介绍了提高梯度下降效率的特征归一化（分为两个步骤：特征缩放和均值归一化），以及在梯度下降中的两个技巧。</p><p>如果你觉得已有的特征不够恰当，可以适当的利用已有的特征增加一些新的特征。</p><p>最后简单介绍了正规方程，在特征量较小时，它也许比需要迭代的梯度下降方便，但如果特征量很大，梯度下降会是更好的选择。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning 第一周</title>
      <link href="2017/09/11/2017/ML%20%E7%AC%AC%E4%B8%80%E5%91%A8/"/>
      <url>2017/09/11/2017/ML%20%E7%AC%AC%E4%B8%80%E5%91%A8/</url>
      
        <content type="html"><![CDATA[<blockquote><p>昨天是教师节，正好完成了 Coursera 上的 Machine Learning 课程的最后一周作业，顺利拿到了证书。这门课程持续了两个多月，现在回忆刚开始学习的内容，发现对有些细节的记忆已经不是那么清晰了，幸亏 Coursera 提供了每周课程的参考文档，记录了每周课程的内容。为了保证课没白上，我打算再花两个多月的时间，每周认真读一篇文档，并把内容记录到网站中，供自己以及其他人有兴趣的人查看。我希望，从来没有接触过机器学习甚至是编程的人也可以轻松看懂我写的内容。</p></blockquote><a id="more"></a><hr><p><img src="http://upload-images.jianshu.io/upload_images/4933688-9b2e608e66231898.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><h1 id="介绍"><a href="#介绍" class="headerlink" title=" 介绍 "></a><p style="color:#3A5FCD"> 介绍 </p></h1><h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a><small>什么是机器学习</small></h2><p>关于机器学习的定义有两种。其中一种非正式的定义是由机器学习之父 —— Arthur Samuel 提出的，他形容机器学习是能让计算机不被明确编程而可以自主学习的研究领域。</p><p>Tom Mitchell 提出了一种更现代的定义：如果一个计算机程序对于某项任务 T 的表现好坏程度 P 随着经验 E 的增多而提升，那么我们就可以认为这段程序在执行任务 T 以及被好坏程度 P 考量的过程中获得了经验 E，并从中进行了学习。</p><blockquote><p>比如对下象棋来说，</p><p>E 就是一段程序下了很多盘象棋的经历。</p><p>T 就是下象棋这件任务。</p><p>P 就是这段程序能够赢得比赛胜利的可能性。</p></blockquote><p>那么机器学习就是说，这段程序通过不断的下象棋，获得越来越多的经验，并从中学习，自身的水平不断提高，使获胜的可能性越来越大（即任务完成的越来越好）。</p><p>通常，任何一个机器学习问题都可以被分为监督学习或者无监督学习。</p><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a><small>监督学习</small></h2><p>在监督学习中，我们的数据集已经被贴上了标签，就是说，对于某个数据来说，我们知道它可能具有的含义有哪些。</p><p>监督学习也可以分为“回归”和“分类”两类。对于回归问题，我们对计算机输入连续的数据，这些数据经过一系列函数就会得到一系列<b style="color:red">连续的</b>输出，然后通过一系列<b style="color:red">连续的</b>输出，我们就可以预测最后的结果。</p><p>对于分类问题，同样是预测结果，但不同于回归问题，我们是通过<b style="color:red">不连续的</b>的输出来预测结果，换句话讲，我们的目标是把输入的数据分成几个不连续的类别。下面举几个简单的例子🌰：</p><blockquote><p><strong>Example 1</strong><br>通过房子大小来预测房价的过程中，由于房价是一种连续的输出，因此这是一个回归问题。  </p><p><strong>Example 2</strong><br>给定一张男人或女人的照片，判断他/她的年龄，由于年龄是连续的，因此这是一个回归问题。然而，如果目标不是判断年龄，而是判断这个人是在读高中还是大学，那么这就是一个分类问题了，因为很明显，读高中或者读大学在数学上没有连续性关系。</p></blockquote><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a><small>无监督学习</small></h2><p>与监督学习不同，在无监督学习中，我们的数据集中没有标签，就是说我们并不知道我们的输出意味着什么，但我们可以通过无监督学习来使一些杂乱无章的数据进行结构化，看看它们之中是否有某种没被发现的联系。</p><p>举两个例子🌰：</p><blockquote><p>集群：收集 1000 篇文章，自动将它们分为几个不同的组，每组文章的字数或者页数都差不多。  </p><p>非集群：有一种可以把杂乱无章的数据自动结构化的算法 —— 鸡尾酒会算法（Cocktail Party Algorithm），它是从鸡尾酒会问题中引申出来的，它的本意是指在声音嘈杂的鸡尾酒会中，自动辨别出人的声音和音乐的声音。  </p><p><small><i>注：关于集群和非集群，后面的文章会有详细介绍。</i></small></p></blockquote><h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title=" 单变量线性回归 "></a><p style="color:#3A5FCD"> 单变量线性回归 </p></h1><p><small><i>注：以下内容有部分公式，感兴趣的可以仔细看看。</i></small></p><h2 id="什么是单变量线性回归"><a href="#什么是单变量线性回归" class="headerlink" title="什么是单变量线性回归"></a><small>什么是单变量线性回归</small></h2><p>从<b style="color:red">一个</b>输入 x 得到<b style="color:red">一个</b>输出 y，这就是单变量线性回归。实际上这在实际问题中很少遇到，这里只是为了方便阐述一些基本的概念。</p><h2 id="假设函数-Hypothesis-Function"><a href="#假设函数-Hypothesis-Function" class="headerlink" title="假设函数(Hypothesis Function)"></a><small>假设函数(Hypothesis Function)</small></h2><p>我们的输入通过假设函数（如何获得一个好的假设函数正是我们的任务所在），才能得到输出，这里的输出只是一个预测值，并不一定等于真实值，我们正是通过得到一系列的预测值来对结果进行<b style="color:red">预测</b>。</p><p>在单变量线性回归中，假设函数的形式为</p><script type="math/tex; mode=display">y = h_\theta(x) = \theta_0 + \theta_1x</script><p>这其实就是一条直线的方程，我们一旦确定了 \(\theta_0\) 和 \(\theta_1\) 的值，就可以对每个输入 x 映射到一个唯一的输出 y 了。</p><p>看一下下面这个例子：</p><div class="table-container"><table><thead><tr><th>输入 x</th><th>输出 y</th></tr></thead><tbody><tr><td>0</td><td>4</td></tr><tr><td>1</td><td>7</td></tr><tr><td>2</td><td>7</td></tr><tr><td>3</td><td>8</td></tr></tbody></table></div><p>从以上四组数据可以看出，每输入一个 x，都可以唯一得到一个输出 y。但由于在实际问题中，数据集中的数据不可能正好拟合一条完美的线（在单变量线性回归中这条线是直线），因此我们的任务是通过已知的数据（即训练集）找到一条最合适的线，使其看起来可以刻画所有的数据，而找这条线的过程其实就是寻找最合适的 \(\theta_0\) 和 \(\theta_1\) 的过程。</p><h2 id="损失函数-Cost-Function"><a href="#损失函数-Cost-Function" class="headerlink" title="损失函数(Cost Function)"></a><small>损失函数(Cost Function)</small></h2><p>通过假设函数得到的输出与真实结果肯定有误差，损失函数就是用来计算这个误差的大小的，对于每一个输入 x，都有一个输出 y 和对应的真实结果，相应的就产生了一个误差，损失函数计算了所有误差的和的平均值，它的一般形式如下：</p><script type="math/tex; mode=display">\begin{align\*}J(\theta_0, \theta_1) &= \frac {1}{2m} \sum_{i=1}^m  \left (\hat{y}_{i} - y_{i} \right)^2 \\&= \frac {1}{2m} \sum_{i=1}^m \left(h_\theta (x_{i}) - y_{i} \right)^2\end{align\*}</script><blockquote><p>其中</p><ol><li>m 是数据集的大小，即你有多少个输入 x；</li><li>\(i \in [1, m]\)；</li><li>\(\hat{y}<em>i\) 和 \(h</em>\theta (x<em>{i})\) 表示由假设函数得到的输出，\(y</em>{i}\) 表示真实结果；</li><li>\(\hat{y}<em>i - y_i\) 和 \(h</em>\theta (x<em>{i}) - y</em>{i}\) 表示预测得到的结果和真实结果之间的误差，最后我们会得到 m 组误差；</li><li>我们把 m 组误差都平方，再加起来除以 m，就得到了所有误差的平均值；</li><li>最后乘上了 \(\frac{1}{2}\) 是为了方便后续步骤（梯度下降），只需记住损失函数的目标是得到所有误差的平均即可。</li></ol></blockquote><p>实际上，损失函数有很多种形式，上面这种形式的损失函数也被称为“平方误差函数”或“均方误差”。</p><p>回想一下上面提到的，我们希望能找到一条直线可以最好的拟合所有数据，那么如果真的有一条直线可以穿过所有数据，那这时的损失函数的值就为 0。你可能会觉得损失函数越小越好，但其实也不一定，后面的文章会讲到原因。</p><h2 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a><small>梯度下降(Gradient Descent)</small></h2><p>由于损失函数 \(J(\theta_0, \theta_1)\) 可以刻画假设函数预测结果的准确性，那么只要损失函数减小了，就可以认为假设函数的预测准确性提高了，那么我们的直接任务就是减小损失函数。</p><p>在机器学习中，经常会用到图表的方式来加深我们对问题的理解。我们可以先考虑一下损失函数 \(J(\theta_0, \theta_1)\) 的图像该怎么画，由于目标是减小损失函数 \(J(\theta_0, \theta_1)\)，而损失函数 \(J(\theta_0, \theta_1)\) 的因变量是 \(\theta_0\) 和 \(\theta_1\)，那么我们可以以 \(\theta_0\) 和 \(\theta_1\) 为 x 轴和 y 轴，损失函数 \(J(\theta_0, \theta_1)\) 为 z 轴画出图像，只要不断地改变 \(\theta_0\) 和 \(\theta_1\)，就可以得到一副类似下面的图像</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-e2ea077ed334b078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>这幅图像就很好的刻画了损失函数 \(J(\theta_0, \theta_1)\) 在 \(\theta_0\) 和 \(\theta_1\) 取不同值时的结果。</p><p>当损失函数的值位于最低点时，我们的目标就达到了，因为此时 \(J(\theta_0, \theta_1)\) 最小，所以假设函数的预测结果与实际结果的误差最小。</p><p>梯度下降法就可以帮助我们找到最低点，为了方便起见，这里用一个二次函数做例子</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-6dcacd2cc5b828dd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>上面这幅图是个二次函数，横轴为 \(\theta<em>1\)，纵轴为 \(J(\theta_1)\)，其实这就是当参数只有一个时的损失函数，对于单变量线性回归，由于没有了 \(\theta_0\)，这时的假设函数就为 \(h\</em>\theta (x_{i}) = \theta_1x\)，但我们的关注点在损失函数 \(J(\theta_1)\) 上。</p><p>同样，我们的目标是减小损失函数，由图像可知，当 \(\theta_1 = 1\) 时 \(J(\theta_1)\) 最小，那么怎样才能时 \(\theta_1\) 取到 1 这一点呢？</p><p>由于我们的目标是找到图像中的最低点，那么就要使 \(\theta_1\) 不断的靠近 \(\theta_1 = 1\) 这个位置，一个办法就是让 \(\theta_1\) 减去当前的导数值。比如，假如 \(\theta_1 = 2\)，那么此时导数值大于 0，如果减去导数值，就会使 \(\theta_1\) 减小，更靠近 \(\theta_1 = 1\) 这个位置，假如 \(\theta_1 = 0\)，那么此时导数值小于 0，如果减去导数值，就会使 \(\theta_1\) 增大，同样更靠近 \(\theta_1 = 1\) 这个位置。因此，只要一遍又一遍的让 \(\theta_1\) 减去当前的导数值，最后达到损失函数最优解（即最低点），这就是<b style="color:red">梯度下降法</b>。</p><p>再回到这幅图像</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-e2ea077ed334b078.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt=""></p><p>与刚才的例子不同的是，这里多了一个参数 \(\theta_0\)，但梯度下降的思想没有变，依然是两个参数分别减去它们当前的导数值，最后就会到达图像中最低的点，需要<b style="color:red">注意</b>的是，这幅图像中有两个看起来都很低的点，其中有一个是局部最优（即在该点在附近区域内最低），另一个是全局最优（即该点在整个图像中最低），如何避免陷入局部最优以后再详细说明。</p><p>下面用公式来刻画梯度下降法</p><script type="math/tex; mode=display">\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)</script><p>这个式子中</p><blockquote><ol><li>\(\theta_j\) 表示损失函数 \(J(\theta_0, \theta_1)\) 中的参数 \(\theta_0\) 或 \(\theta_1\)；</li><li>\(:=\) 可以理解为把右边的式子的值赋给左边，即每次进行梯度下降时，\(\theta\) 的值都减去当前的导数值；</li><li>\(\frac{\partial}{\partial \theta_j}\) 表示对 \(\theta_j\) 求导数；</li><li>\(\alpha\) 表示学习率，用于控制 \(\theta\) 每次减小的幅度的大小</li></ol></blockquote><p>那么，只要不断地执行上面这个式子，就可以使 \(\theta_0\) 和 \(\theta_1\) 的值向最优解靠拢，并使损失函数达到最小值。</p><p>下面对求导的过程进行推导，如果学过导数，就完全没问题，由于 \(\theta_0\) 和 \(\theta_1\) 的求导过程不同，因此分开推导</p><script type="math/tex; mode=display">\begin{align\*}&\,\quad\theta_0 - \alpha \frac{\partial}{\partial \theta_0} J(\theta_0, \theta_1) \\&= \theta_0 - \alpha \frac{\partial}{\partial \theta_0} \frac{1}{2m} \sum_{i = 1}^{m} \left(h_\theta(x_i) - y_i \right) ^ 2 \\&= \theta_0 - \alpha \frac{\partial}{\partial \theta_0} \frac{1}{2m} \sum_{i = 1}^{m} \left(\theta_0 + \theta_1x_i - y_i \right) ^ 2 \\&= \theta_0 - \alpha \sum_{i = 1}^{m}  \frac{1}{2m} · 2 \left(\theta_0 + \theta_1x_i - y_i \right) \\&= \theta_0 - \alpha \sum_{i = 1}^{m}  \frac{1}{m} \left(\theta_0 + \theta_1x_i - y_i \right) \\&= \theta_0 - \alpha \frac{1}{m} \sum_{i = 1}^{m} \left(h_\theta(x) - y_i \right)\end{align\*}</script><script type="math/tex; mode=display">\begin{align\*}&\,\quad\theta_1 - \alpha \frac{\partial}{\partial \theta_1} J(\theta_0, \theta_1) \\&= \theta_1 - \alpha \frac{\partial}{\partial \theta_1} \frac{1}{2m} \sum_{i = 1}^{m} \left(h_\theta(x_i) - y_i \right) ^ 2 \\&= \theta_1 - \alpha \frac{\partial}{\partial \theta_1} \frac{1}{2m} \sum_{i = 1}^{m} \left(\theta_0 + \theta_1x_i - y_i \right) ^ 2 \\&= \theta_1 - \alpha \sum_{i = 1}^{m}  \frac{1}{2m} · 2 \left(\theta_0 + \theta_1x_i - y_i \right)x_i \\&= \theta_1 - \alpha \sum_{i = 1}^{m}  \frac{1}{m} \left((\theta_0 + \theta_1x_i - y_i )x_i\right) \\&= \theta_1 - \alpha \frac{1}{m} \sum_{i = 1}^{m} \left((h_\theta(x) - y_i)x_i\right)\end{align\*}</script><p>如果你一直看到了这里并且看懂了，应该就明白为什么损失函数的式子中有个 \(\frac{1}{2}\) 了，因为 \(\frac{1}{2}\) 可以消掉求导过程平方项的导数产生的 2 。</p><p>那么，梯度下降法就可以归结为，重复以下步骤直到求得最优解：</p><script type="math/tex; mode=display">\begin{align\*}  \theta_0 := & \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}(h_\theta(x_{i}) - y_{i}) \newline  \theta_1 := & \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}\left((h_\theta(x_{i}) - y_{i}) x_{i}\right) \newline  \end{align\*}</script><p>最后，找到了最优的 \(\theta_0\) 和 \(\theta_1\)，就得到了效果最好的假设函数。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a><p style="color:#3A5FCD">总结</p></h1><p>第一周的主要内容都呈现在了左侧的导航栏中，重点是通过单变量线性回归，引入了假设函数、损失函数和梯度下降的概念。在实际问题中，一般先会根据输入 x 的特征数 n，随机生成 n+1 个参数，来得到最初的假设函数，然后一步步通过梯度下降来减小损失函数，最终得到最佳的参数，即得到预测准确性最好的假设函数。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用 Mob 实现发送短信验证码功能</title>
      <link href="2017/09/10/2017/%E7%94%A8%20Mob%20%E5%AE%9E%E7%8E%B0%E5%8F%91%E9%80%81%E7%9F%AD%E4%BF%A1%E9%AA%8C%E8%AF%81%E7%A0%81%E5%8A%9F%E8%83%BD/"/>
      <url>2017/09/10/2017/%E7%94%A8%20Mob%20%E5%AE%9E%E7%8E%B0%E5%8F%91%E9%80%81%E7%9F%AD%E4%BF%A1%E9%AA%8C%E8%AF%81%E7%A0%81%E5%8A%9F%E8%83%BD/</url>
      
        <content type="html"><![CDATA[<blockquote><p>之前和室友参加的互联网大赛要做一个 APP，涉及到用户的登录注册，于是上网找了许多资料，其中有阿里大于，网易云等等，阿里大于的客服给我说他们不支持 Android，网易云还要拍手持身份证的照片，而且这两个都收费，还麻烦，于是找了一个既简单有免费的，叫做 Mob，官网如下 </p><p><a href="http://www.mob.com">Mob 官网</a></p><p>官方文档看了很多，还是觉得写的不好，于是自己写一篇。</p></blockquote><a id="more"></a><h3 id="注册账号"><a href="#注册账号" class="headerlink" title="注册账号"></a>注册账号</h3><p>在 Mob 官网右上角点击注册，依次填入信息，其中公司一栏可以随便填</p><h3 id="添加应用"><a href="#添加应用" class="headerlink" title="添加应用"></a>添加应用</h3><p>登录后，点击 SecurityCodeSDK - 立即使用，Mob 会提示你添加应用，名字自己填，再选 Android，添加完后再点击 SecurityCodeSDk - 进入，之后应该可以看到如下界面</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-6606cd1aa0af536a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="概况"></p><p>AppKey 和 AppSecret 一会会用到</p><h3 id="下载-SDK"><a href="#下载-SDK" class="headerlink" title="下载 SDK"></a>下载 SDK</h3><p>在网页上方“下载SDK”中点击“免费短信验证码SDK”，网页跳转后找到“短信验证码SDK”，根据需要选择系统吧，我做的 Android 就选 Android，网页跳转后根据需要选择 AndroidStudio 或者 eclipse，在屏幕右方选择，选好后点击即可开始下载。</p><p>下好后解压，打开文件夹，目录结构如下</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-c8c7efdf4487503b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="目录结构"></p><p>要用到的 SDK 在 SMSSDK 文件夹中</p><h3 id="导入-SDK-到工程目录"><a href="#导入-SDK-到工程目录" class="headerlink" title="导入 SDK 到工程目录"></a>导入 SDK 到工程目录</h3><p>打开 SMSSDK 文件夹，把两个 .jar 和 .aar 文件复制到工程目录中 libs 文件夹下，如图所示</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-90d62844ac8f416f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="导入 libs"></p><p>粘贴进去后，选中两个 .jar 文件，右键点击，选择 add as library</p><p>再打开 app 目录下的 build.gradle，添加如下两处内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">repositories&#123;</span><br><span class="line">    flatDir&#123;</span><br><span class="line">        dirs &#39;libs&#39;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">compile name:&#39;SMSSDK-2.1.4&#39;,ext:&#39;aar&#39;</span><br><span class="line">compile name:&#39;SMSSDKGUI-2.1.4&#39;,ext:&#39;aar&#39;</span><br></pre></td></tr></table></figure><p>最后看起来是这样的</p><p><img src="http://upload-images.jianshu.io/upload_images/4933688-8a8b78f418b109a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="添加完依赖库的 build.gradle"></p><p>添加的代码为 22-26 行和 36-37 行。</p><p>打开 AndroidManifest.xml，添加如下权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.READ_CONTACTS&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.READ_PHONE_STATE&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.WRITE_EXTERNAL_STORAGE&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.ACCESS_NETWORK_STATE&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.ACCESS_WIFI_STATE&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.INTERNET&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.RECEIVE_SMS&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.READ_SMS&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.GET_TASKS&quot; &#x2F;&gt;</span><br><span class="line">&lt;uses-permission android:name&#x3D;&quot;android.permission.ACCESS_FINE_LOCATION&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure><h3 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h3><p>先把完整代码贴出来（没贴 xml，这个不影响）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Register</span> <span class="keyword">extends</span> <span class="title">AppCompatActivity</span> <span class="keyword">implements</span> <span class="title">View</span>.<span class="title">OnClickListener</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> EditText etPhoneNumber;        <span class="comment">// 电话号码</span></span><br><span class="line">    <span class="keyword">private</span> Button sendVerificationCode;   <span class="comment">// 发送验证码</span></span><br><span class="line">    <span class="keyword">private</span> EditText etVerificationCode;   <span class="comment">// 验证码</span></span><br><span class="line">    <span class="keyword">private</span> Button nextStep;               <span class="comment">// 下一步</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> String phoneNumber;         <span class="comment">// 电话号码</span></span><br><span class="line">    <span class="keyword">private</span> String verificationCode;    <span class="comment">// 验证码</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">boolean</span> flag;   <span class="comment">// 操作是否成功</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">(Bundle savedInstanceState)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.onCreate(savedInstanceState);</span><br><span class="line">        setContentView(R.layout.activity_register);</span><br><span class="line"></span><br><span class="line">        init(); <span class="comment">// 初始化控件、注册点击事件</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> Context context = Register.<span class="keyword">this</span>;                       <span class="comment">// context</span></span><br><span class="line">        <span class="keyword">final</span> String AppKey = <span class="string">&quot;你的 AppKey&quot;</span>;                       <span class="comment">// AppKey</span></span><br><span class="line">        <span class="keyword">final</span> String AppSecret = <span class="string">&quot;你的 AppSecret&quot;</span>; <span class="comment">// AppSecret</span></span><br><span class="line"></span><br><span class="line">        SMSSDK.initSDK(context, AppKey, AppSecret);           <span class="comment">// 初始化 SDK 单例，可以多次调用</span></span><br><span class="line">        EventHandler eventHandler = <span class="keyword">new</span> EventHandler()&#123;       <span class="comment">// 操作回调</span></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">afterEvent</span><span class="params">(<span class="keyword">int</span> event, <span class="keyword">int</span> result, Object data)</span> </span>&#123;</span><br><span class="line">                Message msg = <span class="keyword">new</span> Message();</span><br><span class="line">                msg.arg1 = event;</span><br><span class="line">                msg.arg2 = result;</span><br><span class="line">                msg.obj = data;</span><br><span class="line">                handler.sendMessage(msg);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;;</span><br><span class="line">        SMSSDK.registerEventHandler(eventHandler);     <span class="comment">// 注册回调接口</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        etPhoneNumber = (EditText) findViewById(R.id.edit_phone_number);</span><br><span class="line">        sendVerificationCode = (Button) findViewById(R.id.btn_send_verification_code);</span><br><span class="line">        etVerificationCode = (EditText) findViewById(R.id.edit_verification_code);</span><br><span class="line">        nextStep = (Button) findViewById(R.id.btn_next_step);</span><br><span class="line">        sendVerificationCode.setOnClickListener(<span class="keyword">this</span>);</span><br><span class="line">        nextStep.setOnClickListener(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onClick</span><span class="params">(View v)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">switch</span> (v.getId()) &#123;</span><br><span class="line">            <span class="keyword">case</span> R.id.btn_send_verification_code:</span><br><span class="line">                <span class="keyword">if</span> (!TextUtils.isEmpty(etPhoneNumber.getText())) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (etPhoneNumber.getText().length() == <span class="number">11</span>) &#123;</span><br><span class="line">                        phoneNumber = etPhoneNumber.getText().toString();</span><br><span class="line">                        SMSSDK.getVerificationCode(<span class="string">&quot;86&quot;</span>, phoneNumber); <span class="comment">// 发送验证码给号码的 phoneNumber 的手机</span></span><br><span class="line">                        etVerificationCode.requestFocus();</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">else</span> &#123;</span><br><span class="line">                        Toast.makeText(<span class="keyword">this</span>, <span class="string">&quot;请输入完整的电话号码&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                        etPhoneNumber.requestFocus();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    Toast.makeText(<span class="keyword">this</span>, <span class="string">&quot;请输入电话号码&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    etPhoneNumber.requestFocus();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">case</span> R.id.btn_next_step:</span><br><span class="line">                <span class="keyword">if</span> (!TextUtils.isEmpty(etVerificationCode.getText())) &#123;</span><br><span class="line">                    <span class="keyword">if</span> (etVerificationCode.getText().length() == <span class="number">4</span>) &#123;</span><br><span class="line">                        verificationCode = etVerificationCode.getText().toString();</span><br><span class="line">                        SMSSDK.submitVerificationCode(<span class="string">&quot;86&quot;</span>, phoneNumber, verificationCode);</span><br><span class="line">                        flag = <span class="keyword">false</span>;</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        Toast.makeText(<span class="keyword">this</span>, <span class="string">&quot;请输入完整的验证码&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                        etVerificationCode.requestFocus();</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    Toast.makeText(<span class="keyword">this</span>, <span class="string">&quot;请输入验证码&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    etVerificationCode.requestFocus();</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Handler handler = <span class="keyword">new</span> Handler() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">handleMessage</span><span class="params">(Message msg)</span> </span>&#123;</span><br><span class="line">            <span class="keyword">super</span>.handleMessage(msg);</span><br><span class="line">            <span class="keyword">int</span> event = msg.arg1;</span><br><span class="line">            <span class="keyword">int</span> result = msg.arg2;</span><br><span class="line">            Object data = msg.obj;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (result == SMSSDK.RESULT_COMPLETE) &#123;</span><br><span class="line">                <span class="comment">// 如果操作成功</span></span><br><span class="line">                <span class="keyword">if</span> (event == SMSSDK.EVENT_SUBMIT_VERIFICATION_CODE) &#123;</span><br><span class="line">                    <span class="comment">// 校验验证码，返回校验的手机和国家代码</span></span><br><span class="line">                    Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证成功&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    Intent intent = <span class="keyword">new</span> Intent(Register.<span class="keyword">this</span>, MainActivity.class);</span><br><span class="line">                    startActivity(intent);</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (event == SMSSDK.EVENT_GET_VERIFICATION_CODE) &#123;</span><br><span class="line">                    <span class="comment">// 获取验证码成功，true为智能验证，false为普通下发短信</span></span><br><span class="line">                    Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证码已发送&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                &#125; <span class="keyword">else</span> <span class="keyword">if</span> (event == SMSSDK.EVENT_GET_SUPPORTED_COUNTRIES) &#123;</span><br><span class="line">                    <span class="comment">// 返回支持发送验证码的国家列表</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// 如果操作失败</span></span><br><span class="line">                <span class="keyword">if</span> (flag) &#123;</span><br><span class="line">                    Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证码获取失败，请重新获取&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                    etPhoneNumber.requestFocus();</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    ((Throwable) data).printStackTrace();</span><br><span class="line">                    Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证码错误&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">onDestroy</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.onDestroy();</span><br><span class="line">        SMSSDK.unregisterAllEventHandler();  <span class="comment">// 注销回调接口</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>注意要把你的 AppKey 和 AppSecret 替换进去。</p><p>其实看过很多教程你会发现，很多时候并不是代码看不懂，而是在配置文件的时候出了问题，因为一个刚接触 Android 没两天的人是不可能对很多第三方 SDK 有需求的，一开始都是以基础为主，所以需要配置第三方 SDK 的很多都是写过一些 Android 代码的人，他们所关心的其实就是该调用哪些方法来实现自己的目的，所以知道怎么调用就行了，再根据实际需求写几个 if-else 就可以了。</p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">SMSSDK.initSDK(context, AppKey, AppSecret);           &#x2F;&#x2F; 初始化 SDK 单例，可以多次调用</span><br><span class="line">EventHandler eventHandler &#x3D; new EventHandler()&#123;       &#x2F;&#x2F; 操作回调</span><br><span class="line">@Override</span><br><span class="line">public void afterEvent(int event, int result, Object data) &#123;</span><br><span class="line">Message msg &#x3D; new Message();</span><br><span class="line">msg.arg1 &#x3D; event;</span><br><span class="line">msg.arg2 &#x3D; result;</span><br><span class="line">msg.obj &#x3D; data;</span><br><span class="line">handler.sendMessage(msg);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line">SMSSDK.registerEventHandler(eventHandler);     &#x2F;&#x2F; 注册回调接口</span><br></pre></td></tr></table></figure><p>这几行代码是调用这个 SDK 的基础，写在 onCreate 中。</p></blockquote><p>发送短信验证码的语句是 </p><p><code>SMSSDK.getVerificationCode(&quot;86&quot;, phoneNumber);</code></p><p>phoneNumber 的取值取决于用户输入的号码，“86”是中国的区号，根据需求也可以让用户自己选择，稍微修改一下就行。</p><blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (result == SMSSDK.RESULT_COMPLETE) &#123;</span><br><span class="line"><span class="comment">// 如果操作成功</span></span><br><span class="line"><span class="keyword">if</span> (event == SMSSDK.EVENT_SUBMIT_VERIFICATION_CODE) &#123;</span><br><span class="line"><span class="comment">// 校验验证码，返回校验的手机和国家代码</span></span><br><span class="line">Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证成功&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">Intent intent = <span class="keyword">new</span> Intent(Register.<span class="keyword">this</span>, MainActivity.class);</span><br><span class="line">startActivity(intent);</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (event == SMSSDK.EVENT_GET_VERIFICATION_CODE) &#123;</span><br><span class="line"><span class="comment">// 获取验证码成功，true为智能验证，false为普通下发短信</span></span><br><span class="line">Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证码已发送&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> (event == SMSSDK.EVENT_GET_SUPPORTED_COUNTRIES) &#123;</span><br><span class="line"><span class="comment">// 返回支持发送验证码的国家列表</span></span><br><span class="line">&#125;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line"><span class="comment">// 如果操作失败</span></span><br><span class="line"><span class="keyword">if</span> (flag) &#123;</span><br><span class="line">Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证码获取失败，请重新获取&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">etPhoneNumber.requestFocus();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">((Throwable) data).printStackTrace();</span><br><span class="line">Toast.makeText(Register.<span class="keyword">this</span>, <span class="string">&quot;验证码错误&quot;</span>, Toast.LENGTH_SHORT).show();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这几行代码判断验证码发没发送以及正不正确，根据 event 来确定是哪一种情况，如果想深入了解，可以参考官方文档 </p></blockquote><p><a href="http://wiki.mob.com/android-%E7%9F%AD%E4%BF%A1sdk%E6%93%8D%E4%BD%9C%E5%9B%9E%E8%B0%83/">Android 短信SDK操作回调</a></p><p>当然，免费服务肯定收费的要差点，体现在短信发的比收费的慢，大概要十秒才能收到，收费的三秒就收到了，由于只是个比赛，自己做着玩，用免费的足矣。</p><p>另外，最开始免费的时候每天只能发 20 条短信，而且一个手机号每 12 小时只能收到 5 条，所以，省着点用，当然你可以在应用管理界面申请上线登记（见第一张图），审核通过后就完全免费了。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>小人物</title>
      <link href="2017/08/25/2017/%E5%B0%8F%E4%BA%BA%E7%89%A9/"/>
      <url>2017/08/25/2017/%E5%B0%8F%E4%BA%BA%E7%89%A9/</url>
      
        <content type="html"><![CDATA[<a id="more"></a><p>这是《白鹿原》中，岳维山和鹿兆鹏的对话。</p><p>昨天下午，外面突然下起了大雨，我突然想到早上晒的床单没收，但心想既然已经淋湿了，干脆吃过晚饭再回去收好了。</p><p>回到宿舍楼门口，我发现早上晾着床单的衣杆空无一物，正纳闷有人连床单都偷，转头却瞅见我的床单正堆在门口的一把椅子上，我走过去拿起来一看，竟一点都没有淋湿。</p><p>后来才知道，是宿管帮我收的。</p><p>突然想到半个月前的一天晚上，十一点后，我因为有急事要出门，不得不叫醒宿管帮我开了宿舍楼的大门。我回来后，忘了锁上大门便直接上了楼。第二天早上，我路过大门时宿管叫住了我，我暗道不妙，心想要被臭骂一通了，可宿管的态度比我预料的好得多。宿管说，之前有一任宿管，因为晚上忘了锁门，一个寝室被偷了一台电脑，那任宿管直接遭到了解雇，后来如何不得而知。虽然宿管声音中透露着不高兴，但我能理解，生活不易，更应小心翼翼，我也为自己昨晚忘记锁门而感到后悔。</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fzhwd6uejhj30rs0h4gm1.jpg" alt=""></p><p>几个月前，我和我搭乘的滴滴司机聊天，不知怎的聊到了宿管，那时我说，我见到的所有宿管脾气都特别差，他们活该只能当宿管，帮别人看门。司机却认为，我们最容易忽略平常见惯的小人物，却不知道有时能帮人成事的正是这些小人物，不必亲近，但起码要尊重。那时这番话从我左耳进右耳出，权当司机无心之言，也没在意。现在再想，深觉有理。</p><p>回想之前，每逢有事要和宿管沟通，只要我感到宿管的态度没我预想的好，我就会用更差的态度对待，最后事虽办完了，却落得一阵差心情。每个人都有不为人知的痛楚，不用太过较真。</p><p>开头引用《白鹿原》，觉得有些牵强，但一千个人心中有一千个哈姆雷特，每个人的解读也不太一样，我有自己的看法，姑且拿来作引。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>过期的热恋期</title>
      <link href="2017/08/18/2017/%E8%BF%87%E6%9C%9F%E7%9A%84%E7%83%AD%E6%81%8B%E6%9C%9F/"/>
      <url>2017/08/18/2017/%E8%BF%87%E6%9C%9F%E7%9A%84%E7%83%AD%E6%81%8B%E6%9C%9F/</url>
      
        <content type="html"><![CDATA[<p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fzhwcesih9j30rs0ijq42.jpg" alt=""></p><p>什么是热恋期？不间断的聊天，每天清晨的早安，每天中午的午安和夜晚的晚安，或是走路一直十指紧扣，这些都是热恋期的表现，可我今天要说的，不是这些。</p><p>人是喜新鲜感的动物，朋友，恋人，夫妻刚走到一起时，每天都被新鲜感包围着，遮蔽了双方的瑕疵，朋友间恨相知晚，恋人间情投意合，夫妻间琴瑟调和。</p><p>随着时间推移，新鲜感慢慢消散，双方的瑕疵不断被放大，朋友形同陌路，恋人爱恨纠缠，夫妻同床异梦，比比皆是。</p><p>只有不多的一部分关系经受住了时间和内心的拷问，这些关系没有因为新鲜感消散就中断，也许没有最初那般新鲜，但这份情感却因时间长久而升华，变的深厚牢固。</p><p>总有人说，热恋期不过三个月。如果这是真的，我觉得也是好事，因为甄别感情是否深厚的时间不算太久，如果连三个月都撑不过，那么这份感情早日结束是最好的结果。如果时间到了三个月，半年，一年后，感情依然还在，那么恭喜你们，我觉得你们现在才正处于真正的热恋期。</p><p>前几天女朋友和闺蜜去泰国旅游。一天晚上，十二点半，我发现距离她上次回我消息已经是近一小时前了，因为时差，这时她那里是十一点半，再加上手机显示她 4G 在线，我很担心她是否出了什么事。我是个容易瞎想的人，就开始各种猜测发生了什么，那几分钟我幻想了无数种可能，当然也有很坏的可能。我很慌，那种感觉就像失去了亲人一样难受，虽然明知也许是自己想多了，但本能的反应让我意识到，我早已把她当做至亲看待。过了一会，她回复了我，刚才是去洗澡和洗衣服了，这时我躺在床上，如释重负。</p><p>平时在学校，我和女朋友在一个班上课，在一个实验室做项目，每天都在一起，双方的缺点暴露了很多次，吵过很多次架，我甚至因为吵架摔碎过手机屏幕，我们濒临过很多次分手的边缘，但最后都被拉了回来，这也让我越来越珍惜这个姑娘。</p><p>我总抱怨她表达不好自己的意思，总说些让我莫名其妙的话，但我慢慢发现，即使她只说一半，我也越来越能知道她想表达的意思。我们经常会说默契的话，做默契的事，就像预先排演好的一样，不需言语，一个眼神足矣。</p><p>过期的热恋期，不需要每时每刻的聊天，不需要刻意的形式，你向我投来一个眼神，我就懂你的深情，不是新鲜感，而是灵魂的交汇。</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fzhwcs61rrj30rs0ijdgi.jpg" alt=""></p><hr>]]></content>
      
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
