<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>追本溯源，带你读懂图自编码器 | Deep Road | 推荐系统 | 信息检索</title>

  
  <meta name="author" content="LiuHDme">
  

  
  <meta name="description" content="介绍Kipf 与 Welling 16 年发表了「Variational Graph Auto-Encoders」，提出了基于图的（变分）自编码器 Variational Graph Auto-Encoder（VGAE）。
论文地址：http://bayesiandeeplearning.org/2">
  

  
  
  <meta name="keywords" content="Graph Neural Network">
  

  <meta id="viewport" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">

  <meta property="og:title" content="追本溯源，带你读懂图自编码器"/>

  <meta property="og:site_name" content="Deep Road"/>

  
  <meta property="og:image" content="/favicon.ico"/>
  

  <link href="/favicon.ico" rel="icon">
  <link rel="alternate" href="/atom.xml" title="Deep Road" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head>


<body>
<div class="blog">
  <div class="content">

    <header>
  <div class="site-branding">
    <h1 class="site-title">
      <a href="/">Deep Road</a>
    </h1>
    <p class="site-description">推荐系统 | 信息检索</p>
  </div>
  <nav class="site-navigation">
    <ul>
      
        <li><a href="/">Home</a></li>
      
        <li><a href="/archives">Archives</a></li>
      
    </ul>
  </nav>
</header>

    <main class="site-main posts-loop">
    <article>

  
    
    <h3 class="article-title"><span>追本溯源，带你读懂图自编码器</span></h3>
    
  

  <div class="article-top-meta">
    <span class="posted-on">
      <a href="/2020/02/17/2020/追本溯源，带你读懂图自编码器/" rel="bookmark">
        <time class="entry-date published" datetime="2020-02-16T16:00:00.000Z">
          2020-02-17
        </time>
      </a>
    </span>
  </div>


  

  <div class="article-content">
    <div class="entry">
      
        <p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-082018.png" alt=""></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>Kipf 与 Welling 16 年发表了「Variational Graph Auto-Encoders」，提出了基于图的（变分）自编码器 <strong>Variational Graph Auto-Encoder（VGAE）</strong>。</p>
<p><strong>论文地址</strong>：<a target="_blank" rel="noopener" href="http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf">http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf</a></p>
<p><strong>代码地址</strong>：<a target="_blank" rel="noopener" href="https://github.com/tkipf/gae">https://github.com/tkipf/gae</a></p>
<p><strong>先介绍一下 intention 和用途</strong>。了解 embedding 概念的应该知道，获取一个合适的 embedding 来表示一张图中的节点不是一件容易的事，另外，如果我们能找到合适的 embedding，就能将它们用在上层任务中。VGAE 的用途之一就是通过 encoder-decoder 的结构来获取图中节点的 embedding。</p>
<p><strong>VGAE 的思想和变分自编码器（VAE）很像</strong>：利用隐变量（latent variables），让模型学习出一些分布（distribution），再从这些分布中采样得到 latent representations（或者说 embedding），<strong>这个过程是 encode 阶段</strong>，然后再利用得到的 latent representations 重构（reconstruct）出原始的图，<strong>这个过程是 decode 阶段</strong>。只不过，VGAE 的 encoder 使用了 GCN，decoder 是简单的内积（inner product）形式。</p>
<p><strong>下面具体讲解变分图自编码器（VGAE）。先讲 GAE，即图自编码器（没有变分）。</strong></p>
<h2 id="图自编码器（GAE）"><a href="#图自编码器（GAE）" class="headerlink" title="图自编码器（GAE）"></a>图自编码器（GAE）</h2><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-094210.png" alt=""></p>
<p><strong>统一规范，规定几个 notation 如下：</strong></p>
<ul>
<li>图用 $\mathcal{G} = (\mathcal{V}, \mathcal{E})$ 表示，其中 $\mathcal{V}$ 表示节点集合，$\mathcal{E}$ 表示边集合</li>
<li>$\mathbf{A}$: 邻接矩阵</li>
<li>$\mathbf{D}$: 度矩阵</li>
<li>$N$: 节点数</li>
<li>$d$: 节点的特征（features）维度</li>
<li>$\mathbf{X} \in \Bbb{R}^{N \times d}$表示节点的特征矩阵</li>
<li>$f$: embedding 维度</li>
<li>$\mathbf{Z} \in \Bbb{R}^{N \times f}$: 节点的 embedding</li>
</ul>
<h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p><strong>GAE 使用 GCN 作为 encoder</strong>，来得到节点的 latent representations（或者说 embedding），这个过程可用一行简短的公式表达：</p>
<script type="math/tex; mode=display">\mathbf{Z} = \mathrm{GCN}(\mathbf{X}, \mathbf{A})</script><p>我们将 $\mathrm{GCN}$ 视为一个函数，然后将 $\mathbf{X}$ 和 $\mathbf{A}$ 作为输入，输入到 $\mathrm{GCN}$ 这个函数中，输出 $\mathbf{Z} \in \Bbb{R}^{N×f}$，$\mathbf{Z}$ 代表的就是所有节点的 latent representations，或者说 embedding。</p>
<p>如何定义 $\mathrm{GCN}$ 这个函数？kipf 在论文中定义如下：</p>
<script type="math/tex; mode=display">\mathrm{GCN}(\mathbf{X}, \mathbf{A}) = \tilde{\mathbf{A}} \mathrm{ReLU} (\tilde{\mathbf{A}}\mathbf{XW_0})\mathbf{W_1}</script><p>其中，$\tilde{\mathbf{A}} = \mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}}$，$\mathbf{W_0}$ 和 $\mathbf{W_1}$ 是待学习的参数。</p>
<p>简言之，<strong>这里 $\mathrm{GCN}$ 就相当于一个以节点特征和邻接矩阵为输入、以节点 embedding 为输出的函数，目的只是为了得到 embedding</strong>。</p>
<h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p><strong>GAE 采用 inner-product 作为 decoder 来重构（reconstruct）原始的图</strong>：</p>
<script type="math/tex; mode=display">\hat{\mathbf{A}} = \sigma(\mathbf{Z} \mathbf{Z}^\mathrm{T})</script><p>上式中，$\hat{\mathbf{A}}$ 就是重构（reconstruct）出来的邻接矩阵。</p>
<h3 id="How-to-learn"><a href="#How-to-learn" class="headerlink" title="How to learn"></a>How to learn</h3><p><strong>一个好的 $\mathbf{Z}$，就应该使重构出的邻接矩阵与原始的邻接矩阵尽可能的相似</strong>，因为邻接矩阵决定了图的结构。因此，GAE 在训练过程中，采用交叉熵作为损失函数：</p>
<script type="math/tex; mode=display">\mathcal{L} = - \frac{1}{N} \sum y \log \hat{y} + (1-y) \log (1-\hat{y})</script><p>上式中，$y$ 代表邻接矩阵 $\mathbf{A}$ 中某个元素的值（0 或 1），$\hat{y}$ 代表重构的邻接矩阵 $\mathbf{\hat{A}}$ 中相应元素的值（0 到 1 之间）。</p>
<p>从损失函数可以看出来，我们希望重构的邻接矩阵（或者说重构的图），与原始的邻接矩阵（或者说原始的图）越接近、越相似越好。</p>
<h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p><strong>图自编码器（GAE）的原理简明、清晰，训练起来不麻烦</strong>，因为可训练的参数只有 $\mathbf{W_0}$ 和 $\mathbf{W_1}$，kipf 的代码实现中，这两个参数矩阵的维度分别是 $d \times 32$ 和 $32 \times 16$，如果 $d = 1000$，也只有 32512 个参数，相当少了。</p>
<p>下面讲 VGAE（<strong>变分</strong>图自编码器）。</p>
<h2 id="变分图自编码器（VGAE）"><a href="#变分图自编码器（VGAE）" class="headerlink" title="变分图自编码器（VGAE）"></a>变分图自编码器（VGAE）</h2><p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-094333.png" alt=""></p>
<p><strong>在 GAE 中，一旦 $\mathrm{GCN}$ 中的 $\mathbf{W_0}$ 和 $\mathbf{W_1}$ 确定了，那么 $\mathrm{GCN}$ 就是一个确定的函数，给定 $\mathbf{X}$ 和 $\mathbf{A}$，输出的 $\mathbf{Z}$ 就是确定的</strong>。</p>
<p>而<strong>在 VGAE 中，$\mathbf{Z}$ 不再由一个确定的函数得到</strong>，而是从一个（多维）高斯分布中采样得到，说得更明确些，就是我们先通过 $\mathrm{GCN}$ 确定一个（多维）高斯分布，再从这个分布中采样得到 $\mathbf{Z}$。下面简单描述一下这个过程。</p>
<h3 id="确定均值和方差"><a href="#确定均值和方差" class="headerlink" title="确定均值和方差"></a>确定均值和方差</h3><p><strong>高斯分布可以唯一地由二阶矩确定</strong>。因此，想要确定一个高斯分布，我们只需要知道均值和方差。<strong>VGAE 利用 $\mathrm{GCN}$ 来分别计算均值和方差</strong>：</p>
<script type="math/tex; mode=display">\mathbf{\mu} = \mathrm{GCN}_{\mu} (\mathbf{X}, \mathbf{A})</script><script type="math/tex; mode=display">\log \mathbf{\sigma} = \mathrm{GCN}_{\sigma} (\mathbf{X}, \mathbf{A})</script><p>这里有两个要注意的地方，第一个是 $\mathrm{GCN}$ 的下标，$\mathrm{GCN}<em>{\mu}$ 和 $\mathrm{GCN}</em>{\sigma}$ 中的 $\mathbf{W<em>0}$ 是相同的、共享的，但 $\mathbf{W_1}$ 是不同的，因此用下标来作区分；第二个是通过 $\mathrm{GCN}</em>{\sigma}$ 得到的是 $\log \sigma$，这样可以方便后续的计算。</p>
<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>既然已经得到了均值和方差（准确来说应该是均值向量和协方差矩阵），就可以通过采样得到 $\mathbf{Z}$ 了，但是，<strong>采样操作无法提供梯度信息</strong>，也就是说，反向传播在采样操作无法计算梯度，也就无法更新 $\mathbf{W_0}$ 和 $\mathbf{W_1}$。解决办法是<strong>重参数化（reparameterization）</strong>。<strong>了解（连续分布）重参数化的可以跳过下面一段。</strong></p>
<blockquote>
<p>简单说一下重参数化。大家应该知道，如果 $\epsilon$ 服从 $\cal{N}(0, 1)$，那么 $z = \mu + \epsilon \sigma$ 就服从 $\cal{N} (\mu, \sigma^2)$。因此，我们可以先从标准高斯中采样一个 $\epsilon$，再通过 $\mu + \epsilon \sigma$ 计算出 $z$，这样一来，$z$ 的表达式清晰可见，梯度信息也就有了。</p>
</blockquote>
<p><strong>VGAE 的 decoder 也是一个简单的 inner-product，与 GAE 的 decoder 没有区别。</strong></p>
<h3 id="How-to-learn-1"><a href="#How-to-learn-1" class="headerlink" title="How to learn"></a>How to learn</h3><p>VGAE 依然希望重构出的图和原始的图尽可能相似，除此之外，还希望 $\mathrm{GCN}$ 计算出的分布与标准高斯尽可能相似。因此<strong>损失函数由交叉熵和 KL 散度两部分构成</strong>：</p>
<script type="math/tex; mode=display">\cal{L} = 
\cal{L}_{\mathrm{CE}} \mathrm{+} \mathrm{KL[\mathit{q}(\mathbf{Z} | \mathbf{X}, \mathbf{A}) || \mathit{p}(\mathbf{Z})]}</script><p>其中，$q(\mathbf{Z} | \mathbf{X}, \mathbf{A})$ 是 $\mathrm{GCN}$ 计算出的分布，$p(\mathbf{Z})$ 是标准高斯。</p>
<p>不过，论文中的优化目标是这样的：</p>
<script type="math/tex; mode=display">
\mathcal{L}=\mathbb{E}_{q(\mathbf{Z} | \mathbf{X}, \mathbf{A})}[\log p(\mathbf{A} | \mathbf{Z})]-\operatorname{KL}[q(\mathbf{Z} | \mathbf{X}, \mathbf{A}) \| p(\mathbf{Z})]</script><p>这是用变分下界写出的优化目标，第一项是期望（<strong>我看到有的博客说第一项是交叉熵，这不对</strong>），第二项是 KL 散度。</p>
<h2 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h2><p>Kipf 和 Welling 在三个数据集上进行了效果分析，任务是链接预测，详情见下表：</p>
<p><img src="http://liuhdme-blog.oss-cn-beijing.aliyuncs.com/2020-02-19-084139.png" alt=""></p>
<p>可见，（变分）图自编码器在三个数据集上的效果都很好，不过，<strong>注意带星号的 GAE* 和 VGAE*，这两个模型是不带 input features 的，就是说节点的 features 就是 one-hot 向量，这种情况下，（变分）图自编码器的效果不仅不比 SC 和 DW 好，甚至还有点差</strong>。不过，Kipf 和 Welling 在论文中指出，SC 和 DW 不支持 input features，就是说这是这两个方法本身的缺陷，就不能怪 Kipf 他们使用 input features 了。</p>
<hr>
<p>到这里，这篇介绍（变分）图自编码器的文章应该告一段落了。不过，<strong>关于 kipf 开源的代码，还有很多有意思的事情</strong>。</p>
<h2 id="Analyze-the-code"><a href="#Analyze-the-code" class="headerlink" title="Analyze the code"></a>Analyze the code</h2><h3 id="带权交叉熵"><a href="#带权交叉熵" class="headerlink" title="带权交叉熵"></a>带权交叉熵</h3><p>不管是 GAE 还是 VGAE，损失函数中交叉熵的实现都使用的是这样一个 API：<code>tf.nn.weighted_cross_entropy_with_logits</code>，当我尝试把它换成 <code>tf.nn.sigmoid_cross_entropy_with_logits</code>（即最常用的交叉熵的定义）后，<strong>各项指标从 90 多下降到了 50 左右</strong>，所以有必要分析一下这个带权重的交叉熵在这里的意义。</p>
<p>带权交叉熵损失的数学定义如下：</p>
<script type="math/tex; mode=display">- \mathrm{w} · y \log \hat{y} - (1 - y) \log (1 - \hat{y})</script><p>可以看到，与常用的交叉熵损失相比，只是在正样本的那一项前面乘了一个系数，这个系数是一个参数，是需要传进去的。可以来分析一下这个系数 $\mathrm{w}$ 大于 1 和大于 0 小于 1 对交叉熵损失的影响：</p>
<ul>
<li>$\mathrm{w} \gt 1$: 当模型去预测一个 true label 是正样本的样本时，如果模型误预测成了负样本，那么由于 $\mathrm{w}$ 大于 1，此时交叉熵损失会更大。换句话说，$\mathrm{w} \gt 1$ 使得模型分错一个正样本时受到的惩罚变大了，因此这会使模型更“专注”于对正样本的分类。说的再专业些，就是 $\mathrm{w} \gt 1$ 会减少 false negative 的数量，增加 true positive 的数量，这会使 recall 上升。</li>
<li>$0 \lt \mathrm{w} \lt 1$: 与 $\mathrm{w} \gt 1$ 相反，会减少 false positive 的数量，增加 true negative 的数量，使 precision 上升。</li>
</ul>
<p>什么情况下要设置一个 $\mathrm{w}$ ？没错，就是当训练集正负样本数量不平衡的时候。在一张图（graph）里，往往边的数量远少于节点数量，这会使邻接矩阵中 1 的数量远少于 0 的数量，即<strong>正样本的数量远少于负样本的数量</strong>，这种情况下，我们可以设定一个大于 1 的 $\mathrm{w}$，让模型更“专注”于正样本的分类。可是如何设置一个比较好的 $\mathrm{w}$ 呢？可以这样，用负样本数与正样本数的比例作为 $\mathrm{w}$ 的值，因为这个比例如果大于 1，就说明正样本比负样本少，刚好可以用这个比例作为 $\mathrm{w}$ 的值，来让模型“专注”与正样本；如果这个比例小于 1，就说明正样本比负样本多，也可以用这个比例作为 $\mathrm{w}$ 的值。</p>
<p>下面是 kipf 的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pos_weight = <span class="built_in">float</span>(adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) / adj.<span class="built_in">sum</span>()</span><br><span class="line">norm = adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] / <span class="built_in">float</span>((adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) * <span class="number">2</span>)</span><br><span class="line">self.cost = norm * tf.reduce_mean(</span><br><span class="line">            tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))</span><br></pre></td></tr></table></figure>
<p><code>adj</code> 是邻接矩阵，<code>pos_weight</code> 就是上面说的系数 $\mathrm{w}$，<code>adj.sum()</code> 就是正样本数，<code>adj.shape[0] * adj.shape[0] - adj.sum()</code> 就是负样本数，因此 <code>pos_weight</code> 就是负样本数/正样本数。</p>
<p><strong>另外，这里有一个 <code>norm</code></strong>，因为我们用负样本数/正样本数作为系数 $\mathrm{w}$，相当于把正样本的数目增大到和负样本相同的数目，因此计算损失的时候，应该在总损失的基础上除以 2 倍的负样本数目，而不是原始的所有样本数 $N$（正样本数 $N_1$+负样本数 $N_2$），而 <code>tf.reduce_mean</code> 相当于总损失乘上 $\frac{1}{N}$，因此 <code>norm</code> 应该被设置成 $\frac{N}{2 N_2}$，最后的效果就是总损失乘上 $\frac{1}{2N_2}$。</p>
<p><strong>我觉得这样有点麻烦，直接把 <code>norm</code> 设置成 $\frac{1}{2N_2}$，然后把 <code>tf.reduce_mean</code> 换成 <code>tf.reduce_sum</code> 就好了</strong>，即</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">norm = <span class="number">1</span> / <span class="built_in">float</span>((adj.shape[<span class="number">0</span>] * adj.shape[<span class="number">0</span>] - adj.<span class="built_in">sum</span>()) * <span class="number">2</span>)</span><br><span class="line">self.cost = norm * tf.reduce_sum(</span><br><span class="line">            tf.nn.weighted_cross_entropy_with_logits(logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))</span><br></pre></td></tr></table></figure>
<p><strong>我重新跑了一下代码，效果一样。</strong></p>
<h2 id="Github-Issues"><a href="#Github-Issues" class="headerlink" title="Github Issues"></a>Github Issues</h2><p><strong>说了这么多，图自编码器也有让人质疑的地方</strong>。下面是 Github 上的一些 issues：</p>
<ol>
<li>重构的邻接矩阵中，TP 非常大，导致 recall（TP/(TP+FN)）很高，但 precision（TP/(TP+FP)）非常低，<a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-455966135">@tkipf</a> 建议找到一个合适的threshold，来平衡 recall 和 precision，但 <a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-456907910">@zzheyu</a> 发现即使将 threshold 调整到 0.9 也不能解决这个问题。</li>
<li>另外，<a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-456907910">@zzheyu</a> 认为论文中 VGAE 在 validation 和 test set 上表现出色的原因之一是 validation 和 test set 的数据非常 balance，即 half edges and half non-edges，为了印证这一点，他将 validation 和 test set 中 non-edges 的个数依次增加为之前的 5 倍、30 倍和 100 倍，发现 average precision score 随着 non-edges 的数目增多而大幅下降，同时，他发现 val/test sets 上的 F1 score 也非常低。对此，<a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-456912625">@tkipf</a> 表示可以尝试用负采样来解决这一问题。</li>
<li>此外，@tkipf 还表示，他们用 GAE 的变体在 knowledge base completion (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.06103">https://arxiv.org/abs/1703.06103</a>) 和 recommender system (<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.02263">https://arxiv.org/abs/1706.02263</a>) 进行了测试，实验效果很好。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-478657131">@jlevy44</a> 评论说，他的 dataset 的 pos edges 的数量远少于 neg edges 的数量，他通过调整threshold 缓解了这一问题，但是，他发现模型 reconstruct 的邻接矩阵与原始邻接矩阵差异很大，他表示疑问。<a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-478707136">@tkipf</a> 回复说，可以把 score function 从 $\text{sigmoid}(x^T, x)$ 替换成 $e^{-d(x, y)}$，其中 $d(x, y)$ 表示欧氏距离，他认为这样 reconstruct 的图多多少少会不那么密集。<ol>
<li>关于这一点，他建议 @jlevy44 可以看”A Tutorial on Energy-Based Learning” by LeCun et al.，使用 $e^{-d(x,y)}$ 作为 score function，相当于在使用一个 energy-based model</li>
<li>关于在链接预测任务中使用 energy-based loss，可以参考 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.03815">https://arxiv.org/abs/1707.03815</a></li>
<li>使用内积作为 score function，会带来 graphons 问题（<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Graphon">https://en.wikipedia.org/wiki/Graphon</a>），这会使生成模型倾向于生成一些紧致的区块（dense blobs）</li>
</ol>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-478915428">@XuanHeIIIS</a> 询问 @tkipf 是否尝试过以减小 feature reconstruction loss 作为训练目标来训练 GCN，<a target="_blank" rel="noopener" href="https://github.com/tkipf/gae/issues/20#issuecomment-478925040">@tkipf</a> 给出的方法是，将 decoder 的 output 输入到 node-level MLP 中，以重构node features；如果 features 是二元的值，可以用交叉熵作为损失，如果是连续的值，用 MSE也可以。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用一句话总结图自编码器：<strong>利用 GCN 计算出节点的 embedding，再重构原始的图，要求重构的图与原始的图尽可能相似。</strong> 只不过，在这个过程中海油很多要考虑的细节，比如 $\mathbf{Z}$ 的先验，论文里使用的是高斯，但 kipf 明确指出高斯先验会带来新的问题，比如损失函数中，KL 散度前需要乘上一个系数，减小 KL 散度的影响，否则模型效果会下降，等等，还有很多值得商榷与改进的地方。众所周知，今年图神经网络是大势所趋，我们能否继续发现图自编码器的身影呢？</p>
<p><strong>References：</strong></p>
<p>[1] <a target="_blank" rel="noopener" href="http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf">http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf</a></p>
<p>[2] <a target="_blank" rel="noopener" href="https://github.com/tkipf/gae">https://github.com/tkipf/gae</a></p>
<p>[3] <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/107854139">https://zhuanlan.zhihu.com/p/107854139</a></p>

      
    </div>

  </div>

  <div class="article-footer">
    <div class="article-meta pull-left">

    
      

    <span class="post-categories">
      <i class="icon-categories"></i>
        <a href="/categories/技术/">技术</a>, <a href="/categories/技术/Machine-Learning/">Machine Learning</a>
    </span>
    

    
    

    <span class="post-tags">
      <i class="icon-tags"></i>
        <a href="/tags/Graph-Neural-Network/">Graph Neural Network</a>
    </span>
    

    </div>

    
  </div>
</article>

  






    </main>

    <footer class="site-footer">
  <p class="site-info">
    Proudly powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and
    Theme by <a href="https://github.com/CodeDaraW/Hacker" target="_blank">Hacker</a>
    </br>
    
    &copy; 2021 LiuHDme
    
  </p>
</footer>
    
    
    
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
        tex2jax: {
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
  </script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });
  </script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  </div>
</div>
</body>
</html>